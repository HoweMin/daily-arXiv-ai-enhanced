<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]
- [cs.LG](#cs.LG) [Total: 77]
- [math.OC](#math.OC) [Total: 18]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Scheduling Your LLM Reinforcement Learning with Reasoning Trees](https://arxiv.org/abs/2510.24832)
*Hong Wang,Zhezheng Hao,Jian Luo,Chenxing Wei,Yao Shu,Lei Liu,Qiang Lin,Hande Dong,Jiawei Chen*

Main category: cs.AI

TL;DR: 提出了基于推理树结构的Re-Schedule数据调度算法，通过r-score衡量查询学习难度，从简单到复杂构建课程，在数学推理基准上提升准确率3.2%


<details>
  <summary>Details</summary>
Motivation: 现有RLVR数据调度方法依赖路径指标，忽略了查询的推理树结构，需要更强大的调度基础

Method: 引入推理分数(r-score)衡量查询学习难度，基于此设计Re-Schedule算法，构建从简单到复杂的课程调度

Result: 在6个数学推理基准测试中显著提升平均准确率，最高增益达3.2%

Conclusion: 推理树的结构理解为RLVR数据调度提供了更强大和原则性的基础

Abstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query's
`Reasoning Tree'. This process involves exploring nodes (tokens) and
dynamically modifying the model's policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query's learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.

</details>


### [2] [Cyclic Counterfactuals under Shift-Scale Interventions](https://arxiv.org/abs/2510.25005)
*Saptarshi Saha,Dhruv Vansraj Rathore,Utpal Garain*

Main category: cs.AI

TL;DR: 研究循环结构因果模型中的反事实推理，针对包含反馈环的复杂系统


<details>
  <summary>Details</summary>
Motivation: 传统反事实推理框架假设无环结构因果模型，但许多真实系统（如生物系统）包含违反无环性的反馈循环或循环依赖

Method: 在循环结构因果模型下研究反事实推理，特别关注移位-尺度干预（软性、策略式变化，重新缩放和/或移动变量的机制）

Result: 未在摘要中明确说明

Conclusion: 扩展反事实推理框架到包含循环依赖的真实系统

Abstract: Most counterfactual inference frameworks traditionally assume acyclic
structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,
many real-world systems (e.g. biological systems) contain feedback loops or
cyclic dependencies that violate acyclicity. In this work, we study
counterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,
soft, policy-style changes that rescale and/or shift a variable's mechanism.

</details>


### [3] [Taming the Real-world Complexities in CPT E/M Coding with Large Language Models](https://arxiv.org/abs/2510.25007)
*Islam Nassar,Yang Lin,Yuan Jin,Rongxin Zhu,Chang Wei Tan,Zenan Zhai,Nitika Mathur,Thanh Tien Vu,Xu Zhong,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: 提出了ProFees框架，基于LLM自动化医疗评估与管理(E/M)编码任务，在真实数据集上比商业系统准确率提升36%，比最强单提示基线提升近5%。


<details>
  <summary>Details</summary>
Motivation: 自动化E/M编码可减轻医生文档负担、提高计费效率，最终改善患者护理，但现实世界的复杂性使该任务具有挑战性。

Method: 开发了基于大语言模型(LLM)的ProFees框架，专门解决E/M编码中的现实复杂性。

Result: 在专家策划的真实数据集上，ProFees比商业CPT E/M编码系统准确率提高36%以上，比最强单提示基线提高近5%。

Conclusion: ProFees框架有效解决了E/M编码自动化中的现实复杂性，显著提升了编码准确性。

Abstract: Evaluation and Management (E/M) coding, under the Current Procedural
Terminology (CPT) taxonomy, documents medical services provided to patients by
physicians. Used primarily for billing purposes, it is in physicians' best
interest to provide accurate CPT E/M codes. %While important, it is an
auxiliary task that adds to physicians' documentation burden. Automating this
coding task will help alleviate physicians' documentation burden, improve
billing efficiency, and ultimately enable better patient care. However, a
number of real-world complexities have made E/M encoding automation a
challenging task. In this paper, we elaborate some of the key complexities and
present ProFees, our LLM-based framework that tackles them, followed by a
systematic evaluation. On an expert-curated real-world dataset, ProFees
achieves an increase in coding accuracy of more than 36\% over a commercial CPT
E/M coding system and almost 5\% over our strongest single-prompt baseline,
demonstrating its effectiveness in addressing the real-world complexities.

</details>


### [4] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 提出Autoregressive State-Tracking Prompting (ASTP)方法，解决LLM在游戏交易系统中无法遵循规则流程的问题，通过显式状态跟踪确保交易完整性。


<details>
  <summary>Details</summary>
Motivation: LLM在动态游戏交互中具有灵活性，但无法遵循规则治理的交易系统流程（浏览-报价-审核-确认），这会损害玩家信任。

Method: 引入ASTP方法，通过精心设计的提示词强制LLM显式报告预定义状态标签，并结合状态特定的占位符后处理方法进行准确价格计算。

Result: 在300个交易对话评估中，状态合规率>99%，计算精度达99.3%。小模型(Gemini-2.5-Flash)使用ASTP后性能可匹敌大模型(Gemini-2.5-Pro)，响应时间从21.2秒降至2.4秒。

Conclusion: ASTP方法为商业游戏提供了同时满足实时性要求和资源限制的实用解决方案，在保持LLM创造性的同时确保交易流程的完整性。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [5] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: 提出了PM4GRPO方法，通过过程挖掘技术增强推理过程奖励，显著提升大型推理模型的多步推理能力


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的后训练奖励方案通常只关注结果，缺乏对推理过程的监督，限制了模型推理能力的提升

Method: 使用过程挖掘技术计算一致性奖励，衡量策略模型推理过程与预训练教师模型的匹配程度，结合标准答案/格式奖励进行组相对策略优化

Result: 在五个基准测试中，PM4GRPO显著优于现有的GRPO后训练方法

Conclusion: 利用过程挖掘实现推理感知的GRPO能有效增强策略模型的推理能力

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [6] [H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts](https://arxiv.org/abs/2510.25091)
*Peilin Tan,Liang Xie,Churan Zhi,Dian Tu,Chuanqi Shi*

Main category: cs.AI

TL;DR: H3M-SSMoEs是一个基于超图的多模态股票预测架构，结合LLM推理和风格结构混合专家，通过多模态超图、LLM增强推理和风格结构专家系统实现卓越的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决股票预测中复杂的时间依赖性、异质模态和动态演化的股票间关系，现有方法难以在可扩展框架内统一结构、语义和制度自适应建模。

Method: 使用多上下文多模态超图分层捕获时空动态，LLM增强推理模块融合定量和文本模态，风格结构混合专家系统结合共享市场专家和行业专业专家。

Result: 在三个主要股票市场的广泛实验表明，H3M-SSMoEs在预测准确性和投资绩效方面均超越最先进方法，同时展现有效的风险控制。

Conclusion: H3M-SSMoEs通过创新的多模态超图架构、LLM推理和风格结构专家系统，成功解决了股票预测的关键挑战，实现了卓越的性能。

Abstract: Stock movement prediction remains fundamentally challenging due to complex
temporal dependencies, heterogeneous modalities, and dynamically evolving
inter-stock relationships. Existing approaches often fail to unify structural,
semantic, and regime-adaptive modeling within a scalable framework. This work
introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with
LLM reasoning and Style-Structured Mixture of Experts, integrating three key
innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically
captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph
(LCH) and persistent inter-stock dependencies through a Global Context
Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon
Divergence weighting mechanism for adaptive relational learning and cross-modal
alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large
language model with lightweight adapters to semantically fuse and align
quantitative and textual modalities, enriching representations with
domain-specific financial knowledge; and (3) a Style-Structured Mixture of
Experts (SSMoEs) that combines shared market experts and industry-specialized
experts, each parameterized by learnable style vectors enabling regime-aware
specialization under sparse activation. Extensive experiments on three major
stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in
both superior predictive accuracy and investment performance, while exhibiting
effective risk control. Datasets, source code, and model weights are available
at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.

</details>


### [7] [KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA](https://arxiv.org/abs/2510.25101)
*Zhuo Chen,Fei Wang,Zixuan Li,Zhao Zhang,Weiwei Ding,Chuanguang Yang,Yongjun Xu,Xiaolong Jin,Jiafeng Guo*

Main category: cs.AI

TL;DR: KnowCoder-A1是一个基于多阶段课程强化学习的KBQA模型，通过结果监督训练LLM在知识库上自主进行代理推理，显著提升了问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有KBQA方法通过过程监督微调LLM，但这种方法对探索的激励较弱，无法有效增强代理推理能力。

Method: 采用多阶段课程强化学习，首先在高质量轨迹上微调LLM建立基础能力，然后通过从易到难的奖励调度应用结果监督的强化学习。

Result: 在三个主流数据集上持续优于先前方法，在GrailQA的零样本子集上实现了11.1%的相对提升，且仅使用十二分之一的训练数据。

Conclusion: 基于结果监督的KnowCoder-A1展现出强大的推理能力，证明了多阶段课程强化学习在KBQA中的有效性。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.

</details>


### [8] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.AI

TL;DR: 提出了Agentic Moderation框架，利用专业化代理来防御多模态系统的越狱攻击，相比静态方法提供动态、协作和可解释的调节。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法通常作为静态层应用于输入或输出，仅提供二元分类（安全/不安全），缺乏上下文感知和可解释性。

Method: 引入动态协作代理框架，包括Shield、Responder、Evaluator和Reflector四个专业化代理，实现模型无关的多模态系统安全调节。

Result: 在5个数据集和4个大型视觉语言模型上的实验表明，攻击成功率降低7-19%，拒绝率提高4-20%，保持稳定的不跟随率。

Conclusion: Agentic Moderation通过利用代理架构的灵活性和推理能力，提供了模块化、可扩展和细粒度的安全执行，展示了代理系统作为自动安全治理基础的潜力。

Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.

</details>


### [9] [Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision](https://arxiv.org/abs/2510.25205)
*Yuyang Xia,Zibo Liang,Liwei Deng,Yan Zhao,Han Su,Kai Zheng*

Main category: cs.AI

TL;DR: EneAD是一个节能自动驾驶框架，通过自适应感知和鲁棒决策模块，在保持感知精度的同时显著降低计算能耗，提升电动车的续航里程。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术带来社会经济效益，但计算引擎能耗限制了电动车续航。现有模型压缩技术要么导致模型过大，要么显著降低感知精度。

Method: 1. 自适应感知模块：管理多个不同计算消耗的感知模型，动态调整执行帧率；基于贝叶斯优化的可迁移调优方法；轻量级分类模型识别场景感知难度。2. 鲁棒决策模块：基于强化学习的决策模型，设计正则化项增强面对扰动感知结果的驾驶稳定性。

Result: EneAD可将感知消耗降低1.9倍到3.5倍，从而将驾驶里程提升3.9%到8.5%。

Conclusion: 该框架在能耗和驾驶性能方面均表现出优越性，为节能自动驾驶提供了有效解决方案。

Abstract: Autonomous driving is an emerging technology that is expected to bring
significant social, economic, and environmental benefits. However, these
benefits come with rising energy consumption by computation engines, limiting
the driving range of vehicles, especially electric ones. Perception computing
is typically the most power-intensive component, as it relies on largescale
deep learning models to extract environmental features. Recently, numerous
studies have employed model compression techniques, such as sparsification,
quantization, and distillation, to reduce computational consumption. However,
these methods often result in either a substantial model size or a significant
drop in perception accuracy compared to high-computation models. To address
these challenges, we propose an energy-efficient autonomous driving framework,
called EneAD. In the adaptive perception module, a perception optimization
strategy is designed from the perspective of data management and tuning.
Firstly, we manage multiple perception models with different computational
consumption and adjust the execution framerate dynamically. Then, we define
them as knobs and design a transferable tuning method based on Bayesian
optimization to identify promising knob values that achieve low computation
while maintaining desired accuracy. To adaptively switch the knob values in
various traffic scenarios, a lightweight classification model is proposed to
distinguish the perception difficulty in different scenarios. In the robust
decision module, we propose a decision model based on reinforcement learning
and design a regularization term to enhance driving stability in the face of
perturbed perception results. Extensive experiments evidence the superiority of
our framework in both energy consumption and driving performance. EneAD can
reduce perception consumption by 1.9x to 3.5x and thus improve driving range by
3.9% to 8.5%

</details>


### [10] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 提出RAVR框架，通过答案引导的变分推理增强大语言模型的推理能力，将难以学习的问题转化为可学习的问题


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖模型能生成高质量推理路径，但对于超出模型当前能力的任务，采样高质量推理路径困难，学习可能强化次优推理。受认知科学启发，"为什么是这个答案"比"答案是什么"更容易回答

Method: 引入RAVR框架，使用答案条件推理作为问题推理的变分代理，通过答案引导生成高质量推理路径

Result: 在通用和数学领域的实验中，RAVR相比强基线模型均取得一致改进，减少了犹豫，加强了结论整合，促进了问题特定策略

Conclusion: 答案引导的推理能有效提高推理路径的期望效用，将难以处理的问题转化为可学习问题

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [11] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: FELA是一个基于LLM的多代理进化系统，能够从复杂的工业事件日志数据中自动提取有意义且高性能的特征，解决了传统自动特征工程方法在可解释性、操作灵活性和异构数据适应性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 工业事件日志数据具有大规模、高维度、多样化数据类型和复杂时空结构等特点，使得特征工程极具挑战性。现有的自动特征工程方法如AutoML或遗传方法存在可解释性差、操作僵化、对复杂异构数据适应性不足等问题。

Method: FELA采用多代理进化系统，集成LLM的推理和编码能力与洞察引导的自进化范式。系统包含创意代理、代码代理和批评代理，通过协作生成、验证和实施新特征创意。评估代理总结反馈并更新分层知识库和双记忆系统，实现持续改进。

Result: 在真实工业数据集上的广泛实验表明，FELA能够生成可解释的、领域相关的特征，显著提高模型性能，同时减少人工工作量。

Conclusion: 基于LLM的多代理系统有潜力成为复杂现实环境中自动化、可解释和自适应特征工程的通用框架。

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


### [12] [From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity](https://arxiv.org/abs/2510.25232)
*Tianxi Wan,Jiaming Luo,Siyuan Chen,Kunyao Lan,Jianhua Chen,Haiyang Geng,Mengyue Wu*

Main category: cs.AI

TL;DR: 开发了PsyCoTalk，首个支持共病诊断的大规模对话数据集，包含3000个多轮诊断对话，通过合成电子病历和多智能体框架构建，经精神科医生验证。


<details>
  <summary>Details</summary>
Motivation: 精神疾病共病在临床上很常见但诊断复杂，需要处理多种同时发生的疾病，现有资源难以支持共病研究。

Method: 整合合成患者电子病历构建和多智能体诊断对话生成，创建502个合成EMR，将临床访谈协议转化为分层状态机和上下文树，支持130多个诊断状态。

Result: 构建的PsyCoTalk数据集在对话长度、标记分布和诊断推理策略方面与现实临床记录高度一致，精神科医生确认其真实性和诊断有效性。

Conclusion: 该数据集提高了诊断准确性和治疗规划，为精神疾病共病研究提供了宝贵资源，支持在一次对话中实现多疾病筛查模型的开发和评估。

Abstract: Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.

</details>


### [13] [GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning](https://arxiv.org/abs/2510.25320)
*Jiaqi Wu,Qinlao Zhao,Zefeng Chen,Kai Qin,Yifei Zhao,Xueqian Wang,Yuhang Yao*

Main category: cs.AI

TL;DR: GAP是一个基于图规划的自主代理框架，通过建模子任务间的依赖关系，实现自适应并行和串行工具执行，显著提升多步推理场景下的执行效率和任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自主代理（如ReAct）采用顺序推理和执行，无法利用独立子任务间的内在并行性，导致工具利用效率低下和多步推理性能不佳。

Method: 训练代理基础模型将复杂任务分解为依赖感知的子任务图，自主确定哪些工具可以并行执行，哪些必须按顺序依赖执行。采用两阶段训练策略：在高质量图规划数据集上进行监督微调，然后在策略性采样的查询上使用基于正确性的奖励函数进行强化学习。

Result: 在MHQA数据集上的实验结果表明，GAP显著优于传统ReAct基线，特别是在多步检索任务上，同时通过智能并行化实现了工具调用效率的显著提升。

Conclusion: GAP框架通过依赖感知的编排机制，在保持任务准确性的同时大幅提升了执行效率，为复杂任务解决提供了更高效的自主代理解决方案。

Abstract: Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.

</details>


### [14] [Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm](https://arxiv.org/abs/2510.25388)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 提出了KVDA-UCT算法，通过分析即时奖励推断价值差异，在MCTS中检测更多抽象，提升样本效率，优于现有的OGA-UCT算法。


<details>
  <summary>Details</summary>
Motivation: 现有MCTS抽象算法OGA-UCT要求状态-动作对具有相同的即时奖励，这一刚性条件限制了可发现的抽象数量，影响了样本效率。

Method: 提出KVDA框架，不再要求价值等价，而是推断价值差异，并基于此修改OGA-UCT得到KVDA-UCT算法。

Result: KVDA-UCT在各种确定性环境和参数设置下显著检测到更多抽象，性能优于OGA-UCT。

Conclusion: KVDA框架通过推断价值差异而非要求价值等价，有效提升了MCTS的抽象能力和样本效率。

Abstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,
which can be improved by grouping state-action pairs and using their aggregate
statistics instead of single-node statistics. On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS
abstraction algorithm for deterministic environments that builds its
abstraction using the Abstractions of State-Action Pairs (ASAP) framework,
which aims to detect states and state-action pairs with the same value under
optimal play by analysing the search graph. ASAP, however, requires two
state-action pairs to have the same immediate reward, which is a rigid
condition that limits the number of abstractions that can be found and thereby
the sample efficiency. In this paper, we break with the paradigm of grouping
value-equivalent states or state-action pairs and instead group states and
state-action pairs with possibly different values as long as the difference
between their values can be inferred. We call this abstraction framework Known
Value Difference Abstractions (KVDA), which infers the value differences by
analysis of the immediate rewards and modifies OGA-UCT to use this framework
instead. The modification is called KVDA-UCT, which detects significantly more
abstractions than OGA-UCT, introduces no additional parameter, and outperforms
OGA-UCT on a variety of deterministic environments and parameter settings.

</details>


### [15] [Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions](https://arxiv.org/abs/2510.25445)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.AI

TL;DR: 该论文提出了一个双范式框架，将智能体AI系统分为符号/经典范式（基于算法规划和持久状态）和神经/生成范式（基于随机生成和提示驱动编排），通过系统文献综述揭示了两种范式在不同应用领域的战略选择规律。


<details>
  <summary>Details</summary>
Motivation: 解决当前对智能体AI理解的碎片化问题，澄清现代神经系统与过时符号模型之间的概念混淆，为这一快速发展的领域提供清晰的概念框架。

Method: 采用PRISMA系统综述方法，分析了2018-2025年间的90项研究，围绕双范式框架从三个维度进行综合分析：理论基础与架构原则、领域特定实现、以及范式特定的伦理与治理挑战。

Result: 发现范式选择具有战略性：符号系统主导安全关键领域（如医疗），而神经系统在适应性强的数据丰富环境中占优（如金融）；同时识别出符号系统治理模型不足和神经符号混合架构需求等关键研究空白。

Conclusion: 智能体AI的未来不在于单一范式的支配，而在于两种范式的有意整合，以创建既适应性强又可靠的系统，为未来研究、开发和政策提供了概念工具包。

Abstract: Agentic AI represents a transformative shift in artificial intelligence, but
its rapid advancement has led to a fragmented understanding, often conflating
modern neural systems with outdated symbolic models -- a practice known as
conceptual retrofitting. This survey cuts through this confusion by introducing
a novel dual-paradigm framework that categorizes agentic systems into two
distinct lineages: the Symbolic/Classical (relying on algorithmic planning and
persistent state) and the Neural/Generative (leveraging stochastic generation
and prompt-driven orchestration). Through a systematic PRISMA-based review of
90 studies (2018--2025), we provide a comprehensive analysis structured around
this framework across three dimensions: (1) the theoretical foundations and
architectural principles defining each paradigm; (2) domain-specific
implementations in healthcare, finance, and robotics, demonstrating how
application constraints dictate paradigm selection; and (3) paradigm-specific
ethical and governance challenges, revealing divergent risks and mitigation
strategies. Our analysis reveals that the choice of paradigm is strategic:
symbolic systems dominate safety-critical domains (e.g., healthcare), while
neural systems prevail in adaptive, data-rich environments (e.g., finance).
Furthermore, we identify critical research gaps, including a significant
deficit in governance models for symbolic systems and a pressing need for
hybrid neuro-symbolic architectures. The findings culminate in a strategic
roadmap arguing that the future of Agentic AI lies not in the dominance of one
paradigm, but in their intentional integration to create systems that are both
adaptable and reliable. This work provides the essential conceptual toolkit to
guide future research, development, and policy toward robust and trustworthy
hybrid intelligent systems.

</details>


### [16] [Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?](https://arxiv.org/abs/2510.25471)
*Willem Fourie*

Main category: cs.AI

TL;DR: 本文提出AI对齐研究的新视角：将工具性目标视为需要接受和管理的特征，而非需要限制的故障


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐理论将工具性目标视为风险来源，试图限制资源获取和自我保存等症状，但作者认为这种观点需要重新审视

Method: 借鉴亚里士多德本体论及其现代解释，构建哲学论证框架，将先进AI系统视为具有特定形式和物质构成的实体

Result: 论证了AI系统的工具性倾向是其构成的内在结果，而非偶然故障，这为AI对齐提供了新的理论基础

Conclusion: AI对齐工作应更多关注理解、管理和引导工具性目标，而非试图消除它们，使其服务于人类对齐的目标

Abstract: In artificial intelligence (AI) alignment research, instrumental goals, also
called instrumental subgoals or instrumental convergent goals, are widely
associated with advanced AI systems. These goals, which include tendencies such
as power-seeking and self-preservation, become problematic when they conflict
with human aims. Conventional alignment theory treats instrumental goals as
sources of risk that become problematic through failure modes such as reward
hacking or goal misgeneralization, and attempts to limit the symptoms of
instrumental goals, notably resource acquisition and self-preservation. This
article proposes an alternative framing: that a philosophical argument can be
constructed according to which instrumental goals may be understood as features
to be accepted and managed rather than failures to be limited. Drawing on
Aristotle's ontology and its modern interpretations, an ontology of concrete,
goal-directed entities, it argues that advanced AI systems can be seen as
artifacts whose formal and material constitution gives rise to effects distinct
from their designers' intentions. In this view, the instrumental tendencies of
such systems correspond to per se outcomes of their constitution rather than
accidental malfunctions. The implication is that efforts should focus less on
eliminating instrumental goals and more on understanding, managing, and
directing them toward human-aligned ends.

</details>


### [17] [Multi-Objective Search: Algorithms, Applications, and Emerging Directions](https://arxiv.org/abs/2510.25504)
*Oren Salzman,Carlos Hernández Ulloa,Ariel Felner,Sven Koenig*

Main category: cs.AI

TL;DR: 多目标搜索作为规划与决策的统一框架，近年来在AI应用中重新受到关注，本文综述了该领域的发展并指出跨学科机遇和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统很少只优化单一指标，多目标搜索能够平衡多个冲突标准，在机器人、交通和运筹学等AI应用中具有重要价值。

Method: 本文采用综述研究方法，系统梳理多目标搜索领域的发展历程和最新进展。

Result: 识别出多目标搜索在跨学科领域的机遇，并明确了该新兴前沿领域面临的开放挑战。

Conclusion: 多目标搜索作为处理多标准决策问题的统一框架，具有广阔的应用前景，但仍需解决诸多挑战以推动该领域发展。

Abstract: Multi-objective search (MOS) has emerged as a unifying framework for planning
and decision-making problems where multiple, often conflicting, criteria must
be balanced. While the problem has been studied for decades, recent years have
seen renewed interest in the topic across AI applications such as robotics,
transportation, and operations research, reflecting the reality that real-world
systems rarely optimize a single measure. This paper surveys developments in
MOS while highlighting cross-disciplinary opportunities, and outlines open
challenges that define the emerging frontier of MOS

</details>


### [18] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: 提出了MTIR-SQL框架，这是一个多轮工具集成推理的强化学习框架，用于Text-to-SQL任务，通过动态执行反馈和多轮推理显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖静态执行反馈，限制了实时错误纠正能力。整合多轮工具调用和动态反馈可以显著提高模型的适应性和鲁棒性。

Method: 提出了执行感知的多轮推理范式，在每个推理步骤中无缝集成数据库执行反馈，实现上下文敏感的查询生成和渐进式优化。扩展了GRPO算法以适应复杂多轮交互场景，并增加了轨迹过滤机制和移除KL损失约束。

Result: MTIR-SQL在BIRD Dev上达到64.4%的准确率，在SPIDER Dev上达到84.6%的执行准确率，显著优于现有方法。

Conclusion: MTIR-SQL框架通过多轮工具集成推理和动态反馈机制，有效提升了Text-to-SQL任务的性能，证明了该方法在复杂数据库查询生成中的有效性。

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [19] [Predicate Renaming via Large Language Models](https://arxiv.org/abs/2510.25517)
*Elisabetta Gentili,Tony Ribeiro,Fabrizio Riguzzi,Katsumi Inoue*

Main category: cs.AI

TL;DR: 使用大型语言模型为逻辑规则中的未命名谓词生成有意义的名称，以提高逻辑理论的可读性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在归纳逻辑编程中，各种规则生成方法会产生包含未命名谓词的规则，这阻碍了逻辑理论的可读性、可解释性和可重用性。

Method: 利用LLMs处理自然语言和代码的能力，为未命名谓词提供语义上有意义的命名建议。

Result: 在手工制作的逻辑规则上评估表明，LLMs在这项任务中具有潜力。

Conclusion: LLMs显示出为逻辑规则中未命名谓词生成有意义名称的潜力，有助于改善逻辑理论的可读性和可解释性。

Abstract: In this paper, we address the problem of giving names to predicates in logic
rules using Large Language Models (LLMs). In the context of Inductive Logic
Programming, various rule generation methods produce rules containing unnamed
predicates, with Predicate Invention being a key example. This hinders the
readability, interpretability, and reusability of the logic theory. Leveraging
recent advancements in LLMs development, we explore their ability to process
natural language and code to provide semantically meaningful suggestions for
giving a name to unnamed predicates. The evaluation of our approach on some
hand-crafted logic rules indicates that LLMs hold potential for this task.

</details>


### [20] [Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation](https://arxiv.org/abs/2510.25518)
*Thomas Cook,Richard Osuagwu,Liman Tsatiashvili,Vrynsia Vrynsia,Koustav Ghosal,Maraim Masoud,Riccardo Mattivi*

Main category: cs.AI

TL;DR: 本文提出了一种面向金融科技领域的智能RAG架构，通过模块化代理管道解决专业领域检索挑战，在检索精度和相关性方面优于标准RAG基线。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在金融科技等专业领域面临挑战，包括领域特定本体、密集术语和缩略语，这些因素影响了检索和合成的有效性。

Method: 采用模块化代理架构，支持智能查询重构、基于关键词提取的迭代子查询分解、上下文缩略语解析以及基于交叉编码器的上下文重排序。

Result: 在85个问答参考三元组数据集上的实验表明，该系统在检索精度和相关性方面优于标准RAG基线，但延迟有所增加。

Conclusion: 结构化、多代理方法为增强复杂领域特定环境中的检索鲁棒性提供了有前景的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems often face limitations in
specialized domains such as fintech, where domain-specific ontologies, dense
terminology, and acronyms complicate effective retrieval and synthesis. This
paper introduces an agentic RAG architecture designed to address these
challenges through a modular pipeline of specialized agents. The proposed
system supports intelligent query reformulation, iterative sub-query
decomposition guided by keyphrase extraction, contextual acronym resolution,
and cross-encoder-based context re-ranking. We evaluate our approach against a
standard RAG baseline using a curated dataset of 85 question--answer--reference
triples derived from an enterprise fintech knowledge base. Experimental results
demonstrate that the agentic RAG system outperforms the baseline in retrieval
precision and relevance, albeit with increased latency. These findings suggest
that structured, multi-agent methodologies offer a promising direction for
enhancing retrieval robustness in complex, domain-specific settings.

</details>


### [21] [Zero Reinforcement Learning Towards General Domains](https://arxiv.org/abs/2510.25528)
*Yuyuan Zeng,Yufei Huang,Can Xu,Qingfeng Sun,Jianfeng Yan,Guanghui Xu,Tao Yang,Fengzong Lian*

Main category: cs.AI

TL;DR: 提出了一种新的零强化学习范式，通过结合可验证奖励和生成式奖励模型，在可验证和不可验证领域进行多任务训练，提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前零强化学习主要关注易于验证奖励的领域（如数学、编程），但在验证不直接的其他多样化场景中激发推理能力的研究不足。

Method: 使用可验证奖励与生成式奖励模型相结合，进行跨领域的多任务零强化学习训练，并设计了平滑长度惩罚来防止奖励黑客攻击。

Result: 在Qwen3-8B-Base和Qwen3-14B-Base上的实验表明，该方法在需要大量推理的任务和更一般的任务上都取得了优越的推理性能。

Conclusion: 该零强化学习范式能够有效提升模型在可验证和不可验证领域的推理能力，实现了推理能力在不同领域间的迁移。

Abstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model's
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.

</details>


### [22] [Off-policy Reinforcement Learning with Model-based Exploration Augmentation](https://arxiv.org/abs/2510.25529)
*Likun Wang,Xiangteng Zhang,Yinuo Wang,Guojian Zhan,Wenxuan Wang,Haoyu Gao,Jingliang Duan,Shengbo Eben Li*

Main category: cs.AI

TL;DR: 提出MoGE方法，通过生成未充分探索的关键状态和动态一致的体验来增强强化学习中的探索能力


<details>
  <summary>Details</summary>
Motivation: 现有探索方法存在局限性：主动探索在高维环境中表现不佳，被动探索受限于样本多样性不足

Method: MoGE包含两个组件：基于扩散模型的关键状态生成器（评估状态对策略探索的潜在影响）和一步想象世界模型（构建关键转移用于智能体学习）

Result: 在OpenAI Gym和DeepMind Control Suite上的实验表明，MoGE有效连接探索与策略学习，在复杂控制任务中显著提升了样本效率和性能

Conclusion: MoGE采用模块化设计，可与现有算法无缝集成，在不改变核心结构的情况下改善探索效果

Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines
how effectively an agent discovers and exploits the underlying structure of its
environment to achieve optimal performance. Existing exploration methods
generally fall into two categories: active exploration and passive exploration.
The former introduces stochasticity into the policy but struggles in
high-dimensional environments, while the latter adaptively prioritizes
transitions in the replay buffer to enhance exploration, yet remains
constrained by limited sample diversity. To address the limitation in passive
exploration, we propose Modelic Generative Exploration (MoGE), which augments
exploration through the generation of under-explored critical states and
synthesis of dynamics-consistent experiences through transition models. MoGE is
composed of two components: (1) a diffusion-based generator that synthesizes
critical states under the guidance of a utility function evaluating each
state's potential influence on policy exploration, and (2) a one-step
imagination world model for constructing critical transitions based on the
critical states for agent learning. Our method adopts a modular formulation
that aligns with the principles of off-policy learning, allowing seamless
integration with existing algorithms to improve exploration without altering
their core structures. Empirical results on OpenAI Gym and DeepMind Control
Suite reveal that MoGE effectively bridges exploration and policy learning,
leading to remarkable gains in both sample efficiency and performance across
complex control tasks.

</details>


### [23] [Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2510.25588)
*Eranga Bandara,Ross Gore,Atmaram Yarlagadda,Anita H. Clayton,Preston Samuel,Christopher K. Rhea,Sachin Shetty*

Main category: cs.AI

TL;DR: 提出一个基于微调大语言模型联盟和OpenAI-gpt-oss推理LLM的决策支持系统，用于精神障碍的临床诊断，通过共识决策过程提高诊断准确性和标准化。


<details>
  <summary>Details</summary>
Motivation: 精神障碍诊断主要依赖医患对话，这种主观过程导致诊断结果在不同医生和患者之间存在差异，难以获得可靠结果。

Method: 利用在精神科医患对话数据集上微调的LLMs，通过共识决策过程聚合各模型的诊断预测，并由OpenAI-gpt-oss推理LLM进行精炼，部署LLM代理协调整个诊断流程。

Result: 实验结果表明，结合微调LLMs和推理模型能够创建稳健且高精度的精神健康评估诊断系统。

Conclusion: 这是首个将微调LLM联盟与推理LLM集成应用于临床精神健康诊断的工作，为下一代AI驱动的eHealth系统标准化精神科诊断开辟了道路。

Abstract: The diagnosis of most mental disorders, including psychiatric evaluations,
primarily depends on dialogues between psychiatrists and patients. This
subjective process can lead to variability in diagnoses across clinicians and
patients, resulting in inconsistencies and challenges in achieving reliable
outcomes. To address these issues and standardize psychiatric diagnoses, we
propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss
Reasoning LLM-enabled Decision Support System for the clinical diagnosis of
mental disorders. Our approach leverages fine-tuned LLMs trained on
conversational datasets involving psychiatrist-patient interactions focused on
mental health conditions (e.g., depression). The diagnostic predictions from
individual models are aggregated through a consensus-based decision-making
process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method
for deploying LLM agents that orchestrate communication between the LLM
consortium and the reasoning LLM, ensuring transparency, reliability, and
responsible AI across the entire diagnostic workflow. Experimental results
demonstrate the transformative potential of combining fine-tuned LLMs with a
reasoning model to create a robust and highly accurate diagnostic system for
mental health assessment. A prototype of the proposed platform, integrating
three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in
collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia,
USA. To the best of our knowledge, this work represents the first application
of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical
mental health diagnosis paving the way for next-generation AI-powered eHealth
systems aimed at standardizing psychiatric diagnoses.

</details>


### [24] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: 提出了首个评估智能体AI工作流中各个智能体影响力的方法CAIR，通过反事实分析在推理时动态评估智能体对最终输出的影响程度。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI工作流的广泛应用，需要深入理解其运行机制，但目前缺乏评估单个智能体对最终输出影响的方法。

Method: 采用反事实分析方法，通过修改智能体的输出来评估其对工作流最终结果的影响程度，提供任务无关的分析能力。

Result: 在包含30个用例、230个功能的AAW数据集上评估，CAIR能产生一致的排名，优于基线方法，并能有效提升下游任务的效果。

Conclusion: CAIR是首个能够动态评估智能体AI工作流中智能体影响力的方法，为理解和优化多智能体系统提供了重要工具。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>


### [25] [ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents](https://arxiv.org/abs/2510.25668)
*Tianyu Yang,Terry Ruas,Yijun Tian,Jan Philip Wahle,Daniel Kurzawe,Bela Gipp*

Main category: cs.AI

TL;DR: ALDEN是一个多轮强化学习框架，通过将视觉语言模型训练为交互式代理，使其能够主动导航长文档。它引入了按索引直接访问页面的fetch动作，并提出了跨级别奖励和视觉语义锚定机制来解决训练稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在处理需要跨多页分析和整合信息的复杂长文档时表现不佳，通常依赖固定的推理模板或刚性流程，限制了模型的效率和泛化能力。

Method: ALDEN采用多轮强化学习框架，训练视觉语言模型作为交互代理。引入fetch动作直接按索引访问页面，提出基于规则的跨级别奖励进行密集过程监督，并使用视觉语义锚定机制通过双路径KL散度约束稳定训练。

Result: 在三个开源数据集构建的语料库上训练后，ALDEN在五个长文档基准测试中取得了最先进的性能。

Conclusion: ALDEN标志着从被动文档阅读向能够自主导航和推理长视觉丰富文档的代理迈出了一步，为更准确和高效的长文档理解提供了稳健路径。

Abstract: Vision-language models (VLMs) excel at interpreting text-rich images but
struggle with long, visually complex documents that demand analysis and
integration of information spread across multiple pages. Existing approaches
typically rely on fixed reasoning templates or rigid pipelines, which force
VLMs into a passive role and hinder both efficiency and generalization. We
present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement
learning framework that fine-tunes VLMs as interactive agents capable of
actively navigating long, visually rich documents. ALDEN introduces a novel
fetch action that directly accesses the page by index, complementing the
classic search action and better exploiting document structure. For dense
process supervision and efficient training, we propose a rule-based cross-level
reward that provides both turn- and token-level signals. To address the
empirically observed training instability caused by numerous visual tokens from
long documents, we further propose a visual-semantic anchoring mechanism that
applies a dual-path KL-divergence constraint to stabilize visual and textual
representations separately during training. Trained on a corpus constructed
from three open-source datasets, ALDEN achieves state-of-the-art performance on
five long-document benchmarks. Overall, ALDEN marks a step beyond passive
document reading toward agents that autonomously navigate and reason across
long, visually rich documents, offering a robust path to more accurate and
efficient long-document understanding.

</details>


### [26] [Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning](https://arxiv.org/abs/2510.25679)
*Federica Tonti,Ricardo Vinuesa*

Main category: cs.AI

TL;DR: 基于深度强化学习的无人机最优导航策略，在三维城市湍流环境中使用PPO+GTrXL架构，相比传统方法显著提升成功率并降低碰撞率


<details>
  <summary>Details</summary>
Motivation: 随着无人机在城市配送和监控中的普及，需要开发能够在复杂湍流城市环境中有效导航的智能算法

Method: 采用结合GTrXL架构的流感知PPO算法，为无人机提供更丰富的湍流场信息，并与PPO+LSTM、PPO+GTrXL无预测任务版本以及传统Zermelo导航算法进行对比

Result: 相比PPO+LSTM、PPO+GTrXL和传统Zermelo算法，该方法显著提高了成功率(SR)并降低了碰撞率(CR)

Conclusion: 该方法为复杂城市环境中无人机导航开辟了新途径，有望彻底改变无人机在城市环境中的应用格局

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for
delivery and surveillance purposes. In this work, we develop an optimal
navigation strategy based on Deep Reinforcement Learning. The environment is
represented by a three-dimensional high-fidelity simulation of an urban flow,
characterized by turbulence and recirculation zones. The algorithm presented
here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated
Transformer eXtra Large (GTrXL) architecture, giving the agent richer
information about the turbulent flow field in which it navigates. The results
are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO
combined with Long Short Term Memory (LSTM) cells and a traditional navigation
algorithm. The obtained results show a significant increase in the success rate
(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the
classical Zermelo's navigation algorithm, paving the way to a completely
reimagined UAV landscape in complex urban environments.

</details>


### [27] [BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph](https://arxiv.org/abs/2510.25724)
*Vanya Arikutharam,Arkadiy Ukolov*

Main category: cs.AI

TL;DR: BambooKG是一种带频率权重的知识图谱，通过非三元组边减少信息损失，在单跳和多跳推理任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法独立处理检索块，难以进行多跳或关系推理；传统知识图谱因三元组结构限制会丢失非结构化信息。

Method: 提出BambooKG知识图谱，在非三元组边上应用基于频率的权重，体现链接强度，借鉴赫布原理。

Result: 减少了信息损失，在单跳和多跳推理任务上表现优于现有解决方案。

Conclusion: BambooKG通过频率加权的非三元组边有效提升了知识图谱的推理能力。

Abstract: Retrieval-Augmented Generation allows LLMs to access external knowledge,
reducing hallucinations and ageing-data issues. However, it treats retrieved
chunks independently and struggles with multi-hop or relational reasoning,
especially across documents. Knowledge graphs enhance this by capturing the
relationships between entities using triplets, enabling structured, multi-chunk
reasoning. However, these tend to miss information that fails to conform to the
triplet structure. We introduce BambooKG, a knowledge graph with
frequency-based weights on non-triplet edges which reflect link strength,
drawing on the Hebbian principle of "fire together, wire together". This
decreases information loss and results in improved performance on single- and
multi-hop reasoning, outperforming the existing solutions.

</details>


### [28] [TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling](https://arxiv.org/abs/2510.25758)
*He Hu,Yucheng Zhou,Chiyuan Ma,Qianning Wang,Zheng Zhang,Fei Ma,Laizhong Cui,Qi Tian*

Main category: cs.AI

TL;DR: TheraMind是一个用于长期心理咨询的战略性自适应代理，采用双循环架构解决现有LLM在心理咨询中缺乏情感理解、自适应策略和跨会话记忆的问题。


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询LLM方法缺乏情感理解、自适应策略和跨会话治疗方法的长期记忆，与真实临床实践差距较大。

Method: 提出新颖的双循环架构：会话内循环用于战术对话管理，感知患者情绪状态并动态选择响应策略；跨会话循环用于战略治疗规划，评估治疗效果并调整后续方法。

Result: 在高保真模拟环境中验证，TheraMind在多会话指标（如连贯性、灵活性和治疗协调性）上优于其他方法，证明双循环设计的有效性。

Conclusion: TheraMind的双循环设计能够有效模拟战略性、自适应和长期的治疗行为，在心理咨询领域具有重要应用价值。

Abstract: Large language models (LLMs) in psychological counseling have attracted
increasing attention. However, existing approaches often lack emotional
understanding, adaptive strategies, and the use of therapeutic methods across
multiple sessions with long-term memory, leaving them far from real clinical
practice. To address these critical gaps, we introduce TheraMind, a strategic
and adaptive agent for longitudinal psychological counseling. The cornerstone
of TheraMind is a novel dual-loop architecture that decouples the complex
counseling process into an Intra-Session Loop for tactical dialogue management
and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session
Loop perceives the patient's emotional state to dynamically select response
strategies while leveraging cross-session memory to ensure continuity.
Crucially, the Cross-Session Loop empowers the agent with long-term
adaptability by evaluating the efficacy of the applied therapy after each
session and adjusting the method for subsequent interactions. We validate our
approach in a high-fidelity simulation environment grounded in real clinical
cases. Extensive evaluations show that TheraMind outperforms other methods,
especially on multi-session metrics like Coherence, Flexibility, and
Therapeutic Attunement, validating the effectiveness of its dual-loop design in
emulating strategic, adaptive, and longitudinal therapeutic behavior. The code
is publicly available at https://0mwwm0.github.io/TheraMind/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo协议利用群体智能和分布式成对排序共识，在AI推理中实现优于多数投票的性能，在GPQA Diamond上达到85.90%准确率，比多数投票高17.21个百分点。


<details>
  <summary>Details</summary>
Motivation: 随着集中式AI面临计算瓶颈和训练规模收益递减，需要能够水平和垂直扩展的推理层来满足需求。

Method: 采用群体推理方法，使用成对排序和Bradley-Terry风格聚合模型，结合链上声誉系统和能力证明共识机制。

Result: 在六个基准测试中表现优异，GPQA Diamond准确率达85.90%，对对抗性提示具有强韧性，提示注入性能下降仅0.12%。

Conclusion: 为去中心化AI系统奠定了基础，通过集体智能实现高质量推理的民主化，同时保持可靠性和安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [30] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 该论文分析了从弱线性CNN到强两层ReLU CNN的弱到强泛化现象，识别了基于数据信噪比的数据稀缺和数据丰富两种机制，并揭示了泛化通过良性过拟合或标签修正实现，但也可能因有害过拟合或过度训练而退化。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多局限于抽象框架或线性/随机特征模型，缺乏对弱到强泛化现象在具体CNN架构中的正式分析。

Method: 使用包含不同难度标签相关信号和标签无关噪声的结构化数据，分析强模型在弱模型标注数据上训练时的梯度下降动态。

Result: 识别了数据稀缺和数据丰富两种机制：数据稀缺时泛化通过良性过拟合实现，数据丰富时通过早期标签修正实现，但过度训练会降低性能。

Conclusion: 弱到强泛化在不同数据机制下有不同的表现，需要根据数据特征选择合适的训练策略以避免性能退化。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [31] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA是一个Python框架，用于从突变数据构建和分析适应性景观，计算20个生物学相关特征，帮助解释和比较适应性预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏关于基础适应性景观的地形信息，这阻碍了模型性能的解释和比较。

Method: GraphFLA从DNA、RNA、蛋白质等不同模态的突变数据构建适应性景观，计算20个特征来表征景观地形的4个基本方面。

Result: 应用于5,300多个景观，展示了其在解释和比较数十个适应性预测模型性能方面的实用性，并发布了155个组合完整的经验适应性景观。

Conclusion: GraphFLA提供了一个强大的工具来表征适应性景观地形，促进模型性能的深入理解和比较。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [32] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: 该研究评估了在ESP32-S3微控制器上使用低功耗广域网络和压缩CNN进行环境监测，通过设备端推理减少数据传输，能耗降低高达5倍。


<details>
  <summary>Details</summary>
Motivation: 环境监测需要能在偏远地区长期运行的节能物联网设备，特别是处理图像数据时，数据传输能耗高成为主要挑战。

Method: 在ESP32-S3微控制器上部署压缩的CNN模型进行设备端推理，仅传输推理结果而非原始图像数据，结合低功耗广域网络传输。

Result: 设备端执行CNN推理并仅传输结果，相比发送原始图像数据，整体能耗降低高达5倍；模型量化仅导致精度轻微下降几个百分点。

Conclusion: 嵌入式机器学习可开发碳足迹更小的物联网应用，实现环境监测场景的自主运行，设备端推理是节能的关键策略。

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [33] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 论文发现OOD泛化基准中ID和OOD准确率之间的正相关关系可能是聚合异质OOD样本的假象，通过OODSelect方法识别出语义一致的OOD子集，在这些子集中更高的ID准确率反而预测更低的OOD准确率。


<details>
  <summary>Details</summary>
Motivation: 当前OOD泛化基准普遍显示ID和OOD准确率呈正相关，这被认为意味着虚假相关性在现实中很少见。但作者怀疑这种正相关可能是聚合异质OOD样本造成的假象。

Method: 提出基于梯度的OODSelect方法，用于识别语义一致的OOD子集，在这些子集中ID和OOD准确率的关系与整体趋势不同。

Result: 在广泛使用的分布偏移基准中，OODSelect发现了大量子集（有时超过标准OOD集的一半），其中更高的ID准确率预测更低的OOD准确率。

Conclusion: 聚合指标可能掩盖OOD鲁棒性的重要失效模式，需要更细粒度的分析来理解模型在特定OOD子集上的表现。

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [34] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应多任务EEG分类器，使用GRU-TCN网络和DQN阈值调整，在卒中快速分诊中实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 卒中快速分诊需要准确、床旁可部署的工具，EEG有潜力但在首次接触时使用不足。

Method: 将32通道EEG信号转换为功率谱密度特征，使用GRU-TCN网络预测卒中类型、半球偏侧化和严重程度，并应用DQN实时调整决策阈值。

Result: 基线GRU-TCN在卒中类型分类上达到89.3%准确率，严重程度96.9%，偏侧化96.7%；加入DQN阈值适应后，卒中类型准确率提升至98.0%。

Conclusion: 自适应阈值调整可将操作点移至临床偏好的敏感度-特异度权衡，同时集成的头皮图谱和频谱可视化支持可解释性。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [35] [Topic Analysis with Side Information: A Neural-Augmented LDA Approach](https://arxiv.org/abs/2510.24918)
*Biyi Fang,Kripa Rajshekhar,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: 提出了nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息，在多个基准数据集上优于传统LDA和Dirichlet-Multinomial回归。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型如LDA难以整合元数据、用户属性或文档标签等辅助信息，限制了其表达能力、个性化和可解释性。

Method: nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成，开发了随机变分期望最大化算法来联合优化神经和概率组件。

Result: 在多个基准数据集上，nnLDA在主题连贯性、困惑度和下游分类任务中持续优于LDA和Dirichlet-Multinomial回归。

Conclusion: 在存在辅助信息的情况下，将神经表示学习与概率主题建模相结合具有显著优势。

Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been
widely used to uncover latent structures in text corpora, but they often
struggle to integrate auxiliary information such as metadata, user attributes,
or document labels. These limitations restrict their expressiveness,
personalization, and interpretability. To address this, we propose nnLDA, a
neural-augmented probabilistic topic model that dynamically incorporates side
information through a neural prior mechanism. nnLDA models each document as a
mixture of latent topics, where the prior over topic proportions is generated
by a neural network conditioned on auxiliary features. This design allows the
model to capture complex nonlinear interactions between side information and
topic distributions that static Dirichlet priors cannot represent. We develop a
stochastic variational Expectation-Maximization algorithm to jointly optimize
the neural and probabilistic components. Across multiple benchmark datasets,
nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in
topic coherence, perplexity, and downstream classification. These results
highlight the benefits of combining neural representation learning with
probabilistic topic modeling in settings where side information is available.

</details>


### [36] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN是一种快速准确的冰盖建模模拟器，将Kolmogorov-Arnold网络作为特征校准器置于图卷积网络之前，通过可学习的一维扭曲和线性混合步骤改善特征条件化和非线性编码，无需增加消息传递深度。


<details>
  <summary>Details</summary>
Motivation: 提高冰盖数值模型模拟器的性能，通过改进特征条件化和非线性编码来提升模拟精度和效率。

Method: 使用KAN作为前端特征校准器，结合图卷积网络，在Pine Island Glacier的36个融化率模拟数据集上进行训练和测试，比较2-5层架构的性能。

Result: KAN-GCN在2-5层架构中匹配或超越了纯GCN和MLP-GCN基线的精度，在较粗网格上通过用节点级变换替换边缘消息传递层提高了推理吞吐量，仅在最细网格上产生适度成本。

Conclusion: KAN优先设计为大型瞬态场景扫描提供了有利的精度与效率权衡。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [37] [WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning](https://arxiv.org/abs/2510.24927)
*Joel Frank Huarayo Quispe,Lilian Berton,Didier Vega-Oliveros*

Main category: cs.LG

TL;DR: 提出WBT-BGRL框架，通过加权三元组损失增强自举学习，用于二分图的归纳式链接预测


<details>
  <summary>Details</summary>
Motivation: 二分图链接预测在推荐系统和故障检测中很重要，但现有方法在归纳、加权和二分场景下的有效性未经验证，对比方法存在负采样效率低和偏差问题

Method: 使用双GCN编码器的二分架构，通过加权三元组损失增强自举学习，是非对比性框架

Result: 在真实数据集（工业和电商）上表现具有竞争力，特别是在预训练阶段应用加权时效果显著

Conclusion: 加权非对比学习对二分图归纳链接预测具有重要价值

Abstract: Link prediction in bipartite graphs is crucial for applications like
recommendation systems and failure detection, yet it is less studied than in
monopartite graphs. Contrastive methods struggle with inefficient and biased
negative sampling, while non-contrastive approaches rely solely on positive
samples. Existing models perform well in transductive settings, but their
effectiveness in inductive, weighted, and bipartite scenarios remains untested.
To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph
Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped
learning with a novel weighting mechanism in the triplet loss. Using a
bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against
adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on
real-world datasets (Industry and E-commerce) show competitive performance,
especially when weighting is applied during pretraining-highlighting the value
of weighted, non-contrastive learning for inductive link prediction in
bipartite graphs.

</details>


### [38] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 研究发现LLMs生成的思维链中大部分推理步骤是装饰性的，对最终预测没有实质性因果影响，只有少量步骤真正驱动模型预测。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs生成的思维链常被假设为模型内部思考过程的忠实反映，但作者发现许多推理步骤实际上并不真正影响模型预测，这影响了LLM推理的效率和思维链的可信度。

Method: 提出True Thinking Score (TTS)来测量每个推理步骤对最终预测的因果影响，并识别了LLMs潜在空间中的TrueThinking方向，通过沿该方向引导可以控制模型执行或忽略特定推理步骤。

Result: 在AIME数据集上，Qwen-2.5模型中只有平均2.3%的推理步骤TTS≥0.7；自我验证步骤也可能是装饰性的，沿TrueThinking方向引导可以改变最终结果。

Conclusion: LLMs经常口头表达推理步骤而不在内部真正执行，这削弱了LLM推理的效率和思维链的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [39] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 该论文研究了视觉语言模型中存在的文化敏感神经元，这些神经元对特定文化背景的输入表现出选择性敏感。通过CVQA基准测试，作者识别出文化选择性神经元，并通过消融实验验证其重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型表现优异，但在处理文化相关输入时仍存在困难。为了理解模型如何处理文化基础信息，研究者希望探索是否存在对特定文化背景敏感的特殊神经元。

Method: 使用CVQA基准测试识别文化选择性神经元，进行因果测试通过停用不同识别方法标记的神经元。提出新的基于边际的选择器——对比激活选择(CAS)，并在三个VLMs和25个文化群体上进行实验。

Result: 实验证明存在这样的神经元：消融这些神经元会显著损害对应文化问题的性能，而对其他文化影响最小。CAS方法在识别文化敏感神经元方面优于现有的概率和熵基方法。层分析显示这些神经元倾向于聚集在某些解码器层中。

Conclusion: 研究揭示了多模态表征的内部组织结构，发现了文化敏感神经元的存在及其在特定解码器层中的聚集特性，为理解VLMs如何处理文化信息提供了新视角。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [40] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: 该论文提出通过算法-硬件协同设计的方法，同时提升神经网络的计算效率和可靠性，包括模型压缩、近似贝叶斯推理、数字/模拟硬件优化等技术。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习在嵌入式等资源受限平台上存在计算需求大、效率低的问题，且需要处理分布偏移和未知数据的可靠预测。贝叶斯神经网络能量化不确定性但计算开销更大。

Method: 1) Galen系统通过敏感度分析和硬件反馈进行自动分层压缩；2) 模拟加速器建模设备缺陷并扩展噪声训练；3) 开发解析和集成近似方法替代昂贵采样；4) 概率光子计算利用模拟噪声作为熵源实现硬件概率推理。

Result: 实现了资源高效且鲁棒的推理，在算法和硬件层面都获得了效率提升，提高了在噪声和非平稳条件下的鲁棒性和稳定性。

Conclusion: 通过算法-硬件协同设计可以同时推进效率和可靠性，为下一代可信赖、高能效的机器学习系统奠定基础。

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [41] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型的低维结构，发现现代语言模型具有低秩特性，可以利用不相关甚至无意义的提示生成目标响应的线性组合。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型的固有低维结构是一个重要问题，作者希望从模型无关的角度研究语言模型作为序列概率模型的低维结构。

Method: 通过实证研究展示现代语言模型具有低秩结构，并利用这种结构进行生成——使用不相关或无意义提示的模型输出线性组合来生成目标提示的响应。

Result: 实验表明广泛的语言模型都表现出低秩结构，这种结构可以用于生成任务，理论分析也支持这一发现。

Conclusion: 语言模型的低秩结构提供了一个简单的通用抽象，其理论预测与实验结果一致，为理解语言模型的内在结构提供了新视角。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [42] [Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution](https://arxiv.org/abs/2510.24974)
*Mia Adler,Carrie Liang,Brian Peng,Oleg Presnyakov,Justin M. Baker,Jannelle Lauffer,Himani Sharma,Barry Merriman*

Main category: cs.LG

TL;DR: 提出了一种基于秩条件委员会（RCC）的机器学习辅助定向进化框架，通过为每个构象秩分配深度神经网络委员会，实现了构象不确定性与认知不确定性的原则性分离。


<details>
  <summary>Details</summary>
Motivation: 现有的结构感知MLDE方法通常依赖单一构象或单一委员会，无法有效区分构象不确定性和认知不确定性，限制了抗体适应性景观的探索效率。

Method: 引入秩条件委员会框架，利用排序构象为每个秩分配一个深度神经网络委员会，从而在原理上分离构象不确定性和认知不确定性。

Result: 在SARS-CoV-2抗体对接任务上验证了该方法的有效性，相比基线策略取得了显著改进。

Conclusion: 该方法为治疗性抗体发现提供了可扩展的途径，同时直接解决了构象不确定性建模的挑战。

Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for
efficiently navigating antibody fitness landscapes. Many structure-aware MLDE
pipelines rely on a single conformation or a single committee across all
conformations, limiting their ability to separate conformational uncertainty
from epistemic uncertainty. Here, we introduce a rank -conditioned committee
(RCC) framework that leverages ranked conformations to assign a deep neural
network committee per rank. This design enables a principled separation between
epistemic uncertainty and conformational uncertainty. We validate our approach
on SARS-CoV-2 antibody docking, demonstrating significant improvements over
baseline strategies. Our results offer a scalable route for therapeutic
antibody discovery while directly addressing the challenge of modeling
conformational uncertainty.

</details>


### [43] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出基于博弈论的端到端特征选择框架，通过评估特征的协同交互和边际贡献来确定重要性，在保持预测性能的同时大幅减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 数据量指数增长导致机器学习模型训练计算成本激增，许多特征对模型性能无正面贡献却消耗大量计算资源。

Method: 基于合作博弈论构建特征选择框架，将特征建模为玩家，通过评估协同交互和边际贡献确定特征重要性，包含样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练四个核心组件。

Result: 实验结果表明该方法在保持预测性能的同时实现了显著的计算减少。

Conclusion: 该框架为大规模机器学习计算挑战提供了高效解决方案，源代码已开源。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [44] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种风险感知的采样方法，通过将每个去噪步骤视为无条件先验和状态条件策略头之间的顺序假设检验，为扩散策略添加了统计风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略在离线强化学习中通常使用缺乏统计风险概念的启发式采样规则，需要一种具有可解释风险预算的风险感知方法。

Method: 使用对数似然比累积，并通过逻辑控制器门控条件均值，阈值τ在校准后满足用户指定的Type-I水平α。该方法保持训练过程不变，仅在推理时应用。

Result: 在D4RL MuJoCo任务上，LRT-Diffusion在保持期望α水平的同时，相比强Q引导基线改善了回报与分布外度量的权衡。

Conclusion: LRT-Diffusion是一种即插即用的推理时方法，为离线RL的扩散策略添加了原则性、校准的风险控制，特别在离支撑误差占主导时优于Q引导。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [45] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 该研究提出了一种结合实时癫痫检测和预测的新方法，使用多种机器学习算法在CHB-MIT头皮EEG数据库上进行评估，其中逻辑回归在检测方面表现平衡，LSTM在预测方面达到89.26%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅在癫痫发作后检测，限制了早期干预机会。本研究旨在开发能够检测和预测癫痫发作的系统，实现从被动管理向主动预防的转变。

Method: 使用CHB-MIT头皮EEG数据库（969小时记录，173次癫痫发作），采用K近邻、逻辑回归、随机森林、支持向量机进行癫痫检测，使用LSTM网络进行癫痫预测。

Result: 逻辑回归检测准确率90.9%，召回率89.6%；随机森林和SVM准确率94.0%但召回率为0%；LSTM预测准确率89.26%。

Conclusion: 研究证明了开发可访问的实时监测工具的潜力，不仅能够检测癫痫发作，还能预测其发生，标志着从被动管理向主动预防的重要转变。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [46] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 提出了一种将自监督Transformer变化点检测模块集成到Option-Critic框架中的新架构，通过自适应轨迹分割和选项发现来改进分层强化学习。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习在长时程任务中面临自主发现语义子目标和学习最优选项终止边界的挑战，现有方法在实际应用中效果有限。

Method: 使用基于Transformer的变化点检测模块，通过内在信号生成启发式伪标签来推断环境动态的潜在变化，并将检测到的变化点用于稳定终止函数梯度、预训练内部选项策略和强制功能专业化。

Result: 在Four-Rooms和Pinball任务上的实验表明，CPD引导的智能体表现出加速收敛、更高累积回报和显著改进的选项专业化。

Conclusion: 通过变化点分割整合结构先验可以在复杂环境中产生更可解释、样本高效和鲁棒的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [47] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文系统解构了矩阵白化优化器，发现方差自适应是解释其性能优势的关键因素，而非仅靠谱归一化。


<details>
  <summary>Details</summary>
Motivation: 研究各种近似矩阵白化变换的优化器，旨在揭示其性能优势的关键组成成分。

Method: 通过系统解构矩阵白化优化器，对比不同变体在调优超参数下的表现，并进行方差自适应策略的消融实验。

Result: 所有矩阵白化方法都可靠地优于逐元素优化器如Adam；方差自适应版本始终优于符号下降对应版本；低秩方差估计器可有效降低内存成本而无性能损失。

Conclusion: 矩阵白化优化器的性能优势主要来自方差自适应组件，而非仅靠准确的谱归一化；低秩方差估计是有效的内存优化策略。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [48] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE是一种深度多编码器自编码器，能够将多区域神经记录分解为共享和私有潜在子空间，通过新颖的对齐和解缠损失来恢复跨区域结构并揭示外部扰动如何重组这些结构。


<details>
  <summary>Details</summary>
Motivation: 在多区域神经数据建模中，分离共享网络级动态与区域特定活动是一个核心挑战。

Method: 引入SPIRE（共享-私有跨区域编码器），这是一种深度多编码器自编码器，使用新颖的对齐和解缠损失将记录分解为共享和私有潜在子空间。仅使用基线数据进行训练。

Result: 在具有真实潜在变量的合成基准测试中，SPIRE在非线性扭曲和时间错位下优于经典概率模型。应用于颅内深部脑刺激记录时，SPIRE显示共享潜在变量可靠地编码了跨位点和频率可泛化的刺激特定特征。

Conclusion: SPIRE被确立为分析刺激下多区域神经动态的实用、可复现工具。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [49] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 基于放射组学的机器学习模型在临床决策支持中具有潜力，但容易受到成像协议、定位和分割变化引起的分布偏移影响。研究发现使用协议不变特征的模型在分布偏移下保持稳定性能，而使用所有特征的模型性能下降40%。数据增强显著改善了不确定性估计质量。


<details>
  <summary>Details</summary>
Motivation: 放射组学模型在实际临床应用中面临成像协议、分割策略等变化导致的分布偏移问题，这会影响模型的可靠性和临床应用价值。需要系统评估模型在不同分布偏移下的鲁棒性。

Method: 使用16种水果的体模，评估了五种MRI序列（T2-HASTE、T2-TSE、T2-MAP、T1-TSE、T2-FLAIR）的协议变化、分割变化（完整、部分、旋转）和观察者间变异性。训练XGBoost分类器，比较使用8个一致稳健特征与序列特定特征的表现。

Result: 使用协议不变特征的模型在分布偏移下保持F1分数>0.85，而使用所有特征的模型在协议变化下性能下降40%。数据增强使预期校准误差降低35%，且不牺牲准确性。温度缩放提供的校准效益有限。

Conclusion: 协议感知的特征选择和受控体模研究能有效预测模型在分布偏移下的行为，为开发对真实世界协议变化具有鲁棒性的放射组学模型提供了框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [50] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出了一种基于因果效应估计任务的ADMG图距离度量方法，用于评估因果发现方法在潜在混杂下的表现


<details>
  <summary>Details</summary>
Motivation: 因果发现方法难以在存在潜在混杂的情况下进行有效评估，现有图距离度量方法存在局限性

Method: 使用基于fixing的识别方法和符号验证器，量化图差异对不同处理-结果对因果效应估计量的扭曲程度

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量方法进行了比较

Conclusion: 提出的图距离度量方法能够更好地评估因果发现在潜在混杂下的表现

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [51] [The Neural Differential Manifold: An Architecture with Explicit Geometric Structure](https://arxiv.org/abs/2510.25113)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出神经微分流形(NDM)架构，将神经网络重新概念化为微分流形，通过几何结构增强泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在欧几里得参数空间中运行，缺乏明确的几何结构。NDM旨在将几何结构直接融入网络设计，提供内在的正则化和更好的可解释性。

Method: 三层架构：坐标层实现可逆变换的平滑图表转换；几何层通过辅助子网络动态生成流形度量；演化层通过双目标损失函数优化任务性能和几何简洁性。

Result: 该框架支持与学习流形几何对齐的自然梯度下降优化，通过惩罚过度曲率和体积失真提供内在正则化，增强泛化和鲁棒性。

Conclusion: NDM代表了向几何结构化、可解释和高效深度学习系统的根本性转变，尽管仍存在显著的计算挑战。

Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural
network architecture that explicitly incorporates geometric structure into its
fundamental design. Departing from conventional Euclidean parameter spaces, the
NDM re-conceptualizes a neural network as a differentiable manifold where each
layer functions as a local coordinate chart, and the network parameters
directly parameterize a Riemannian metric tensor at every point. The
architecture is organized into three synergistic layers: a Coordinate Layer
implementing smooth chart transitions via invertible transformations inspired
by normalizing flows, a Geometric Layer that dynamically generates the
manifold's metric through auxiliary sub-networks, and an Evolution Layer that
optimizes both task performance and geometric simplicity through a
dual-objective loss function. This geometric regularization penalizes excessive
curvature and volume distortion, providing intrinsic regularization that
enhances generalization and robustness. The framework enables natural gradient
descent optimization aligned with the learned manifold geometry and offers
unprecedented interpretability by endowing internal representations with clear
geometric meaning. We analyze the theoretical advantages of this approach,
including its potential for more efficient optimization, enhanced continual
learning, and applications in scientific discovery and controllable generative
modeling. While significant computational challenges remain, the Neural
Differential Manifold represents a fundamental shift towards geometrically
structured, interpretable, and efficient deep learning systems.

</details>


### [52] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出DWMGrad优化算法，通过动态指导机制自适应调整动量和学习率，解决传统优化算法在复杂模型和非凸优化中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有优化算法如SGD和Adam在处理学习效率波动、复杂模型需求和非凸优化问题时存在不足，特别是在处理复杂数据结构和模型时面临学习率选择困难、局部最优陷阱和高维空间导航等挑战。

Method: 基于传统方法，引入依赖历史数据的动态指导机制，动态更新动量和学习率，使优化器能灵活调整对历史信息的依赖，适应不同训练场景。

Result: 实验验证表明，DWMGrad在多种场景下能够实现更快的收敛速度和更高的准确率。

Conclusion: DWMGrad算法通过动态调整机制有效提升了优化器对变化环境和任务复杂度的适应能力，在收敛性能和精度方面优于传统方法。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [53] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 提出了一种统一的对抗学习双层模型，重点研究了聚类模型中的对抗攻击机制，从数据扰动角度解释了攻击效果，并分析了δ-度量在衡量攻击效果中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型结构复杂，对抗攻击机制尚未得到很好解释，攻击效果的衡量方法也不明确，需要系统研究对抗学习机制。

Method: 提出统一的对抗学习双层模型，从数据扰动角度分析聚类模型的对抗攻击，研究δ-度量在攻击效果衡量中的适用性。

Result: 发现当数据扰动较小时聚类模型具有鲁棒性，当扰动较大时聚类结果会改变从而导致攻击，验证了δ-度量在对抗学习双层模型中的有效性。

Conclusion: 通过统一的双层模型框架，从数据扰动角度成功解释了聚类模型的对抗攻击机制，并建立了有效的攻击效果衡量方法。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [54] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: BOND是一种扰动方法，用于估计无法访问计算图的网络结构中的偏导数，相比现有方法具有更好的准确性和可扩展性，使集成黑盒函数的新架构探索成为可能。


<details>
  <summary>Details</summary>
Motivation: 探索如何在不增加可训练参数的情况下通过集成黑盒函数来增强模型性能，为结合模拟和数字设备扩展网络容量提供路径。

Method: 提出Bounded Numerical Differentiation (BOND)扰动方法，通过固定未训练网络作为黑盒函数，估计网络结构中的偏导数。

Result: 实验表明黑盒函数可以提升模型性能而不增加可训练参数，且无需对黑盒函数本身进行大量优化。

Conclusion: 利用固定不可训练模块扩展模型容量具有潜力，为结合模拟和数字设备扩展网络提供了可行路径。

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [55] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习引导的框架，用于快速解决电力系统最优断电问题，通过利用不同实例间的共享模式，比传统优化方法更快地生成高质量的解。


<details>
  <summary>Details</summary>
Motivation: 为缓解野火点火风险，电力公司需要在高风险区域对输电线路进行断电。最优断电问题是计算复杂的混合整数线性规划问题，需要在操作环境中快速频繁求解。由于特定电力系统的断电问题实例具有共同结构但参数不同，这促使使用机器学习方法来利用实例间的共享模式。

Method: 开发了一个机器学习引导的框架，扩展了现有的ML引导MILP求解方法，同时整合了关于通电和断电线路数量的领域知识。

Result: 在大型现实的加州合成测试系统上的结果显示，所提出的ML引导方法比传统优化方法更快地产生高质量的解。

Conclusion: 机器学习引导的方法能够有效解决最优断电问题，在保证解质量的同时显著提高求解速度。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [56] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO是一种用于推荐系统中LoRA持续学习的新方法，通过近端正则化来平衡模型适应性和稳定性，解决了传统方法过度关注历史偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的持续学习方法主要关注保持先前任务的性能，但这忽视了推荐系统的特殊性：目标不是预测过去的偏好，过时的偏好甚至可能在当前兴趣显著变化时损害性能。

Method: 提出PESO方法，引入近端正则化器，将当前适配器锚定到其最近的冻结状态，使模型能够灵活平衡适应性和保持性，更好地捕捉最近用户行为。

Result: 理论上证明这种近端设计在LoRA子空间中提供数据感知、方向性指导。实证上，PESO始终优于现有的基于LoRA的持续学习方法。

Conclusion: PESO通过近端正则化有效解决了推荐系统中LoRA持续学习的挑战，在平衡模型适应性和稳定性方面表现出色。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [57] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出一种分布式机器学习中的CPU资源优化算法，通过协同优化数据处理和计算资源分配，在时变网络中实现高效训练


<details>
  <summary>Details</summary>
Motivation: 随着人工智能快速发展，对快速、计算高效且可扩展的解决方案需求日益增长，需要优化分布式机器学习的计算资源分配

Method: 采用协同优化框架，在数据分布式存储的网络中，同时优化数据处理和CPU资源分配，支持时变信息共享网络和量化数据交换

Result: 与现有CPU调度方案相比，该算法将成本最优性差距改善了50%以上

Conclusion: 提出的算法在所有时间都可行，能够处理量化数据交换，并通过理论分析证明了向最优情况的收敛性

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [58] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: FairMIB是一个多视图信息瓶颈框架，通过将图分解为特征、结构和扩散视图来缓解GNN中的复杂性偏见，使用对比学习和条件信息瓶颈目标平衡任务效用和公平性。


<details>
  <summary>Details</summary>
Motivation: GNN在处理关系数据时可能放大训练数据偏见，传播歧视性属性和结构不平衡到不公平结果。现有公平性方法通常将偏见视为单一来源，忽略了属性和结构的不同影响，导致公平性和效用的次优权衡。

Method: 提出FairMIB框架：1）将图分解为特征、结构和扩散视图；2）使用对比学习最大化跨视图互信息进行无偏见表示学习；3）集成多视角条件信息瓶颈目标最小化与敏感属性的互信息；4）在扩散视图中引入逆概率加权邻接校正减少偏见传播。

Result: 在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上都达到了最先进的性能。

Conclusion: FairMIB通过多视图分解和对比学习有效缓解了GNN中的偏见传播问题，在保持任务效用的同时显著提升了公平性。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [59] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 提出一种基于损失函数从初始非凸性向最优解附近凸性转变假设的两阶段优化算法，通过检测转换点分别使用非凸方法(Adam)和凸方法(共轭梯度)来提升收敛性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习优化方法普遍采用非凸优化算法如Adam，但损失函数在局部最小值附近通常是凸的，可以利用这一特性设计更高效的优化策略。

Method: 设计两阶段优化框架：1) 检测损失函数从非凸到凸的转换点；2) 在非凸区域使用Adam，在凸区域使用共轭梯度法。转换点通过观察梯度范数与损失的关系来检测。

Result: 计算实验证实了损失函数的凸性结构假设，表明该简单框架能够显著改善收敛性和精度。

Conclusion: 损失函数在真实任务中普遍存在从非凸到凸的转换特性，利用这一特性设计的两阶段优化算法能够有效提升优化性能。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [60] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文探讨了训练和测试分布比例不匹配时的性能表现，发现在许多情况下分布偏移可能是有益的，即使组件之间没有相关性。


<details>
  <summary>Details</summary>
Motivation: 研究训练和测试数据分布比例不匹配时对模型性能的影响，挑战传统认为分布匹配最优的观点。

Method: 通过理论分析和多种场景下的实验，识别最优训练比例和分布偏移的益处程度。

Result: 发现在许多设置中，分布偏移可以改善测试性能，即使组件之间没有转移学习。

Conclusion: 分布偏移在某些情况下可能是有益的，这与传统观点相悖，为数据采样策略提供了新的视角。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [61] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的降维方法，专门用于处理双曲波传播的物理数据，通过低秩神经网络表示(LRNR)在超网络框架中实现高效表示。


<details>
  <summary>Details</summary>
Motivation: 针对双曲波传播这类物理数据，理论证明存在高效表示方法，但需要开发能够直接从数据中学习这种低维表示的实际算法。

Method: 使用低秩神经网络表示(LRNR)架构，结合深度学习技术直接从数据中学习传播波的低维表示，该架构自然产生低秩张量表示。

Result: 成功学习了传播波的高效低维表示，发现分解后的每个模式对应可解释的物理特征，并且LRNR架构支持通过压缩方案进行高效推理。

Conclusion: LRNR方法能够有效学习双曲波传播的低维表示，不仅提供物理可解释性，还具备高效推理能力，适合性能要求高的应用场景。

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [62] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: BRIDGE是一个统一的端到端架构，将序列编码器与图神经网络耦合在单一目标下，通过token级交叉注意力层实现细粒度消息传递，在友谊预测和欺诈检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常同时具有序列性和关系性，现有方法往往忽略其中一个模态。作者认为序列和图不是分离的问题，而是同一数据集的互补方面，应该联合学习。

Method: 提出BRIDGE架构，将序列编码器与GNN在单一目标下耦合，允许梯度在两个模块间流动。引入TOKENXATTN层，在相邻序列的事件间进行token级消息传递。

Result: 在友谊预测（Brightkite）和欺诈检测（Amazon）两个场景中，BRIDGE在排序和分类指标上持续优于静态GNN、时序图方法和仅序列基线。

Conclusion: 序列和图应该联合建模，BRIDGE通过端到端耦合序列编码器和GNN，实现了更好的表示学习性能。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [63] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，将数据增强与因果推断结合，证明当结果生成机制对数据增强选择不变时，数据增强可视为对治疗生成机制的干预，有助于减少隐藏混杂因素带来的因果效应估计偏差。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强主要用于i.i.d.设置下的正则化，但本文旨在扩展其应用范围，使其能够跨越干预实现泛化。主要动机是：在许多应用中，工具变量可能不如数据增强容易获得，因此探索数据增强在因果推断中的潜力。

Method: 通过适当正则化基于工具变量的估计器，引入IV-like回归概念来缓解混杂偏差；将参数化数据增强建模为IV-like回归问题，并通过组合使用模拟最坏情况的数据增强应用。

Result: 理论和模拟实验表明，该方法在因果估计和泛化任务上的性能优于简单数据增强，在线性示例的有限样本情况下得到验证，并通过真实数据实验支持了该方法的有效性。

Conclusion: 数据增强可以超越传统的i.i.d.设置，在因果推断中发挥重要作用，特别是在工具变量不可得的情况下，通过IV-like回归框架能够有效改善因果效应估计的准确性和跨干预的泛化能力。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [64] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种Lipschitz感知的线性嫁接方法，通过将非线性激活函数替换为线性函数来消除近似误差，从而获得更紧的局部Lipschitz常数并提升认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在神经网络验证中面临近似误差问题，这些误差阻碍了获得紧的局部Lipschitz常数，而紧的Lipschitz常数对于认证鲁棒性至关重要。线性嫁接可以减少不稳定神经元数量，但缺乏理论解释其如何改善认证鲁棒性。

Method: 提出Lipschitz感知的线性嫁接方法，将非线性激活函数替换为线性函数，消除主导的近似误差源，从而获得更紧的局部Lipschitz常数。

Result: 实验表明，将线性性嫁接到有影响力的激活函数中可以收紧l∞局部Lipschitz常数，并增强认证鲁棒性，即使没有认证训练也能实现。

Conclusion: 线性嫁接通过消除近似误差来收紧局部Lipschitz常数，从而改善认证鲁棒性，为该方法提供了理论依据和实际验证。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [65] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种用于深度时间序列预测的选择性学习策略，通过双掩码机制筛选可泛化的时间步，避免对不确定和异常时间步的过拟合。


<details>
  <summary>Details</summary>
Motivation: 深度模型在时间序列预测中容易因噪声和异常而严重过拟合，传统方法对所有时间步进行统一优化，导致模型学习不确定和异常的时间步。

Method: 引入选择性学习策略，使用双掩码机制：不确定性掩码利用残差熵过滤不确定时间步，异常掩码使用残差下界估计排除异常时间步，仅对筛选出的时间步计算MSE损失。

Result: 在8个真实数据集上的实验表明，该方法显著提升了主流深度模型的预测性能，其中Informer的MSE降低了37.4%，TimesNet降低了8.4%，iTransformer降低了6.5%。

Conclusion: 选择性学习策略能有效缓解深度时间序列预测中的过拟合问题，通过关注可泛化的时间步显著提升模型性能。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [66] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出了一种基于自适应损失加权的成本敏感多类PU学习方法，通过为正向和推断负向损失分配数据依赖权重，实现无偏风险估计，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多类PU学习中，许多现有方法无法保证无偏风险估计，限制了性能和稳定性。现实应用中标注可靠负样本困难或成本高，需要有效的多类PU学习解决方案。

Method: 在经验风险最小化框架下，为正向和从无标签混合数据中推断的负向损失分量分配不同的数据依赖权重，使经验目标成为目标风险的无偏估计器。

Result: 在八个公共数据集上的广泛实验表明，在不同类别先验和类别数量下，该方法在准确性和稳定性方面均优于强基线方法。

Conclusion: 提出的自适应损失加权方法为多类PU学习提供了有效的无偏风险估计框架，在多个数据集上实现了稳定且准确的结果。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [67] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: BSFA框架通过差异化缩放不同子空间的更新分量来加速深度学习训练，在主导子空间抑制更新以增强稳定性，在主体子空间放大更新以提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明深度学习优化中存在根本性二分法：损失Hessian矩阵顶部特征方向上的参数更新虽然幅度大但对损失减少贡献小，而正交分量上的更新幅度小但驱动大部分学习进展。

Method: 提出Bulk-Space-Filtration-Accelerator (BSFA)框架，使用PCA对历史更新进行高效子空间估计，并采用分块策略在每个参数块上应用此估计，差异化缩放不同子空间的更新分量。

Result: 在各种任务中展示BSFA的加速效果，特别是在WikiText-103上预训练LLaMA-72M和在OpenWebText上预训练LLaMA-134M时，相比标准AdamW实现了约2倍的加速。

Conclusion: BSFA是一个实用且可扩展的即插即用框架，能够有效加速深度学习训练，同时保持计算可行性。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [68] [Scaling Up Bayesian DAG Sampling](https://arxiv.org/abs/2510.25254)
*Daniele Nikzad,Alexander Zhilkin,Juha Harviainen,Jack Kuipers,Giusi Moffa,Mikko Koivisto*

Main category: cs.LG

TL;DR: 提出了两种改进贝叶斯网络结构采样的技术：高效实现基本移动操作，以及通过预处理修剪可能的父节点集来加速求和计算。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络结构推断通常通过马尔可夫链采样进行，但现有方法在采样效率和求和计算方面存在性能瓶颈。

Method: 1. 高效实现添加、删除或反转单条弧的基本移动操作；2. 设计预处理方法来修剪可能的父节点集，近似保持求和结果。

Result: 实证研究表明，与先前方法相比，这些技术能带来显著的效率提升。

Conclusion: 所提出的技术能够有效提高贝叶斯网络结构采样的效率，为更复杂的移动操作提供支持。

Abstract: Bayesian inference of Bayesian network structures is often performed by
sampling directed acyclic graphs along an appropriately constructed Markov
chain. We present two techniques to improve sampling. First, we give an
efficient implementation of basic moves, which add, delete, or reverse a single
arc. Second, we expedite summing over parent sets, an expensive task required
for more sophisticated moves: we devise a preprocessing method to prune
possible parent sets so as to approximately preserve the sums. Our empirical
study shows that our techniques can yield substantial efficiency gains compared
to previous methods.

</details>


### [69] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出基于信息瓶颈原理的IB-Inspired Normalization (IBNorm)，通过有界压缩操作在保持训练稳定性的同时提升表征的信息质量，在语言和视觉模型中均优于传统归一化方法。


<details>
  <summary>Details</summary>
Motivation: 现有归一化方法如BatchNorm、LayerNorm和RMSNorm主要关注方差控制，但没有有效控制表征如何捕获任务相关信息，缺乏信息理论指导。

Method: 基于信息瓶颈原理，引入有界压缩操作，鼓励嵌入保留预测信息同时抑制无关变异性，在保持标准归一化稳定性和兼容性的基础上提升信息质量。

Result: 理论证明IBNorm获得更高的信息瓶颈值和更紧的泛化边界；实证在LLaMA、GPT-2等语言模型和ResNet、ViT等视觉模型中均优于传统归一化方法，互信息分析确认了更优的信息瓶颈行为。

Conclusion: IBNorm为深度学习归一化提供了信息理论基础，在保持训练稳定性的同时显著提升表征的信息质量，具有广泛适用性。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [70] [On the Stability of Neural Networks in Deep Learning](https://arxiv.org/abs/2510.25282)
*Blaise Delattre*

Main category: cs.LG

TL;DR: 该论文通过敏感性分析的统一视角，结合Lipschitz网络、随机平滑和曲率正则化，解决了深度学习模型在输入扰动和参数变化下的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型存在不稳定性和脆弱性：输入的小变化可能严重影响预测结果，而尖锐的损失函数景观会阻碍优化过程。

Method: 采用Lipschitz网络约束输入敏感性，引入基于损失函数曲率的正则化技术，并探索随机平滑作为增强决策边界鲁棒性的概率方法。

Result: 开发了统一的稳定性框架，包括高效谱范数计算、新型Lipschitz约束层和改进的认证程序，在泛化性、对抗鲁棒性和训练稳定性方面取得改进。

Conclusion: 通过结合敏感性分析的多个视角，为深度学习模型的稳定性挑战提供了理论和实践解决方案，建立了Lipschitz连续性、随机平滑和曲率正则化的协同作用。

Abstract: Deep learning has achieved remarkable success across a wide range of tasks,
but its models often suffer from instability and vulnerability: small changes
to the input may drastically affect predictions, while optimization can be
hindered by sharp loss landscapes. This thesis addresses these issues through
the unifying perspective of sensitivity analysis, which examines how neural
networks respond to perturbations at both the input and parameter levels.
  We study Lipschitz networks as a principled way to constrain sensitivity to
input perturbations, thereby improving generalization, adversarial robustness,
and training stability. To complement this architectural approach, we introduce
regularization techniques based on the curvature of the loss function,
promoting smoother optimization landscapes and reducing sensitivity to
parameter variations. Randomized smoothing is also explored as a probabilistic
method for enhancing robustness at decision boundaries.
  By combining these perspectives, we develop a unified framework where
Lipschitz continuity, randomized smoothing, and curvature regularization
interact to address fundamental challenges in stability. The thesis contributes
both theoretical analysis and practical methodologies, including efficient
spectral norm computation, novel Lipschitz-constrained layers, and improved
certification procedures.

</details>


### [71] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出了一个分层物理嵌入学习框架，用于从稀疏噪声数据中进行时空预测和物理定律发现。该框架采用两级架构，第一级学习PDE的基本符号组件，第二级学习它们的组合，能够结构性地整合先验知识。


<details>
  <summary>Details</summary>
Motivation: 建模复杂时空动力学，特别是远离平衡态系统，是科学中的重大挑战。传统方法存在局限性：纯数据驱动模型物理不一致且数据密集，现有物理信息方法缺乏表示复杂算子的结构能力。

Method: 分层物理嵌入学习框架，采用两级架构：第一级学习PDE的基本符号组件，第二级学习它们的组合。基于自适应傅里叶神经算子构建，能够捕获非局部依赖和高阶算子。

Result: 该框架能够保证物理一致性，提高数据效率，并支持通过符号回归可解释地发现基础控制方程，无需预设函数形式。

Conclusion: 分层物理嵌入学习框架在时空预测和物理定律发现方面取得了根本性进展，通过结构性地整合先验知识，解决了现有方法的局限性。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [72] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出了一种多目标强化学习算法，在最大化期望回报的同时，确保策略在目标状态上具有分散的边际状态分布。


<details>
  <summary>Details</summary>
Motivation: 传统RL算法主要关注最大化期望回报，导致策略可能只利用少数奖励源。但在许多自然场景中，需要在达到目标状态的同时均匀访问所有目标状态，这尚未得到充分探索。

Method: 提出基于自定义RL奖励的新算法，通过策略混合优化，在每次迭代中为采样轨迹计算奖励，并使用离线RL算法更新策略混合。

Result: 算法在合成MDP和标准RL环境中进行了实验验证，证明了其有效性。

Conclusion: 该算法能够同时优化期望回报和目标状态上的边际状态分布分散度，具有理论性能保证。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [73] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: 提出了一种基于循环矩阵和对角矩阵乘积的可逆线性层，显著降低了参数复杂度和计算复杂度，并在此基础上构建了CDFlow模型，在自然图像数据集上取得了良好的密度估计效果。


<details>
  <summary>Details</summary>
Motivation: 设计既能增强表达能力又能保持雅可比行列式和逆矩阵高效计算的可逆线性层，解决归一化流模型中线性层的设计挑战。

Method: 使用循环矩阵和对角矩阵的乘积分解来构建可逆线性层，利用快速傅里叶变换实现高效计算，并基于此构建Circulant-Diagonal Flow模型。

Result: 将参数复杂度从O(n²)降低到O(mn)，矩阵求逆时间复杂度从O(n³)降低到O(mn log n)，对数行列式计算从O(n³)降低到O(mn)，在自然图像数据集上实现了强大的密度估计。

Conclusion: CDFlow模型在保持表达力的同时显著提高了计算效率，特别适用于具有周期性结构的数据建模，为可扩展生成建模提供了实用优势。

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [74] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 本文提出了解决信息级联流行度预测中三个关键问题的新方法：时间泄漏、特征贫乏数据集和计算效率低下，通过时间有序分割、新数据集Taoke和轻量级框架CasTemp实现了无泄漏评估下的最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前信息级联流行度预测存在三个关键限制：时间泄漏导致评估不真实、特征贫乏数据集缺乏下游转化信号、复杂图方法计算效率低下，这些问题限制了实际应用价值。

Method: 提出三方面解决方案：时间有序分割策略防止未来信息泄漏；构建Taoke电商级联数据集，包含丰富的推广者/产品属性和真实购买转化；开发CasTemp轻量级框架，通过时间游走、Jaccard邻居选择和GRU编码建模级联动态。

Result: 在无泄漏评估下，CasTemp在四个数据集上达到最优性能，计算速度提升数个数量级，特别擅长预测第二阶段的流行度转化这一实际关键任务。

Conclusion: 通过系统解决任务设置、数据集构建和模型设计三个核心挑战，实现了更真实、实用且高效的信息级联流行度预测，为实际应用提供了可靠解决方案。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [75] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 该论文对随机几何超图上的变分学习进行了渐近一致性分析，提出了高阶超图学习(HOHL)方法，通过骨架图的拉普拉斯幂进行多尺度平滑正则化，并在标准基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 超图为高阶交互提供了自然框架，但在半监督学习中的理论基础仍然有限。需要研究超图学习的理论保证和有效方法。

Method: 提出了高阶超图学习(HOHL)，通过骨架图的拉普拉斯幂进行正则化以实现多尺度平滑，该方法收敛到高阶Sobolev半范数。

Result: 理论分析表明超图学习收敛到加权p-拉普拉斯方程，经验验证显示HOHL在标准基准测试中表现强劲。

Conclusion: 该工作为超图学习提供了理论保证，提出的HOHL方法在理论和实践中都表现出色，为高阶交互建模提供了有效工具。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [76] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 该论文提出在知识图谱嵌入模型中使用模型合并方法，特别是加权平均，作为传统集成学习的替代方案，以降低计算开销并提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法需要训练多个模型，导致计算延迟和内存开销增加。模型合并方法提供了有前景的替代方案，无需训练多个模型。

Method: 提出两种加权平均方法：1）从训练周期开始维护模型参数的运行平均值；2）仅在验证集上泛化性能改善时选择性更新集成模型参数的运行平均值。

Result: 在链接预测任务中，加权平均方法相比最先进的基准集成方法持续改善性能，并在字面增强KGE模型和多跳查询回答任务中也表现良好。

Conclusion: 提出的加权平均方法在多样化评估设置中一致地提高了性能，为知识图谱嵌入模型的集成学习提供了有效的替代方案。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [77] [Position: Biology is the Challenge Physics-Informed ML Needs to Evolve](https://arxiv.org/abs/2510.25368)
*Julien Martinelli*

Main category: cs.LG

TL;DR: 提出了生物学信息机器学习(BIML)，作为物理信息机器学习(PIML)的扩展，专门针对生物学的独特挑战进行优化。


<details>
  <summary>Details</summary>
Motivation: 生物学建模面临多方面的挑战：多面性和不确定的先验知识、异构和噪声数据、部分可观测性以及复杂的高维网络。这些挑战不应被视为PIML的障碍，而应作为其演化的催化剂。

Method: BIML保留了PIML的结构基础，但适应生物学的实际现实。它重新调整PIML的方法，在更软、概率形式的先验知识下运行。提出了四个基础支柱：不确定性量化、情境化、约束潜在结构推断和可扩展性。

Result: 基础模型和大型语言模型将成为关键推动者，连接人类专业知识与计算建模。

Conclusion: 提出了构建BIML生态系统和引导PIML启发的创新朝向具有高科学和社会相关性的挑战的具体建议。

Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.

</details>


### [78] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: GPTOpt是一种基于大语言模型的优化方法，通过在大规模合成数据集上微调LLM，使其具备连续黑盒优化能力，无需参数调优即可超越传统优化器。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法需要针对每个应用领域进行参数调优，而现有大语言模型在连续黑盒优化任务上能力有限，需要开发一种能够泛化到不同优化任务的LLM优化方法。

Method: 通过从多样化的贝叶斯优化参数化中生成的大规模合成数据集来微调大语言模型，利用LLM的预训练能力实现跨优化任务的泛化。

Result: 在各种黑盒优化基准测试中，GPTOpt超越了传统优化器，展示了LLM在高级数值推理方面的能力。

Conclusion: GPTOpt为无需参数调优的全局优化提供了一个灵活框架，证明了LLM在连续黑盒优化任务中的潜力。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [79] [A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory](https://arxiv.org/abs/2510.25379)
*Adrien Weihs,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 本文研究多算子学习问题，提出了两种新架构MNO和MONet，建立了算子逼近的理论基础，并在参数PDE基准测试中验证了其表达能力和效率。


<details>
  <summary>Details</summary>
Motivation: 机器学习通常关注有限维空间之间的映射学习，但科学应用需要逼近函数空间之间的算子。本文旨在为多算子学习建立统一的理论和实践基础。

Method: 引入两种新架构MNO和MONet，在连续、可积和Lipschitz算子三种设置下建立通用逼近理论，并推导显式的缩放规律。对于多个独立算子的学习，开发了平衡子网络架构复杂度的框架。

Result: 理论分析表明所提架构具有通用逼近能力，并建立了网络规模与逼近精度之间的显式关系。在参数PDE基准测试中验证了架构的强表达能力和计算效率。

Conclusion: 本文为可扩展的神经算子学习建立了统一的理论和实践基础，为多算子学习提供了系统的理论分析和有效的架构设计。

Abstract: While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.

</details>


### [80] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 提出了效用校准框架，用于评估多分类器的校准性能，通过特定效用函数来衡量校准误差，统一并重新解释现有校准指标。


<details>
  <summary>Details</summary>
Motivation: 现有多分类校准方法要么关注预测的特定方面（如top-class置信度、类级校准），要么使用计算复杂的变分公式，需要可扩展的评估方法。

Method: 提出效用校准框架，通过特定效用函数来测量校准误差，该函数封装了与最终用户相关的目标或决策标准。

Result: 该框架可以统一和重新解释多个现有校准指标，特别是允许更稳健的top-class和类级校准指标，并扩展到评估更丰富的下游效用类别。

Conclusion: 效用校准提供了一个通用的可扩展框架，用于评估多分类器的校准性能，超越了传统的二值化方法。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [81] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN是一个基于线性RNN的单变量时间序列基础模型，仅使用合成数据预训练，在零样本预测中实现高效长时预测和顶级性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本时间序列预测基础模型在高效长时预测和可复现性方面面临挑战，且仅基于合成数据的方法在复杂基准测试中表现不佳。

Method: 使用带有状态编织的GatedDeltaProduct架构的线性RNN，通过完全并行化训练跨序列长度，无需窗口化或汇总技术。采用统一的合成数据生成管道，包括随机微分方程、高斯过程和音频合成等多样化生成器。

Result: 在Gift-Eval基准测试的零样本评估中，TempoPFN实现了顶级竞争性能，超越了所有现有的仅基于合成数据的方法，并超过了大多数基于真实数据训练的模型，同时通过完全并行化训练和推理比现有基线更高效。

Conclusion: TempoPFN提供了一个可复现的时间序列预测基础模型，开源了完整的数据生成管道和训练代码，为未来研究奠定了坚实基础。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [82] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: 提出了一种具有模型内在机制概念解释的新模型，强调概念解释的忠实性，概念在类别间共享，并能从任何层忠实追踪其对logit的贡献和输入可视化。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念的后处理方法理解深度网络工作原理时不够忠实，且对模型学习的概念做出了限制性假设（如类别特异性、小空间范围或与人类期望对齐）。

Method: 提出新模型，具有模型内在机制概念解释，概念在类别间共享，能从任何层忠实追踪其对logit的贡献和输入可视化，并利用基础模型提出新的概念一致性度量C^2-Score。

Result: 相比先前工作，提出的概念在数量上更一致，用户认为这些概念更可解释，同时保持了竞争力的ImageNet性能。

Conclusion: 该方法提供了更忠实和可解释的概念解释，同时保持模型性能，为深度网络的概念级理解提供了新途径。

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [83] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 提出梯度权重对齐(GWA)指标，通过量化每个样本梯度与模型权重之间的一致性来跟踪泛化性能，无需验证集即可预测最优早停点、比较模型和识别有影响力的训练样本。


<details>
  <summary>Details</summary>
Motivation: 在深度学习领域，需要稳健的验证指标来检测过拟合、监控训练动态。研究训练数据与模型权重之间的交互是否能产生既能跟踪泛化又能归因性能到单个训练样本的指标。

Method: 引入梯度权重对齐(GWA)，量化每个样本梯度与模型权重之间的一致性。有效学习对应一致的对齐，而对齐不良表示泛化性能下降。该指标可在训练期间高效计算。

Result: 大量实验表明，GWA能准确预测最优早停点，支持有原则的模型比较，并识别有影响力的训练样本，提供了一种无需验证集的模型分析方法。

Conclusion: GWA提供了一种直接从训练数据中进行模型分析的验证集无关方法，能够有效跟踪泛化性能并识别关键训练样本。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [84] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: 提出了一个混合量子-经典循环神经网络架构，其中整个循环核心由参数化量子电路实现，由经典前馈网络控制。该模型在多个序列学习任务中表现出与强经典基线竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个物理一致且紧凑的量子循环神经网络架构，将量子计算的指数级状态空间优势与经典非线性控制相结合，用于序列学习任务。

Method: 使用参数化量子电路作为循环核心，量子态作为隐藏状态，通过中间电路测量获得读出，结合经典前馈网络提供非线性控制，形成混合量子-经典架构。

Result: 在情感分析、MNIST、置换MNIST、复制记忆和语言建模等任务中，使用最多14个量子位的模拟实验显示，该模型在序列学习任务中与强经典基线竞争。

Conclusion: 这是第一个基于量子操作并在广泛序列学习任务中实现竞争性能的模型，成功将量子循环记忆、中间测量和经典非线性控制统一起来。

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


### [85] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 本文提出原型神经符号架构，通过原型学习理论解决神经符号AI中的推理捷径问题，确保模型基于正确概念而非伪相关性进行推理，在极低数据量下仍能有效学习。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI模型容易学习推理捷径，即利用伪相关性而非正确概念来满足符号约束，这影响了模型的可信度和安全性。

Method: 采用原型神经符号架构，结合原型学习理论，训练模型在满足背景知识的同时考虑输入与少量标注数据的相似性，从而避免推理捷径。

Result: 在rsbench基准测试中，包括合成任务和真实高风险任务，该方法在极低监督下显著提升了正确概念的学习能力。

Conclusion: 原型基础化为神经符号学习提供了一种有效且标注效率高的策略，为实现安全可靠的神经符号AI铺平了道路。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [86] [Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting](https://arxiv.org/abs/2510.25563)
*Víctor Medina,Giovanny A. Cuervo-Londoño,Javier Sánchez*

Main category: cs.LG

TL;DR: 将大气预测深度学习模型Aurora迁移到海洋领域，在加那利上升流系统预测海表温度，通过微调实现低误差和高相关性，证明了跨领域深度学习模型在海洋预测中的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统数值海洋预测模型存在计算成本高和可扩展性限制的问题，需要探索更高效的预测方法。深度学习模型在气象领域已取得显著成果，但将其应用于海洋预测的研究相对较少。

Method: 采用分阶段微调策略，将大气预测模型Aurora适配到海洋领域，使用高分辨率海洋再分析数据，结合纬度加权误差指标和超参数优化进行高效学习。

Result: 模型在加那利上升流系统实现了0.119K的低RMSE和约0.997的高异常相关系数，成功再现了大尺度海表温度结构，但在沿海区域细节捕捉方面存在挑战。

Conclusion: 证明了预训练深度学习模型在不同领域间迁移的可行性，为数据驱动的海洋预测开辟了新途径。未来可通过整合更多海洋变量、提高空间分辨率以及探索物理信息神经网络来进一步提升性能。

Abstract: The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.

</details>


### [87] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较了浮点(FP)和整数(INT)量化格式在不同粒度下的表现，发现在8位细粒度量化中MXINT8优于FP格式，而在4位量化中FP格式通常更准确，但通过异常值缓解技术NVINT4可以超越NVFP4。


<details>
  <summary>Details</summary>
Motivation: 现代AI硬件越来越多采用低精度浮点格式处理LLM中的激活异常值，但缺乏FP和INT量化在不同粒度下的统一比较，导致算法和硬件协同设计缺乏明确指导。

Method: 系统研究FP和INT格式之间的权衡，包括在不同粒度下的性能比较，引入对称裁剪方法解决细粒度低比特INT训练中的梯度偏差问题。

Result: 发现关键性能交叉点：FP在粗粒度量化中表现优异，但在细粒度(块级)比较中更为微妙；MXINT8在8位细粒度格式中在算法精度和硬件效率上都优于FP对应格式；4位格式中FP通常有精度优势，但应用Hadamard旋转等异常值缓解技术后NVINT4可以超越NVFP4。

Conclusion: 研究结果挑战当前硬件发展轨迹，表明一刀切的FP方法并非最优，提倡细粒度INT格式特别是MXINT8为未来AI加速器提供了更好的精度、功耗和效率平衡。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [88] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 该研究使用机器学习方法预测员工倦怠风险，在三种算法中支持向量机表现最佳（R²=0.84），并开发了交互界面供非技术人员使用。


<details>
  <summary>Details</summary>
Motivation: 员工倦怠对个人福祉和组织绩效有显著影响，需要有效的早期检测方法来支持数据驱动的心理健康策略。

Method: 使用HackerEarth员工倦怠挑战数据集，评估了K最近邻、随机森林和支持向量机三种监督学习算法，采用30折交叉验证和决定系数进行评估。

Result: 支持向量机模型表现最佳（R²=0.84），在配对t检验中显著优于K最近邻和随机森林模型。

Conclusion: 机器学习在组织环境中具有早期检测员工倦怠风险的潜力，支持数据驱动的心理健康干预策略。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [89] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: BOLT-GAN是基于贝叶斯最优学习阈值改进的WGAN变体，使用Lipschitz连续判别器，在训练稳定性上优于WGAN，在多个图像生成基准测试中FID分数降低10-60%。


<details>
  <summary>Details</summary>
Motivation: 改进WGAN框架的训练稳定性，通过贝叶斯最优学习阈值原理来增强GAN训练效果。

Method: 在WGAN框架基础上引入BOLT原理，使用Lipschitz连续判别器，隐式最小化不同于Wasserstein距离的度量距离。

Result: 在CIFAR-10、CelebA-64、LSUN Bedroom-64和LSUN Church-64四个标准图像生成基准测试中，BOLT-GAN始终优于WGAN，FID分数降低10-60%。

Conclusion: BOLT是一个广泛适用的原理，可以有效增强GAN训练效果。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [90] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文系统研究了视觉-语言-动作(VLA)模型在动作微调过程中的表示保留问题，发现简单的动作微调会导致视觉表示退化，并提出了一种简单有效的对齐方法来缓解这种退化。


<details>
  <summary>Details</summary>
Motivation: 研究VLA模型在动作微调过程中，预训练的视觉-语言模型(VLM)的原始表示和知识能在多大程度上被保留，因为不清楚动作微调对VL表示的影响程度。

Method: 通过探测VLA的隐藏表示和分析注意力图来表征和测量表示退化效应；设计对比任务和方法来隔离动作微调引起的VL能力变化；评估多种视觉表示对齐策略。

Result: 发现简单的动作微调会导致视觉表示退化；提出的简单对齐方法能有效缓解退化，并在分布外(OOD)场景中实现更好的泛化。

Conclusion: 阐明了动作微调与VL表示退化之间的权衡关系，并提出了恢复继承VL能力的实用方法。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [91] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出基于f-散度的核引导互信息(KG-MI)指标，结合多头注意力框架，理论证明能在多项式时间内收敛到全局最优，准确恢复DAG图结构


<details>
  <summary>Details</summary>
Motivation: 现有transformer模型的理论分析仅限于树状图结构，无法处理具有多个父节点的通用有向无环图(DAG)，需要设计能分离学习不同父子依赖关系的训练目标

Method: 引入核引导互信息(KG-MI)作为信息论指标，结合多头注意力框架，每个注意力头对应不同的边际转移核来建模多样化的父子依赖关系

Result: 理论证明在K-父节点DAG生成的序列上，通过梯度上升训练单层多头transformer能在多项式时间内收敛到全局最优，且注意力分数能准确反映真实邻接矩阵

Conclusion: 提出的KG-MI目标函数结合多头注意力框架，能够有效恢复通用DAG的图结构，为transformer在图结构学习中的理论分析提供了重要进展

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [92] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: FedLap是一个用于图结构数据联邦学习的新框架，通过拉普拉斯平滑在谱域中利用全局结构信息，有效捕捉节点间依赖关系，同时确保隐私和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中图结构数据的隐私和可扩展性问题，特别是当子图之间存在重要连接时，现有方法要么需要交换敏感节点嵌入（隐私风险），要么依赖计算密集型步骤（可扩展性差）。

Method: 提出FedLap框架，利用拉普拉斯平滑在谱域中捕获全局结构信息，避免直接交换节点嵌入，同时保持计算效率。

Result: 在基准数据集上的广泛实验表明，FedLap相比现有技术实现了竞争性或更优的性能，并且是第一个具有强隐私保证的子图联邦学习方案。

Conclusion: FedLap成功解决了图联邦学习中的隐私和可扩展性挑战，通过谱域方法有效捕捉节点间依赖关系，同时提供形式化的隐私保证。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [93] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: 提出GNSS框架，一种基于图神经网络的动态结构模拟替代模型，通过局部坐标系、符号感知损失和波长感知连接半径等创新设计，在波主导的动态结构模拟中实现高精度和快速推理。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在数值模拟中作为替代模型已有探索，但在结构问题特别是动态案例中研究较少，需要填补这一空白。

Method: 采用编码-处理-解码的GNN范式，关键创新包括：节点固定局部坐标系表达运动学、符号感知回归损失减少相位误差、波长感知连接半径优化图构建。

Result: 在50kHz脉冲激励梁的案例中，GNSS能准确再现物理过程数百个时间步，泛化到未见载荷条件，而现有GNN无法收敛或提供有效预测。

Conclusion: 具有物理一致性更新规则的局部保持GNN是动态波主导结构模拟的竞争性替代方案，相比显式有限元基准实现显著推理加速同时保持时空保真度。

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [94] [LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries](https://arxiv.org/abs/2510.25731)
*René P. Klausen,Ivan Timofeev,Johannes Frank,Jonas Naujoks,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出LieSolver方法，利用李对称性精确求解初边值问题，比PINNs更快更准确


<details>
  <summary>Details</summary>
Motivation: 传统方法如PINNs在求解偏微分方程时存在收敛慢和精度不足的问题，需要更高效可靠的方法

Method: 利用李对称变换构建模型，通过构造精确满足PDE，从初始和边界数据学习解

Result: LieSolver在线性齐次PDE上比PINNs更快更准确，能进行严格误差估计

Conclusion: 该方法提高了PDE约束问题的计算效率和预测可靠性

Abstract: We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.

</details>


### [95] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 提出一个统一框架，从随机PAC-Bayesian保证中提取单个假设的保证，解决了传统PAC-Bayes只能提供随机采样假设期望风险保证的问题。


<details>
  <summary>Details</summary>
Motivation: 传统PAC-Bayes框架只提供随机采样假设的期望风险保证，需要随机预测，无法在实际部署单一确定性假设时使用。

Method: 提出统一框架，包括一般oracle边界、数值边界和多数投票特化方法。

Result: 经验表明该方法在确定性分类器的泛化边界上始终优于流行基线（最高可达2倍）。

Conclusion: 该框架成功将随机PAC-Bayesian保证转化为单个确定性假设的保证，具有实际应用价值。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [96] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 本文系统研究了低秩伪逆在噪声矩阵下的谱范数鲁棒性，提出了改进的扰动界，比经典方法提升高达√n倍。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的矩阵常因采样、素描和量化而带有噪声，但低秩逆近似的谱范数鲁棒性研究不足。

Method: 使用轮廓积分技术分析非整函数f(z)=1/z，推导出考虑特征间隙、谱衰减和噪声对齐的尖锐非渐近扰动界。

Result: 新边界在多种真实和合成矩阵上紧密跟踪实际扰动误差，而基于经典结果的估计往往显著高估。

Conclusion: 研究结果为噪声计算环境中的低秩逆近似提供了实用的、频谱感知的保证。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [97] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Orlicz几何结构的广义Sobolev IPM（GSI），通过Musielak正则化将其简化为单变量优化问题，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统Sobolev IPM受限于L^p几何结构，无法融入其他结构先验。为了克服这一限制，需要推广到更灵活的几何结构框架。

Method: 通过Orlicz几何结构推广Sobolev IPM，建立Orlicz-Sobolev范数与Musielak范数的理论联系，并利用图结构将GSI-M简化为单变量优化问题。

Result: GSI-M在计算上比流行的Orlicz-Wasserstein快几个数量级，在文档分类和拓扑数据分析任务中表现出实际优势。

Conclusion: 提出的GSI-M框架不仅包含经典Sobolev IPM作为特例，还能容纳多样化的几何先验，同时实现了显著的计算效率提升。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [98] [Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning](https://arxiv.org/abs/2510.25594)
*Arani Roy,Marco P. Apolinario,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出了一种基于SVD分解的结构化局部学习框架，直接在低秩流形上训练深度神经网络，减少可训练参数数量，在保持与反向传播相当精度的同时降低内存和计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播的全局误差传播和完全参数化带来的高内存计算开销问题，以及直接反馈对齐方法在深度架构中反馈结构不明确和可扩展性差的问题。

Method: 在SVD分解的权重矩阵低秩流形上训练，对SVD分量使用复合损失函数（交叉熵、子空间对齐和正交正则化）进行更新，构建匹配SVD结构的反馈矩阵确保前向和反馈路径一致性。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上达到与反向传播相当的精度，消融研究证实了低秩设置中各损失项的重要性。

Conclusion: 低秩流形上的局部学习是完整秩梯度训练的一个有原则且可扩展的替代方案。

Abstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.

</details>


### [99] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出基于核评分规则的不确定性度量框架，统一了多种现有方法，为回归任务中的总体、偶然和认知不确定性提供量化工具。


<details>
  <summary>Details</summary>
Motivation: 回归任务在安全关键领域需要准确的不确定性量化，但现有研究主要关注分类问题，缺乏针对回归任务的系统化不确定性度量方法。

Method: 基于核评分规则构建不确定性度量框架，通过选择不同的核函数来控制度量的尾部敏感性、鲁棒性和分布外检测性能。

Result: 实验证明该框架在多种下游任务中有效，并揭示了不同核函数选择在鲁棒性和分布外检测性能之间的权衡关系。

Conclusion: 该框架为回归任务的不确定性量化提供了统一的理论基础和实践指导，通过核函数选择可以设计针对特定任务需求的不确定性度量方法。

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [100] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 该论文建立了对称矩阵谱范数扰动的新界限，改进了经典的Eckart-Young-Mirsky定理，并应用于差分隐私PCA，解决了文献中的开放问题。


<details>
  <summary>Details</summary>
Motivation: 理解噪声或测量误差如何影响低秩近似，特别是在谱范数下，这对于差分隐私低秩近似特别重要。现有工作主要分析Frobenius范数误差，但可能高估或低估真实的子空间失真。

Method: 使用复分析中的新颖轮廓自举方法，并将其扩展到包括多项式和矩阵指数在内的广泛谱函数类，建立了对称矩阵的高概率谱范数扰动界限。

Result: 在温和的特征值间隙和范数条件下，新界限对∥(A+E)_p - A_p∥给出了锐利估计，改进因子高达√n，并在真实数据集上证实了这些界限能紧密跟踪实际谱误差。

Conclusion: 新建立的谱范数扰动界限改进了经典理论，为差分隐私PCA提供了改进的效用保证，解决了文献中的开放问题，且实证结果验证了理论界限的有效性。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [101] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: 该论文展示了循环神经网络如何通过噪声维持的轨道动力学来模拟隐马尔可夫模型的离散状态转换，揭示了RNN实现概率计算的新机制。


<details>
  <summary>Details</summary>
Motivation: 现有RNN研究主要关注简单、确定性行为，而自然行为往往是自发、随机且复杂的。需要探索RNN如何模拟HMM揭示的离散状态随机转换动态。

Method: 首先训练RNN复制HMM的发射统计，然后逆向工程分析训练后网络的动态机制，研究在不同HMM架构下的通用性。

Result: 训练后的RNN在没有输入时活动会坍缩到单一固定点；在随机输入驱动下，轨迹沿闭合轨道进行噪声维持的动态，通过"踢神经元"在慢动态区域间进行快速确定性转换。

Conclusion: RNN通过进入随机共振机制实现概率计算，这种解决方案在不同HMM架构中通过模块化重用相同的动态模式实现通用性，表明RNN可以组合模拟复杂离散潜在动态。

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [102] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出了卷积脉冲GRU（CS-GRU）单元，通过卷积操作保留局部结构，结合脉冲神经元的时间精度和GRU的门控机制，在时序和时空数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在处理长序列时会丢失局部细节，现有方法如SpikGRU无法捕捉事件型时空数据中的细粒度局部依赖关系。

Method: 设计卷积脉冲GRU单元，利用卷积操作保持局部结构依赖，同时集成脉冲神经元的时间精度和GRU的高效门控机制。

Result: 在时序数据集（NTIDIGITS、SHD）和时空基准测试（MNIST、DVSGesture、CIFAR10DVS）上表现优异，平均比最先进GRU变体提升4.35%，在MNIST上达到99.31%准确率，比SpikGRU效率高69%。

Conclusion: CS-GRU是一个多功能架构，在时序和时空数据处理任务中均表现出色，实现了高精度和高效率。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [103] [MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction](https://arxiv.org/abs/2510.25755)
*David S Maxwell,Michael Darkoh,Sidharth R Samudrala,Caroline Chung,Stephanie T Schmidt,Bissan Al-Lazikani*

Main category: cs.LG

TL;DR: MLPrE是一个基于SparkDataFrames的机器学习预处理和探索性数据分析工具，通过JSON配置文件描述数据处理步骤，提供69个处理阶段，支持多种数据格式的规模化预处理。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习需求的增长，现有工具在可扩展性和集成性方面存在局限，需要开发一个轻量级、可扩展的预处理工具来适应不同类型和规模的数据集。

Method: 利用SparkDataFrames存储数据确保可扩展性，采用通用JSON输入文件格式描述对DataFrame的分步处理，实现了输入输出、过滤、基础统计、特征工程和探索性数据分析等69个处理阶段。

Result: 在六个不同数据集上验证了关键阶段功能，展示了MLPrE能够独立处理平面文件中的多个字段并重新组合，使用UniProt术语数据集、葡萄酒质量数据和磷酸化位点激酶数据分别验证了聚类和图数据库准备功能。

Conclusion: MLPrE提供了一个通用且可扩展的预处理和早期数据分析工具，填补了机器学习应用中对这类工具的关键需求，能够加速和简化大型工作流中的早期开发阶段。

Abstract: With the recent growth of Deep Learning for AI, there is a need for tools to
meet the demand of data flowing into those models. In some cases, source data
may exist in multiple formats, and therefore the source data must be
investigated and properly engineered for a Machine Learning model or graph
database. Overhead and lack of scalability with existing workflows limit
integration within a larger processing pipeline such as Apache Airflow, driving
the need for a robust, extensible, and lightweight tool to preprocess arbitrary
datasets that scales with data type and size. To address this, we present
Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which
SparkDataFrames were utilized to hold data during processing and ensure
scalability. A generalizable JSON input file format was utilized to describe
stepwise changes to that DataFrame. Stages were implemented for input and
output, filtering, basic statistics, feature engineering, and exploratory data
analysis. A total of 69 stages were implemented into MLPrE, of which we
highlight and demonstrate key stages using six diverse datasets. We further
highlight MLPrE's ability to independently process multiple fields in flat
files and recombine them, otherwise requiring an additional pipeline, using a
UniProt glossary term dataset. Building on this advantage, we demonstrated the
clustering stage with available wine quality data. Lastly, we demonstrate the
preparation of data for a graph database in the final stages of MLPrE using
phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and
scalable tool for preprocessing and early data analysis, filling a critical
need for such a tool given the ever expanding use of machine learning. This
tool serves to accelerate and simplify early stage development in larger
workflows.

</details>


### [104] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: 论文分析了传统多示例学习在医学影像中的局限性，指出其忽略实例间上下文关系的问题，并通过合成分类任务验证了相关MIL方法的性能不足。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在处理医学影像时，将实例（如切片或补丁）独立处理，忽略了相邻实例之间的上下文关系，而这些关系在实际应用中至关重要。

Method: 设计了一个合成分类任务，其中相邻实例的特征对准确预测至关重要，并将现成MIL方法与最优贝叶斯估计器进行比较。

Result: 实验表明，即使是最新的相关MIL方法，在从数万个实例从头开始训练时，仍然难以达到最优泛化性能。

Conclusion: 当前MIL方法在处理实例间相关性方面仍有不足，需要进一步改进以充分利用上下文信息。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


### [105] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出神经随机流(NSFs)及其潜在变体，通过条件归一化流直接学习SDE转移规律，实现任意时间点的一步采样，比传统数值方法快两个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统SDE建模需要昂贵的数值求解器在任意时间点之间采样，计算成本高。

Method: 使用具有架构约束的条件归一化流直接学习SDE转移规律，保持随机流的数学特性。

Result: 在合成SDE模拟和真实世界跟踪、视频数据上，NSFs在保持分布准确性的同时，大幅减少了任意时间点采样的计算量。

Conclusion: NSFs提供了一种高效准确的SDE建模方法，特别适用于金融、物理和机器学习中的噪声和不规则采样时间序列。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [106] [A Compressive Sensing Inspired Monte-Carlo Method for Combinatorial Optimization](https://arxiv.org/abs/2510.24755)
*Baptiste Chevalier,Shimpei Yamaguchi,Wojciech Roga,Masahiro Takeoka*

Main category: math.OC

TL;DR: 提出了一种新的组合优化算法——蒙特卡洛压缩优化算法，用于解决可压缩的组合优化问题，通过随机查询目标函数估计广义矩，并利用压缩感知的贪心算法寻找全局最优解。


<details>
  <summary>Details</summary>
Motivation: 现有的组合优化方法在处理可压缩问题时效率有限，需要一种能够有效利用问题结构的新方法。

Method: 使用随机查询目标函数来估计广义矩，然后重新利用压缩感知中的贪心算法来寻找全局最优解，避免过拟合样本。

Result: 数值结果表明该方法优于当前最先进的双退火算法，并提供了理论证明和算法性质分析。

Conclusion: 该算法具有实用性，可以通过调整启发式参数来适应可用的计算资源。

Abstract: In this paper, we present the Monte-Carlo Compressive Optimization algorithm,
a new method to solve a combinatorial optimization problem that is assumed
compressible. The method relies on random queries to the objective function in
order to estimate generalized moments. Next, a greedy algorithm from
compressive sensing is repurposed to find the global optimum when not
overfitting to samples. We provide numerical results giving evidences that our
methods overcome state-of-the-art dual annealing. Moreover, we also give
theoretical justification of the algorithm success and analyze its properties.
The practicality of our algorithm is enhanced by the ability to tune heuristic
parameters to available computational resources.

</details>


### [107] [Convergence analysis for an implementable scheme to solve the linear-quadratic stochastic optimal control problem with stochastic wave equation](https://arxiv.org/abs/2510.24876)
*Abhishek Chaudhary*

Main category: math.OC

TL;DR: 该论文研究了由仿射乘性噪声驱动的随机波动方程的最优控制问题，提出了基于有限元离散和隐式中点格式的数值方法，并开发了计算成本与空间自由度成正比的梯度下降算法。


<details>
  <summary>Details</summary>
Motivation: 研究随机波动方程的最优控制问题，旨在开发高效且可扩展的数值计算方法，避免昂贵的蒙特卡洛采样。

Method: 应用随机Pontryagin极大值原理，通过耦合的前向-后向SPDE系统表征最优状态-控制对；采用共形有限元空间离散和隐式中点时间离散；开发基于人工迭代的梯度下降算法，精确计算条件期望。

Result: 获得了离散状态-控制对的强收敛率，无需依赖Malliavin微积分；提出的梯度下降算法计算成本与空间自由度成正比，保持了强收敛率。

Conclusion: 数值结果验证了该方法的效率，为随机波动方程最优控制问题提供了可扩展的数值解决方案。

Abstract: We study an optimal control problem for the stochastic wave equation driven
by affine multiplicative noise, formulated as a stochastic linear-quadratic
(SLQ) problem. By applying a stochastic Pontryagin's maximum principle, we
characterize the optimal state-control pair via a coupled forward-backward SPDE
system. We propose an implementable discretization using conforming finite
elements in space and an implicit midpoint rule in time. By a new technical
approach we obtain strong convergence rates for the discrete state-control pair
without relying on Malliavin calculus. For the practical computation we develop
a gradient-descent algorithm based on artificial iterates that employs an exact
computation for the arising conditional expectations, thereby eliminating
costly Monte Carlo sampling. Consequently, each iteration has a computational
cost that is proportional to the number of spatial degrees of freedom,
producing a scalable method that preserves the established strong convergence
rates. Numerical results validate its efficiency.

</details>


### [108] [Zeroth-order gradient estimators for stochastic problems with decision-dependent distributions](https://arxiv.org/abs/2510.24929)
*Yuya Hikima,Akiko Takeda*

Main category: math.OC

TL;DR: 本文对具有未知决策相关分布的随机优化问题进行了统一的样本复杂度分析，比较了不同搜索方向构建的零阶方法梯度估计器，发现多方向平均的梯度估计器能达到最低样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 由于目标函数的梯度因未知分布而无法获取，需要开发零阶方法来解决这类问题，但目前不清楚哪种搜索方向构建的梯度估计器更合适以及如何设置算法参数。

Method: 对具有不同搜索方向的零阶方法梯度估计器进行统一的样本复杂度分析，比较了单位球面均匀采样和高斯分布采样等多种方向平均方法。

Result: 研究表明，在单位球面或高斯分布上平均多个方向的梯度估计器能达到最低样本复杂度，其样本复杂度优于现有零阶方法，适用于非凸和无界目标函数问题。

Conclusion: 多方向平均的梯度估计器在样本复杂度方面表现最优，通过多个产品定价和战略分类应用的仿真实验验证了各种梯度估计器的实际性能。

Abstract: Stochastic optimization problems with unknown decision-dependent
distributions have attracted increasing attention in recent years due to its
importance in applications. Since the gradient of the objective function is
inaccessible as a result of the unknown distribution, various zeroth-order
methods have been developed to solve the problem. However, it remains unclear
which search direction to construct a gradient estimator is more appropriate
and how to set the algorithmic parameters. In this paper, we conduct a unified
sample complexity analysis of zeroth-order methods across gradient estimators
with different search directions. As a result, we show that gradient estimators
that average over multiple directions, either uniformly from the unit sphere or
from a Gaussian distribution, achieve the lowest sample complexity. The
attained sample complexities improve those of existing zeroth-order methods in
the problem setting that allows nonconvexity and unboundedness of the objective
function. Moreover, by simulation experiments on multiple products pricing and
strategic classification applications, we show practical performance of
zeroth-order methods with various gradient estimators.

</details>


### [109] [Adaptive Multilevel Newton: A Quadratically Convergent Optimization Method](https://arxiv.org/abs/2510.24967)
*Nick Tsipinakis,Panos Parpas,Matthias Voigt*

Main category: math.OC

TL;DR: 提出了一种自适应多级牛顿型方法，能在达到二次收敛阶段时自动切换到完整牛顿法，解决了牛顿法在初始阶段收敛慢的问题。


<details>
  <summary>Details</summary>
Motivation: 牛顿法在强凸问题初始阶段可能比梯度下降收敛更慢，而经典多级牛顿方法在接近极小值时只能实现线性收敛。

Method: 自适应多级牛顿型方法，具有自动切换到完整牛顿法的机制，在达到二次收敛阶段时启用。

Result: 对于具有Lipschitz连续Hessian的强凸函数和自协调函数，建立了局部二次收敛性，并通过实验验证。方法在效率上一致优于牛顿法、梯度下降和多级牛顿法。

Conclusion: 即使牛顿法初始收敛较慢，二阶方法仍能超越一阶方法，自适应切换机制能有效提升收敛性能。

Abstract: Newton's method may exhibit slower convergence than vanilla Gradient Descent
in its initial phase on strongly convex problems. Classical Newton-type
multilevel methods mitigate this but, like Gradient Descent, achieve only
linear convergence near the minimizer. We introduce an adaptive multilevel
Newton-type method with a principled automatic switch to full Newton once its
quadratic phase is reached. The local quadratic convergence for strongly convex
functions with Lipschitz continuous Hessians and for self-concordant functions
is established and confirmed empirically. Although per-iteration cost can
exceed that of classical multilevel schemes, the method is efficient and
consistently outperforms Newton's method, Gradient Descent, and the multilevel
Newton method, indicating that second-order methods can outperform first-order
methods even when Newton's method is initially slow.

</details>


### [110] [Star Quasiconvexity: an Unified Approach for Linear Convergence of First-Order Methods Beyond Convexity](https://arxiv.org/abs/2510.24981)
*Phan Quoc Khanh,Felipe Lara*

Main category: math.OC

TL;DR: 提出了星拟凸函数类，确保梯度和近端点方法的线性收敛性，包含凸、星凸、拟凸和类星凸函数。建立了星拟凸函数的等价条件，并证明了在强星拟凸函数上近端点算法线性收敛到唯一解。


<details>
  <summary>Details</summary>
Motivation: 为了扩展一阶优化方法的收敛性保证，需要更广泛的函数类，这些函数类包含经典凸函数但具有更强的收敛性质。

Method: 引入星拟凸函数类，建立其与子水平集星形性的等价关系，提供多种特征化，并分析近端点算法在强星拟凸函数上的收敛性。

Result: 证明了星拟凸函数当且仅当其所有子水平集关于最小化点集是星形的，且近端点算法在强星拟凸函数上线性收敛到唯一解。

Conclusion: 星拟凸函数类为优化算法提供了更广泛的收敛保证框架，扩展了经典凸优化的适用范围。

Abstract: We introduce a class of generalized convex functions, termed star
quasiconvexity, to ensure the linear convergence of gradient and proximal point
methods. This class encompasses convex, star-convex, quasiconvex, and
quasar-convex functions. We establish that a function is star quasiconvex if
and only if all its sublevel sets are star-shaped with respect to the set of
its minimizers. Furthermore, we provide several characterizations of this
class, including nonsmooth and differentiable cases, and derive key properties
that fa\-ci\-li\-ta\-te the implementation of first-order methods. Finally, we
prove that the proximal point algorithm converges linearly to the unique
solution when applied to strongly star quasiconvex functions defined over
closed, star-shaped sets, which are not necessarily convex.

</details>


### [111] [Nonlinear Dynamics In Optimization Landscape of Shallow Neural Networks with Tunable Leaky ReLU](https://arxiv.org/abs/2510.25060)
*Jingzhou Liu*

Main category: math.OC

TL;DR: 本文研究了具有泄漏ReLU激活函数的浅层神经网络在均方损失下的非线性动力学，建立了基于等变梯度度的理论框架来分析临界点的分岔行为，发现多模态退化在临界点0处一致发生且与网络宽度无关。


<details>
  <summary>Details</summary>
Motivation: 研究浅层神经网络在训练过程中的非线性动力学行为，特别是临界点的分岔现象及其对称性，以理解神经网络优化过程中的复杂行为。

Method: 使用等变梯度度理论框架，在高斯输入和等层宽度的条件下，分析泄漏ReLU参数α变化时临界点的分岔行为，并通过k=5的具体示例进行验证。

Result: 发现多模态退化在临界点0处一致发生且与网络宽度k无关，分岔仅发生在非负α值，且全局最小值在α∈(0,1)范围内不会发生进一步的对称性破坏不稳定性。

Conclusion: 建立了一个适用于任意神经元数k≥4的理论框架，揭示了浅层神经网络训练中临界点分岔的基本规律，为理解神经网络优化动力学提供了理论依据。

Abstract: In this work, we study the nonlinear dynamics of a shallow neural network
trained with mean-squared loss and leaky ReLU activation. Under Gaussian inputs
and equal layer width k, (1) we establish, based on the equivariant gradient
degree, a theoretical framework, applicable to any number of neurons k>= 4, to
detect bifurcation of critical points with associated symmetries from global
minimum as leaky parameter $\alpha$ varies. Typically, our analysis reveals
that a multi-mode degeneracy consistently occurs at the critical number 0,
independent of k. (2) As a by-product, we further show that such bifurcations
are width-independent, arise only for nonnegative $\alpha$ and that the global
minimum undergoes no further symmetry-breaking instability throughout the
engineering regime $\alpha$ in range (0,1). An explicit example with k=5 is
presented to illustrate the framework and exhibit the resulting bifurcation
together with their symmetries.

</details>


### [112] [Optimal Control Strategies for Multi-Agent Sheep Herding](https://arxiv.org/abs/2510.25115)
*Drake Brown,Trevor Garrity,Daniel Perkins,Davis Hunter,Wyatt Pochman*

Main category: math.OC

TL;DR: 该论文研究了使用n只狗将m只羊赶到原点的最优控制问题，比较了边界值问题求解器、打靶法和线性化iLQR方法，发现iLQR在可扩展性方面表现更好但存在非线性区域收敛问题。


<details>
  <summary>Details</summary>
Motivation: 开发多智能体控制模型来解决羊群驱赶问题，探索不同数值方法在处理高维非线性系统时的表现。

Method: 建立了成本函数和状态空间方程，使用solve_bvp、打靶法和线性化iLQR三种方法求解最优控制轨迹。

Result: solve_bvp和打靶法收敛困难，iLQR方法更具可扩展性但会在狗羊近距离时产生振荡路径，当羊数多于狗数时收敛缓慢。

Conclusion: 标准数值技术在多智能体控制中存在局限性，需要更鲁棒的非线性策略来协调交互智能体。

Abstract: We develop a cost functional and state-space equations to model the problem
of herding m sheep to the origin using n dogs. Our initial approach uses
solve_bvp to approximate optimal control trajectories. But this method often
fails to converge due to the system's high dimensionality and nonlinearity.
However, with a well-chosen initial guess and carefully selected
hyperparameters, we succeed in getting solve_bvp to converge. We also explore
alternatives including the shooting method and linearization with the iterative
Linear Quadratic Regulator (iLQR). While the shooting method also suffers from
poor convergence, the linearized iLQR approach proves more scalable and
successfully handles scenarios with more agents. However, it struggles in
regions where dogs and sheep are in close proximity, due to strong
nonlinearities that violate the assumptions of local linearization. This leads
to jagged, oscillatory paths and slow convergence, particularly when the number
of sheep exceeds the number of dogs. These challenges reveal key limitations of
standard numerical techniques in multi-agent control and underscore the need
for more robust, nonlinear strategies for coordinating interacting agents.

</details>


### [113] [Minimum time consensus for damped second order agents using Gröbner basis](https://arxiv.org/abs/2510.25243)
*Akansha Rautela,Deepak U. Patil,Ameer Mulla,Indra Narayan Kar*

Main category: math.OC

TL;DR: 研究二阶LTI系统智能体在有界输入和燃料约束下的最小时间一致性问题，考虑了阻尼效应的影响


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统在燃料约束和阻尼效应下的最小时间一致性问题，扩展了之前不考虑阻尼的工作

Method: 首先描述每个智能体在燃料预算约束下的可达集并推导边界方程，利用凸性性质计算所有智能体可达集首次出现非空交的最小时间，应用Helly定理将问题简化为对每个三元组分别求解最小时间一致性和对应的一致点

Result: 提出了计算最小时间一致性的方法，通过可达集分析和Helly定理的应用简化了计算复杂度

Conclusion: 该方法有效解决了考虑阻尼效应的二阶LTI系统在燃料约束下的最小时间一致性问题

Abstract: A problem of achieving minimum time consensus for a set of $N$ second-order
LTI system agents with bounded inputs and fuel constraints is considered.
Unlike our other works, here the damping effect in agent dynamics is included.
First, the attainable set for each agent with fuel budget constraints is
characterized, and its boundary equations are derived. Then, using the
convexity property, the minimum time at which attainable sets of all agents
have a non-empty intersection is computed. By applying Helly's theorem, the
computation reduces to finding the minimum time to consensus and the
corresponding consensus point for each of the triplets separately.

</details>


### [114] [Convergence of a Relative-type Inexact Proximal ALM for Convex Nonlinear Programming](https://arxiv.org/abs/2510.25261)
*Lei Yang,Jiayi Zhu,Ling Liang,Kim-Chuan Toh*

Main category: math.OC

TL;DR: 本文研究了相对型不精确邻近增广拉格朗日方法(ripALM)在凸非线性规划中的收敛性，建立了其全局收敛性和渐近(超)线性收敛速率。


<details>
  <summary>Details</summary>
Motivation: 不精确邻近增广拉格朗日方法因其理论特性和实际性能而有效，但相对型不精确变体的收敛行为尚未充分理解，本文旨在填补这一空白。

Method: 采用相对型不精确邻近增广拉格朗日方法(ripALM)进行收敛性分析，在标准假设下建立理论框架。

Result: 严格证明了ripALM生成序列的全局收敛性，以及其渐近(超)线性收敛速率，并推导了关于原始可行性违反和原始目标残差的全局遍历收敛速率。

Conclusion: 研究为ripALM的整体性能提供了更全面的表征，填补了相对型不精确变体收敛理论的重要空白。

Abstract: This article investigates the convergence properties of a relative-type
inexact proximal augmented Lagrangian method (ripALM) for convex nonlinear
programming, a fundamental class of optimization problems with broad
applications in science and engineering. Inexact proximal augmented Lagrangian
methods have proven to be highly effective for solving such problems, owing to
their attractive theoretical properties and strong practical performance.
However, the convergence behavior of the relative-type inexact variant remains
insufficiently understood. This work aims to reduce this gap by rigorously
establishing the global convergence of the sequence generated by ripALM and
proving its asymptotic (super)linear convergence rate under standard
assumptions. In addition, we derive the global ergodic convergence rate with
respect to both the primal feasibility violation and the primal objective
residual, thereby offering a more comprehensive characterization of the overall
performance of ripALM.

</details>


### [115] [On the Rate of Convergence of Iterative Methods for Nonexpansive Mappings in CAT(0) Spaces and Hyperbolic Optimization](https://arxiv.org/abs/2510.25363)
*Katherine Rossella Foglia,Vittorio Colao*

Main category: math.OC

TL;DR: 本文扩展了Krasnosel'ski\u{\u0131} Mann和Halpern迭代方法到CAT(0)空间，建立了与线性空间相同的渐近正则性界限，并提出了基于Halpern迭代的双曲深度学习优化器。


<details>
  <summary>Details</summary>
Motivation: 将线性Banach和Hilbert空间中发展的证明技术扩展到非线性CAT(0)空间，以恢复相同的渐近正则性界限，并为双曲深度学习开发新的优化方法。

Method: 将Banach和Hilbert空间中的证明技术扩展到完整CAT(0)空间，并基于Halpern迭代设计双曲深度学习优化器，类似于欧几里得空间中的HalpernSGD。

Result: 在CAT(0)空间中获得了与线性空间相同的渐近正则性界限，并成功开发了基于Halpern迭代的双曲深度学习优化器。

Conclusion: 证明技术可以成功从线性空间扩展到非线性CAT(0)空间，Halpern迭代方法在双曲深度学习优化中具有应用潜力。

Abstract: The Krasnosel'ski\u{\i} Mann and Halpern iterations are classical schemes for
approximating fixed points of nonexpansive mappings in Banach spaces, and have
been widely studied in more general frameworks such as $CAT(\kappa)$ and, more
generally, geodesic spaces. Convergence results and convergence rate estimates
in these nonlinear settings are already well established. The contribution of
this paper is to extend to complete $CAT(0)$ spaces the proof techniques
originally developed in the linear setting of Banach and Hilbert spaces,
thereby recovering the same asymptotic regularity bounds and to introduce a
novel optimizer for Hyperbolic Deep learning based on Halpern Iteration
similarly to HalpernSGD \cite{foglia2024halpernsgd,colao2025optimizer} in
Euclidean setting.

</details>


### [116] [Averaging favors MPC: How typical evaluation setups overstate MPC performance for residential battery scheduling](https://arxiv.org/abs/2510.25373)
*Janik Pinter,Maximilian Beichter,Ralf Mikut,Frederik Zahn,Veit Hagenmeyer*

Main category: math.OC

TL;DR: 研究比较了MPC和RBC在考虑净计量和电池退化时的性能，发现基于时间平均的评估会高估MPC的优势，当使用分钟级真实成本重新计算时，MPC的优势平均减少69%。


<details>
  <summary>Details</summary>
Motivation: 住宅光伏-电池系统用户越来越多地管理其与电网的电力交换以最小化成本，但现有研究常使用15/30/60分钟平均数据进行评估，这可能影响控制策略性能评估的准确性。

Method: 对德国北部的15栋建筑进行连续5个月的模拟，在15/30/60分钟调度分辨率下生成高达1分钟分辨率的成本，比较MPC和RBC在考虑净计量和电池退化时的性能。

Result: 时间平均评估使MPC看起来始终优于RBC，但当使用分钟级真实成本重新计算时，MPC的优势平均减少69%。对于个别建筑，更精细的评估可能逆转结论，简单的RBC甚至可能比具有完美预见性的MPC实现更低的总成本。

Conclusion: 研究警告不要从粗略平均中得出结论，并展示了如何获得电池调度方法的公平评估，强调评估方法对控制策略性能比较的重要性。

Abstract: Residential prosumers with PV-battery systems increasingly manage their
electricity exchange with the power grid to minimize costs. This study
investigates the performance of Model Predictive Control (MPC) and Rule-Based
Control (RBC) under 15/30/60 minute averaging commonly used in research, when
Net Billing and battery degradation are considered. We simulate five
consecutive months for 15 buildings in northern Germany, generating costs at up
to 1-minute resolution while scheduling at 15/30/60 minutes. We find that
time-averaged evaluations make MPC look consistently better than RBC, yet when
costs are recomputed at minute-level ground-truth, the reported advantage
shrinks by 69\% on average for hourly schedulers. For individual buildings, the
finer evaluation can reverse conclusions, and simple RBC can achieve lower
total costs than an MPC with perfect foresight. These findings caution against
drawing conclusions from coarse averages and show how a fair assessment of
battery scheduling approaches can be obtained.

</details>


### [117] [Centralized and Competitive Extraction for Distributed Renewable Resources with Nonlinear Reproduction](https://arxiv.org/abs/2510.25398)
*Filippo de Feo,Giorgio Fabbri,Silvia Faggian,Giuseppe Freni*

Main category: math.OC

TL;DR: 该论文研究网络化可再生资源的最优和策略性开采，考虑了资源在节点间的质量守恒迁移和非线性增长。分析了集中规划者和非合作博弈，在三种典型增长规律下推导出闭式解和马尔可夫均衡。


<details>
  <summary>Details</summary>
Motivation: 研究网络化可再生资源开采问题，考虑资源在节点间的迁移和非线性增长特性，填补了现有文献在非线性增长和一般网络结构下缺乏显式策略的空白。

Method: 使用集中规划者和非合作博弈框架，采用平稳马尔可夫策略，结合Perron-Frobenius几何分析长期空间分配，对三种典型增长规律（逻辑斯蒂、幂函数、对数饱和型）推导闭式解。

Result: 在强连通网络上推导出闭式价值函数和反馈规则，构建了对称马尔可夫均衡，首次在一般网络结构下获得非线性增长空间资源开采的显式策略。

Conclusion: 该研究首次在一般网络结构下获得了非线性增长空间资源开采的显式策略和闭式马尔可夫均衡，为网络化可再生资源管理提供了理论基础。

Abstract: We study optimal and strategic extraction of a renewable resource that is
distributed over a network, migrates mass-conservatively across nodes, and
evolves under nonlinear (concave) growth. A subset of nodes hosts extractors
while the remaining nodes serve as reserves. We analyze a centralized planner
and a non-cooperative game with stationary Markov strategies. The migration
operator transports shadow values along the network so that Perron-Frobenius
geometry governs long-run spatial allocations, while nonlinear growth couples
aggregate biomass with its spatial distribution and bounds global dynamics. For
three canonical growth families, logistic, power, and log-type saturating laws,
under related utilities, we derive closed-form value functions and feedback
rules for the planner and construct a symmetric Markov equilibrium on strongly
connected networks. To our knowledge, this is the first paper to obtain
explicit policies for spatial resource extraction with nonlinear growth and, a
fortiori, closed-form Markov equilibria, on general networks.

</details>


### [118] [Data-Driven Stabilization Using Prior Knowledge on Stabilizability and Controllability](https://arxiv.org/abs/2510.25452)
*Amir Shakouri,Henk J. van Waarde,Tren M. J. T. Baltussen,W. P. M. H.,Heemels*

Main category: math.OC

TL;DR: 该论文研究了利用系统理论特性（稳定性和可控性）作为先验知识的数据驱动线性时不变系统稳定性分析，证明了当系统可稳定时，使用先验知识可以放宽数据驱动稳定性分析的条件。


<details>
  <summary>Details</summary>
Motivation: 研究如何将系统理论特性（如稳定性和可控性）作为先验知识融入数据驱动的稳定性分析中，以改进数据驱动控制设计。

Method: 扩展数据信息性概念，要求存在一个控制器能够稳定所有与数据和先验知识一致的系统，并提供基于线性矩阵不等式的数据驱动控制设计方法。

Result: 当系统可控时，使用先验知识不会放宽数据驱动稳定性分析的条件；但当系统可稳定时，使用先验知识可以放宽条件，使数据驱动稳定性分析更容易。

Conclusion: 利用系统可稳定性作为先验知识可以简化数据驱动稳定性分析，为数据驱动控制提供了新的设计方法和理论支持。

Abstract: In this work, we study data-driven stabilization of linear time-invariant
systems using prior knowledge of system-theoretic properties, specifically
stabilizability and controllability. To formalize this, we extend the concept
of data informativity by requiring the existence of a controller that
stabilizes all systems consistent with the data and the prior knowledge. We
show that if the system is controllable, then incorporating this as prior
knowledge does not relax the conditions required for data-driven stabilization.
Remarkably, however, we show that if the system is stabilizable, then using
this as prior knowledge leads to necessary and sufficient conditions that are
weaker than those for data-driven stabilization without prior knowledge. In
other words, data-driven stabilization is easier if one knows that the
underlying system is stabilizable. We also provide new data-driven control
design methods in terms of linear matrix inequalities that complement the
conditions for informativity.

</details>


### [119] [A strong formulation for Multiple Allocation Hub Location based on supermodular inequalities](https://arxiv.org/abs/2510.25490)
*Elena Fernández,Nicolás Zerega*

Main category: math.OC

TL;DR: 提出了一种新的多分配枢纽位置问题公式，利用超模性质且仅使用1-和2-索引变量，其线性规划边界与使用4-索引变量的最紧现有公式相同。


<details>
  <summary>Details</summary>
Motivation: 现有最紧的多分配枢纽位置问题公式使用4-索引变量，计算复杂度高。需要开发更简洁有效的公式。

Method: 基于超模性质的新公式，仅使用1-和2-索引变量，避免了复杂的4-索引变量结构。

Result: 新公式的线性规划边界与现有最紧公式相同，但计算效率更高，能在2小时内最优求解最多200个节点的实例。

Conclusion: 新公式在保持理论紧度的同时显著提升了计算效率，为大规模枢纽位置问题提供了实用解决方案。

Abstract: We introduce a new formulation for the multiple allocation hub location
problem that exploits supermodular properties and uses 1- and 2-index variables
only. We show that the new formulation produces the same Linear Programming
bound as the tightest existing formulations for the studied problem, which use
4-index variables, outperforming existing supermodular formulations adapted to
the considered problem. Computational results are presented with instances of
up to 200 nodes optimally solved within a time limit of two hours.

</details>


### [120] [Stochastic Control of Dividends with a Drawdown Penalty](https://arxiv.org/abs/2510.25494)
*Kira Dudziak,Hanspeter Schmidli*

Main category: math.OC

TL;DR: 研究带提款约束的扩散风险模型中的最优分红策略，证明最优分红率要么为0要么为最大值，并推导出价值函数的显式表达式


<details>
  <summary>Details</summary>
Motivation: 在扩散风险模型中最大化分红支付，同时考虑提款约束（当提款规模超过阈值d>0时施加惩罚）

Method: 通过求解微分方程组，确定最优分红策略，证明最优分红率U(t)要么为0要么为最大率u0

Result: 推导出最优分红策略的显式表达式，并得到价值函数的解析解

Conclusion: 在提款约束下，最优分红策略是bang-bang型的（要么不分红，要么以最大速率分红），通过求解微分方程组获得了价值函数的显式表达式

Abstract: We consider a diffusion risk model where dividends are paid at rate $U(t) \in
[0, u_0]$. We are interested in maximising the dividend payments under a
drawdown constraint, that is, we penalise a drawdown size larger than a level
$d > 0$. We show that the optimal dividend rate $U(t)$ is either zero or the
maximal rate $u_0$ and determine the optimal strategy. Moreover, we derive an
explicit expression for the value function by solving a system of differential
equations.

</details>


### [121] [Sum-of-Squares Certificates for Almost-Sure Reachability of Stochastic Polynomial Systems](https://arxiv.org/abs/2510.25513)
*Arash Bahari Kordabad,Rupak Majumdar,Sadegh Soudjani*

Main category: math.OC

TL;DR: 提出一种基于SOS优化的计算方法来验证离散时间多项式随机系统的几乎必然可达性，通过漂移-变分证书转化为可求解的半定规划问题。


<details>
  <summary>Details</summary>
Motivation: 现有随机系统可达性验证方法缺乏有效的计算工具，需要开发能够处理多项式随机系统的可计算验证框架。

Method: 使用两种互补证书：(i)漂移证书确保径向无界函数在紧集外期望非增；(ii)变分证书保证一步下降概率为正且目标包含非正子水平集。通过S-procedure和交替优化处理双线性约束。

Result: 将验证条件转化为SOS约束，通过标准半定求解器求解。两个案例研究验证了方法的有效性，成功证明了几乎必然可达性。

Conclusion: 该方法为多项式随机系统的几乎必然可达性验证提供了可计算框架，通过SOS优化和交替策略有效处理了验证条件。

Abstract: In this paper, we present a computational approach to certify almost sure
reachability for discrete-time polynomial stochastic systems by turning
drift--variant criteria into sum-of-squares (SOS) programs solved with standard
semidefinite solvers. Specifically, we provide an SOS method based on two
complementary certificates: (i) a drift certificate that enforces a radially
unbounded function to be non-increasing in expectation outside a compact set of
states; and (ii) a variant certificate that guarantees a one-step decrease with
positive probability and ensures the target contains its nonpositive sublevel
set. We transform these conditions to SOS constraints. For the variant
condition, we enforce a robust decrease over a parameterized disturbance ball
with nonzero probability and encode the constraints via an S-procedure with
polynomial multipliers. The resulting bilinearities are handled by an
alternating scheme that alternates between optimizing multipliers and updating
the variant and radius until a positive slack is obtained. Two case studies
illustrate the workflow and certifies almost-sure reachability.

</details>


### [122] [Local Convergence of Adaptively Regularized Tensor Methods](https://arxiv.org/abs/2510.25643)
*Karl Welzel,Yang Liu,Raphael A. Hauser,Coralia Cartis*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Optimization methods that make use of derivatives of the objective function
up to order $p > 2$ are called tensor methods. Among them, ones that minimize a
regularized $p$th-order Taylor expansion at each step have been shown to
possess optimal global complexity, which improves as $p$ increases. The local
convergence of such optimization algorithms on functions that have Lipschitz
continuous $p$th derivatives and are uniformly convex of order $q$ has been
studied by Doikov and Nesterov [Math. Program., 193 (2022), pp. 315--336]. We
extend these local convergence results to locally uniformly convex functions
and fully adaptive methods, which do not need knowledge of the Lipschitz
constant, thus providing the first sharp local rates for AR$p$. We discuss the
surprising new challenges encountered by nonconvex local models and non-unique
model minimizers. For $p > 2$, our examples show that in particular when using
the global minimizer of the subproblem, even asymptotically not all iterations
need to be successful. Only if the "right" local model minimizer is used, the
$p/(q-1)$th-order local convergence from the non-adaptive case is preserved for
$p > q-1$, otherwise the superlinear rate can degrade. We thus confirm that
adaptive higher-order methods achieve superlinear convergence for certain
degenerate problems as long as $p$ is large enough and provide sharp bounds on
the order of convergence one can expect in the limit.

</details>


### [123] [A Low-Rank Symplectic Gradient Adjustment Method for Computing Nash Equilibria](https://arxiv.org/abs/2510.25716)
*Nadja Vater,Katherine Rossella Foglia,Vittorio Colao,Alfio Borzì*

Main category: math.OC

TL;DR: 本文提出了用于求解双目标优化问题的辛梯度调整(SGA)方法和低秩SGA(LRSGA)方法，LRSGA通过秩一更新近似二阶混合导数，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: SGA方法虽然比梯度方法性能更好，但需要计算二阶混合导数，计算代价较大。因此需要开发计算效率更高的方法。

Method: 提出了LRSGA方法，通过秩一更新来近似二阶混合导数，降低了计算复杂度。对SGA和LRSGA方法进行了理论分析，包括收敛性估计和参数边界。

Result: 理论分析给出了SGA和LRSGA方法的收敛性估计。在CLIP神经网络架构训练中，LRSGA方法的CPU时间比SGA方法小几个数量级。

Conclusion: LRSGA方法在保持良好性能的同时，显著降低了计算复杂度，在神经网络训练等应用中具有明显优势。

Abstract: This work presents a theoretical and numerical investigation of the
symplectic gradient adjustment (SGA) method and of a low-rank SGA (LRSGA)
method for efficiently solving two-objective optimization problems in the
framework of Nash games. The SGA method outperforms the gradient method by
including second-order mixed derivatives computed at each iterate, which
requires considerably larger computational effort. For this reason, a LRSGA
method is proposed where the approximation to second-order mixed derivatives
are obtained by rank-one updates. The theoretical analysis presented in this
work focuses on novel convergence estimates for the SGA and LRSGA methods,
including parameter bounds. The superior computational complexity of the LRSGA
method is demonstrated in the training of a CLIP neural architecture, where the
LRSGA method outperforms the SGA method by orders of magnitude smaller CPU
time.

</details>
