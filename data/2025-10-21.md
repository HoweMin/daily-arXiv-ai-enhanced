<div id=toc></div>

# Table of Contents

- [math.OC](#math.OC) [Total: 30]
- [cs.AI](#cs.AI) [Total: 70]
- [cs.LG](#cs.LG) [Total: 224]


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [1] [Fitting an Escalier to a Curve](https://arxiv.org/abs/2510.16148)
*Sebastien Bossu,Andrew Papanicolaou,Nour El Hatto*

Main category: math.OC

TL;DR: 提出了一种在L^2希尔伯特空间中用多步函数拟合曲线的两阶段优化方法，通过固定步长位置和递归求解一阶条件实现全局最优拟合。


<details>
  <summary>Details</summary>
Motivation: 解决在L^2希尔伯特空间中用阶梯函数拟合曲线的问题，寻求一种能够高效找到全局最优解的方法。

Method: 采用两阶段优化方法：第一阶段固定步长位置，使用经典线性最小二乘法求解；第二阶段允许步长位置变化，通过递归求解一阶条件。

Result: 在满足正则性条件下，当步数n趋于无穷时收敛速度为线性；基于扫描搜索实现的数值结果在速度和精度方面表现出良好性能。

Conclusion: 该方法能够有效恢复全局最优拟合，在数值实验中展现出快速和准确的性能。

Abstract: We analyze the problem of fitting a fonction en escalier or multi-step
function to a curve in L^2 Hilbert space. We propose a two-stage optimization
approach whereby the step positions are initially fixed, corresponding to a
classic linear least-squares problem with closed-form solution, and then are
allowed to vary, leading to first-order conditions that can be solved
recursively. We find that, subject to regularity conditions, the speed of
convergence is linear as the number of steps $n$ goes to infinity, and we
develop a simple algorithm to recover the global optimum fit. Our numerical
results based on a sweep search implementation show promising performance in
terms of speed and accuracy.

</details>


### [2] [Agent-Based Optimal Control for Image Processing](https://arxiv.org/abs/2510.16154)
*Alessio Oliviero,Simone Cacace,Giuseppe Visconti*

Main category: math.OC

TL;DR: 使用多智能体系统解决图像处理任务，将任务建模为最优控制问题，通过平衡颜色场的总变差和原始图像保真度来获得颜色聚类分割图像。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统在经典图像处理任务（如颜色量化和分割）中的应用潜力，将图像处理问题转化为最优控制问题。

Method: 将图像处理任务建模为最优控制问题，使用原始-对偶分裂方法和乘子法求解，通过平衡颜色场的总变差和原始图像保真度来引导多智能体动态。

Result: 数值实验（使用CUDA并行实现）证明了该方法的有效性，并展示了其在高维数据处理方面的潜力。

Conclusion: 多智能体系统框架为图像处理任务提供了有效的解决方案，特别是在高维数据处理方面具有良好前景。

Abstract: We investigate the use of multi-agent systems to solve classical image
processing tasks, such as colour quantization and segmentation. We frame the
task as an optimal control problem, where the objective is to steer the
multi-agent dynamics to obtain colour clusters that segment the image. To do
so, we balance the total variation of the colour field and fidelity to the
original image. The solution is obtained resorting to primal-dual splitting and
the method of multipliers. Numerical experiments, implemented in parallel with
CUDA, demonstrate the efficacy of the approach and its potential for
high-dimensional data.

</details>


### [3] [Conformal Prediction in The Loop: A Feedback-Based Uncertainty Model for Trajectory Optimization](https://arxiv.org/abs/2510.16376)
*Han Wang,Chao Ning*

Main category: math.OC

TL;DR: 提出了一种基于反馈的共形预测框架(Fb-CP)，用于具有联合风险约束的收缩时域轨迹优化，通过将已实现轨迹的信息反馈给CP来调整预测区域，提高轨迹性能并保持覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用顺序方案，决策单向依赖预测区域，而决策信息无法反馈给CP进行指导。需要一种能够双向交互的框架来改进轨迹优化性能。

Method: 开发了基于CP的后验风险计算方法，利用已实现轨迹调整后验允许风险，并将其分配到未来时间以更新预测区域。还提出了决策聚焦的迭代风险分配算法。

Result: 该方法能够持续反馈已实现轨迹信息，实现预测区域的有吸引力的反馈调整，并在轨迹性能上提供可证明的在线改进，同时保持预测区域的覆盖保证。

Conclusion: 提出的Fb-CP框架在基准实验中表现出有效性和优越性，能够处理分布偏移，为轨迹优化提供了可证明的安全保障和性能提升。

Abstract: Conformal Prediction (CP) is a powerful statistical machine learning tool to
construct uncertainty sets with coverage guarantees, which has fueled its
extensive adoption in generating prediction regions for decision-making tasks,
e.g., Trajectory Optimization (TO) in uncertain environments. However, existing
methods predominantly employ a sequential scheme, where decisions rely
unidirectionally on the prediction regions, and consequently the information
from decision-making fails to be fed back to instruct CP. In this paper, we
propose a novel Feedback-Based CP (Fb-CP) framework for shrinking-horizon TO
with a joint risk constraint over the entire mission time. Specifically, a
CP-based posterior risk calculation method is developed by fully leveraging the
realized trajectories to adjust the posterior allowable risk, which is then
allocated to future times to update prediction regions. In this way, the
information in the realized trajectories is continuously fed back to the CP,
enabling attractive feedback-based adjustments of the prediction regions and a
provable online improvement in trajectory performance. Furthermore, we
theoretically prove that such adjustments consistently maintain the coverage
guarantees of the prediction regions, thereby ensuring provable safety.
Additionally, we develop a decision-focused iterative risk allocation algorithm
with theoretical convergence analysis for allocating the posterior allowable
risk which closely aligns with Fb-CP. Furthermore, we extend the proposed
method to handle distribution shift. The effectiveness and superiority of the
proposed method are demonstrated through benchmark experiments.

</details>


### [4] [A Simple First-Order Algorithm for Full-Rank Equality Constrained Optimization](https://arxiv.org/abs/2510.16390)
*Serge Gratton,Philippe L. Toint*

Main category: math.OC

TL;DR: 提出一种简单的一阶算法，用于求解具有确定性非线性等式约束的优化问题。该算法自适应选择沿约束切平面的步长或减少不可行性的步长，不使用价值函数或过滤器，适用于含噪声问题。


<details>
  <summary>Details</summary>
Motivation: 针对含非线性等式约束的优化问题，特别是梯度存在噪声的情况，开发一种简单高效的一阶算法，避免使用复杂的价值函数或过滤器机制。

Method: 算法自适应选择两种步长：沿约束切平面的步长（基于AdaGrad方法）和减少不可行性的步长。关键特点是不评估目标函数值，仅使用梯度信息。

Result: 理论分析显示最坏情况评估复杂度为O(1/√k)，与无约束问题一阶方法的最佳已知收敛率匹配。数值实验表明性能与无约束一阶方法相当，且在梯度噪声下可靠性稳定。

Conclusion: 该算法为含约束优化问题提供了一种简单有效的一阶方法，特别适用于梯度存在噪声的场景，收敛性能与无约束问题的最佳方法相当。

Abstract: A very simple first-order algorithm is proposed for solving nonlinear
optimization problems with deterministic nonlinear equality constraints. This
algorithm adaptively selects steps in the plane tangent to the constraints or
steps that reduce infeasibility, without using a merit function or filter. The
tangent steps are based on the AdaGrad method for unconstrained minimization.
The objective function is never evaluated by the algorithm, making it suitable
for noisy problems. Its worst-case evaluation complexity is analyzed, yielding
a global convergence rate in O(1/sqrt{k}), which matches the best known rate of
first-order methods for unconstrained problems. Numerical experiments are
presented suggesting that the performance of the algorithm is comparable to
that of first-order methods for unconstrained problems, and that its
reliability is remarkably stable in the presence of noise on the gradient.

</details>


### [5] [Charnes--Cooper transformation and fractional optimization with SOS-convex polynomials](https://arxiv.org/abs/2510.16400)
*Chengmiao Yang,Liguo Jiao,Jae Hyoung Lee*

Main category: math.OC

TL;DR: 提出基于Charnes-Cooper变换的参数自由方案，用于求解一类具有SOS凸多项式的分式规划问题，建立了解存在性、强对偶性和解提取定理。


<details>
  <summary>Details</summary>
Motivation: 针对具有SOS凸多项式的分式规划问题，开发参数自由的计算方案，避免传统方法中的参数调整问题。

Method: 采用Charnes-Cooper变换，将原分式规划问题转化为等价的凸优化问题，建立解存在性和强对偶性理论。

Result: 在特定条件下证明了解决方案的存在性、强对偶性，并提供了解决方案提取方法，通过示例验证了理论结果。

Conclusion: 所提出的参数自由方案能有效求解SOS凸多项式分式规划问题，具有理论保证和实际可行性。

Abstract: This paper proposes a parameter-free scheme that is based on the
Charnes--Cooper transformation for solving a class of fractional programs with
SOS-convex polynomials. Under certain conditions, we establish theorems of
solution existence,strong duality and solution extraction. An illustrative
example is designed to show the obtained results.

</details>


### [6] [Frank-Wolfe Algorithms for (L0, L1)-smooth functions](https://arxiv.org/abs/2510.16468)
*A. A. Vyguzov,F. S. Stonyakin*

Main category: math.OC

TL;DR: 提出了(L0,L1)-Frank-Wolfe算法及其自适应版本，用于优化具有(L0,L1)-光滑目标函数的问题，相比经典Frank-Wolfe方法具有更优的理论收敛速率和实际性能。


<details>
  <summary>Details</summary>
Motivation: 针对具有(L0,L1)-光滑目标函数的优化问题，经典Frank-Wolfe方法的收敛性能有待提升，需要开发更高效的算法变体。

Method: 设计了(L0,L1)-Frank-Wolfe算法，并进一步提出了自适应版本，能够动态调整光滑参数以提高性能和稳定性。

Result: 理论分析表明新算法具有优于经典Frank-Wolfe方法的收敛速率，数值实验验证了理论结果并展示了实际优势。

Conclusion: 提出的(L0,L1)-Frank-Wolfe算法及其自适应变体在理论和实践上都优于现有Frank-Wolfe方法变种，为相关优化问题提供了更有效的解决方案。

Abstract: We propose a new version of the Frank-Wolfe method, called the (L0,
L1)-Frank-Wolfe algorithm, developed for optimization problems with (L0,
L1)-smooth objectives. We establish that this algorithm achieves superior
theoretical convergence rates compared to the classical Frank-Wolfe method. In
addition, we introduce a novel adaptive procedure, termed the Adaptive (L0,
L1)-Frank-Wolfe algorithm, which dynamically adjusts the smoothness parameters
to further improve performance and stability. Comprehensive numerical
experiments confirm the theoretical results and demonstrate the clear practical
advantages of both proposed algorithms over existing Frank-Wolfe variants.

</details>


### [7] [On the convergence rate of the boosted Difference-of-Convex Algorithm (DCA)](https://arxiv.org/abs/2510.16569)
*Hadi Abbaszadehpeivasti,Etienne de Klerk,Adrien Taylor*

Main category: math.OC

TL;DR: 本文研究了无约束DC算法（DCA）及其增强版本的最坏情况性能，证明了在某些DC分解类别中，增强DCA在理论上优于经典DCA。


<details>
  <summary>Details</summary>
Motivation: 虽然多个数值研究表明增强DCA优于经典DCA，但此前缺乏理论解释，本文旨在填补这一理论空白。

Method: 使用半定规划（SDP）性能估计技术来分析DCA及其增强版本的最坏情况性能。

Result: 对于某些DC分解类别，增强DCA在理论上被证明比经典DCA具有更好的最坏情况性能。

Conclusion: 本文首次为增强DCA优于经典DCA的现象提供了理论解释，证明了在某些条件下增强DCA确实具有更好的最坏情况性能保证。

Abstract: The difference-of-convex algorithm (DCA) is a well-established nonlinear
programming technique that solves successive convex optimization problems.
These sub-problems are obtained from the difference-of-convex~(DC)
decompositions of the objective and constraint functions. We investigate the
worst-case performance of the unconstrained DCA, with and without boosting,
where boosting simply performs an additional step in the direction generated by
the usual DCA method. We show that, for certain classes of DC decompositions,
the boosted DCA is provably better in the worst-case than the usual DCA. While
several numerical studies have reported that boosted DCA outperforms classical
DCA, a theoretical explanation for this behavior has, to the best of our
knowledge, not been given until now. Our proof technique relies on semidefinite
programming (SDP) performance estimation.

</details>


### [8] [Convexification of a Separable Function over a Polyhedral Ground Set](https://arxiv.org/abs/2510.16595)
*Santanu S. Dey,Burak Kocuk*

Main category: math.OC

TL;DR: 本文研究集合 $\mathcal{S}^\kappa = \{(x,y)\in\mathcal{G}\times\mathbb{R}^n : y_j = x_j^\kappa , j=1,\dots,n\}$ 的凸包或紧外逼近，其中 $\kappa > 1$，基集 $\mathcal{G}$ 是包含在 $[0,1]^n$ 中的非空多面体。


<details>
  <summary>Details</summary>
Motivation: 该非凸集与可分离标准二次规划密切相关，并出现在天然气和水网络等基于势能的网络流问题中。研究其凸包有助于解决这些实际问题。

Method: 提出了幂锥、二阶锥和半定规划松弛方法，并通过Reformulation-Linearization技术和Reformulation-Perspectification技术进一步强化这些松弛。

Result: 对于 $\kappa=2$，在低维情况下获得了 $\mathcal{S}^\kappa$ 的凸包；对于一般 $\kappa$，给出了幂锥可表示松弛的近似保证，并证明当 $n\to\infty$ 时，该最弱松弛在均匀生成的线性目标下几乎必然紧致。

Conclusion: 通过广泛的数值实验比较了多种锥规划松弛的经验强度，为相关优化问题提供了有效的求解方法。

Abstract: In this paper, we study the set $\mathcal{S}^\kappa = \{
(x,y)\in\mathcal{G}\times\mathbb{R}^n : y_j = x_j^\kappa , j=1,\dots,n\}$,
where $\kappa > 1$ and the ground set $\mathcal{G}$ is a nonempty polytope
contained in $[0,1]^n$. This nonconvex set is closely related to separable
standard quadratic programming and appears as a substructure in potential-based
network flow problems from gas and water networks. Our aim is to obtain the
convex hull of $\mathcal{S}^\kappa$ or its tight outer-approximation for the
special case when the ground set $\mathcal{G}$ is the standard simplex. We
propose power cone, second-order cone and semidefinite programming relaxations
for this purpose, which are further strengthened by the
Reformulation-Linearization Technique and the Reformulation-Perspectification
Technique. For $\kappa=2$, we obtain the convex hull of $\mathcal{S}^\kappa$ in
the low-dimensional setting. For general $\kappa$, we give approximation
guarantees for the power cone representable relaxation, the weakest relaxation
we consider. We prove that this weakest relaxation is tight with probability
one as $n\to\infty$ when a uniformly generated linear objective is optimized
over it. Finally, we provide the results of our extensive computational
experiments comparing the empirical strength of several conic programming
relaxations that we propose.

</details>


### [9] [A class of singular control problems with tipping points](https://arxiv.org/abs/2510.16599)
*Jean-Paul Décamps,Fabien Gensbittel,Thomas Mariotti,Stéphane Villeneuve*

Main category: math.OC

TL;DR: 本文研究了一个奇异随机控制问题，其中性能标准依赖于非停止时间的随机水平命中时间，建立了与涉及扩散及其运行最小值的奇异控制问题的联系，并应用于资源提取问题。


<details>
  <summary>Details</summary>
Motivation: 研究系统在临界点处经历突然且不可逆变化的情况，特别是当性能标准依赖于非停止时间的随机水平命中时间时的控制问题。

Method: 建立了奇异随机控制问题与涉及扩散及其运行最小值的奇异控制问题之间的联系，证明了验证定理。

Result: 成功建立了两种控制问题之间的等价关系，并将结果应用于资源提取问题的显式求解。

Conclusion: 提出的方法能够有效处理临界点相关的随机控制问题，为资源管理等实际应用提供了理论支持。

Abstract: Tipping points define situations where a system experiences sudden and
irreversible changes and are generally associated with a random level of the
system below which the changes materialize. In this paper, we study a singular
stochastic control problem in which the performance criterion depends on the
hitting time of a random level that is not a stopping time for the reference
filtration. We establish a connection between the value of the problem and the
value of a singular control problem involving a diffusion and its running
minimum. We prove a verification theorem and apply our results to explicitly
solve a resource extraction problem where the random evolution of the resource
changes when it crosses a tipping point.

</details>


### [10] [Adversarial Reinforcement Learning for Robust Control of Fixed-Wing Aircraft under Model Uncertainty](https://arxiv.org/abs/2510.16650)
*Dennis J. Marquis,Blake Wilhelm,Devaprakash Muniraj,Mazen Farhood*

Main category: math.OC

TL;DR: 提出基于强化学习的固定翼小型无人机路径跟踪控制器，使用鲁棒对抗强化学习框架训练，能抵抗气动模型不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决固定翼小型无人机在气动模型存在不确定性时的鲁棒路径跟踪控制问题。

Method: 采用鲁棒对抗强化学习框架，通过对手扰动气动模型系数来训练控制器，提高对不确定性的鲁棒性。

Result: 对抗训练比随机模型不确定性训练更鲁棒，在高保真六自由度仿真中表现出准确和鲁棒的路径跟踪性能。

Conclusion: 鲁棒对抗强化学习能有效提高无人机控制器对气动模型不确定性的鲁棒性。

Abstract: This paper presents a reinforcement learning-based path-following controller
for a fixed-wing small uncrewed aircraft system (sUAS) that is robust to
uncertainties in the aerodynamic model of the sUAS. The controller is trained
using the Robust Adversarial Reinforcement Learning framework, where an
adversary perturbs the environment (aerodynamic model) to expose the agent
(sUAS) to demanding scenarios. In our formulation, the adversary introduces
rate-bounded perturbations to the aerodynamic model coefficients. We
demonstrate that adversarial training improves robustness compared to
controllers trained using stochastic model uncertainty. The learned controller
is also benchmarked against a switched uncertain initial condition controller.
The effectiveness of the approach is validated through high-fidelity
simulations using a realistic six-degree-of-freedom fixed-wing aircraft model,
showing accurate and robust path-following performance under a variety of
uncertain aerodynamic conditions.

</details>


### [11] [Bregman Stochastic Proximal Point Algorithm with Variance Reduction](https://arxiv.org/abs/2510.16655)
*Cheik Traoré,Peter Ochs*

Main category: math.OC

TL;DR: 本文提出了Bregman随机近点算法(BSPPA)的方差缩减技术，结合了SAGA和SVRG方法，提高了收敛速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 随机近点算法(SPPA)虽然比SGD对步长设置更鲁棒，但仍因需要递减步长而导致收敛速度下降。同时，在确定性设置中，使用Bregman距离的非欧几何能更高效解决问题。

Method: 结合Bregman随机近点算法(BSPPA)与方差缩减技术，提出了类似SAGA和SVRG的方差缩减方法。

Result: 理论和数值结果表明，相比使用常数和递减步长的原始BSPPA，该方法具有更好的稳定性和收敛速度。

Conclusion: 该方法成功提升了BSPPA的性能，并能以统一方式恢复Bregman SGD的方差缩减技术。

Abstract: Stochastic algorithms, especially stochastic gradient descent (SGD), have
proven to be the go-to methods in data science and machine learning. In recent
years, the stochastic proximal point algorithm (SPPA) emerged, and it was shown
to be more robust than SGD with respect to stepsize settings. However, SPPA
still suffers from a decreased convergence rate due to the need for vanishing
stepsizes, which is resolved by using variance reduction methods. In the
deterministic setting, there are many problems that can be solved more
efficiently when viewing them in a non-Euclidean geometry using Bregman
distances. This paper combines these two worlds and proposes variance reduction
techniques for the Bregman stochastic proximal point algorithm (BSPPA). As
special cases, we obtain SAGA- and SVRG-like variance reduction techniques for
BSPPA. Our theoretical and numerical results demonstrate improved stability and
convergence rates compared to the vanilla BSPPA with constant and vanishing
stepsizes, respectively. Our analysis, also, allow to recover the same variance
reduction techniques for Bregman SGD in a unified way.

</details>


### [12] [HNAG++: A Super-Fast Accelerated Gradient Method for Strongly Convex Optimization](https://arxiv.org/abs/2510.16680)
*Long Chen,Zeyi Xu*

Main category: math.OC

TL;DR: 提出了HNAG+和HNAG++两种方法用于最小化强凸函数，HNAG+达到信息论最优收敛率，HNAG++在Hölder连续Hessian条件下具有最快的全局收敛率。


<details>
  <summary>Details</summary>
Motivation: 针对具有大条件数κ的强凸函数优化问题，现有方法收敛速度不够快，需要开发更高效的优化算法。

Method: 设计了两种加速梯度方法：HNAG+和HNAG++，分别针对不同函数类进行优化。

Result: HNAG+达到1-2/√κ的全局线性收敛率，HNAG++在Hölder连续Hessian条件下达到1-2√(2/κ)的渐近线性收敛率，数值实验显示HNAG++优于现有加速梯度方法。

Conclusion: HNAG++是目前全局收敛一阶方法中最快的算法，在理论和实验上都表现出优越性能。

Abstract: We introduce and analyze two methods, HNAG+ and HNAG++, for minimizing
strongly convex functions with large condition number kappa. For HNAG+, we
prove a global linear convergence rate of 1 - 2/sqrt(kappa), achieving the
information-theoretic optimal rate. For HNAG++, we establish a global
asymptotic linear rate of 1 - 2*sqrt(2/kappa) for functions with H\"older
continuous Hessians, representing the fastest known rate among globally
convergent first-order methods. Extensive numerical experiments on linear and
nonlinear problems show that HNAG++ consistently outperforms existing
accelerated gradient methods.

</details>


### [13] [Geometric Control Theory Over Networks: Minimal Node Cardinality Disturbance Decoupling Problems](https://arxiv.org/abs/2510.16689)
*Luca Claude Gino Lebon,Claudio Altafini*

Main category: math.OC

TL;DR: 本文提出了在网络中选择最少数量的输入和输出节点来解决干扰解耦问题的方法，使用状态、输出和动态反馈来隔离和消除干扰节点对特定目标节点的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过网络控制来隔离和消除干扰对特定目标节点的影响，同时最小化所需的输入和输出节点数量，以提高网络控制的效率和可行性。

Method: 利用节点集合而非子空间的重新表述，使受控和条件不变性具有简单的图形解释。对于状态和动态反馈，通过最小割/最大流算法在多项式时间内精确计算最小输入和输出节点数量的解决方案。

Result: 开发了使用状态、输出和动态反馈的反馈律，能够隔离和消除干扰节点对特定目标节点的影响。对于状态和动态反馈，可以精确计算最小输入和输出节点数量的解决方案。

Conclusion: 通过图形化方法和最小割/最大流算法，可以在多项式时间内有效解决网络中的干扰解耦问题，并找到最少数量的输入和输出节点配置。

Abstract: In this paper we show how to formulate and solve disturbance decoupling
problems over networks while choosing a minimal number of input and output
nodes. Feedback laws that isolate and eliminate the impact of disturbance nodes
on specific target nodes to be protected are provided using state, output, and
dynamical feedback. For that, we leverage the fact that when reformulated in
terms of sets of nodes rather than subspaces, the controlled and conditional
invariance properties admit a simple graphical interpretation. For state and
dynamical feedback, the minimal input and output cardinality solutions can be
computed exactly in polynomial time, via min-cut/max-flow algorithms.

</details>


### [14] [Local integral input-to-state stability for non-autonomous infinite-dimensional systems](https://arxiv.org/abs/2510.16725)
*Yongchun Bi,Panyu Deng,Jun Zheng,Guchuan Zhu*

Main category: math.OC

TL;DR: 本文建立了非线性微分方程的比较原理，并开发了李雅普诺夫分析工具，用于分析具有超线性增长的非线性非自治无穷维系统的积分输入状态稳定性。


<details>
  <summary>Details</summary>
Motivation: 处理具有时变系数和超线性增长的非线性非自治无穷维系统的积分输入状态稳定性分析困难，需要建立新的比较原理和分析框架。

Method: 首先建立具有时变系数和超线性项的常微分方程的比较原理，然后在Banach空间框架下证明局部iISS李雅普诺夫定理，在Hilbert空间框架下构造LiISS-LF。

Result: 成功建立了比较原理，证明了局部iISS李雅普诺夫定理，构造了LiISS-LF，并通过数值实验验证了方法的有效性。

Conclusion: 提出的李雅普诺夫方法能够有效分析具有超线性增长的非线性非自治无穷维系统的积分输入状态稳定性，为这类系统的稳定性分析提供了新的工具。

Abstract: In this paper, we prove comparison principles for nonlinear differential
equations with time-varying coefficients and develop Lyapunov analytical tools
for the integral input-to-state stability (iISS) analysis of nonlinear
non-autonomous infinite-dimensional systems, which involve nonlinearities
satisfying a superlinear growth, {bringing} difficulties to the iISS
{analysis.} Specifically, our approach starts by establishing several forms of
comparison principles for a wide range of ordinary differential equations
having time-varying coefficients and superlinear terms, paving the way to
conduct iISS assessment for general nonlinear non-autonomous
infinite-dimensional systems within the Lyapunov stability framework. Then, by
using the comparison principles, we prove a local {iISS} {(LiISS)} Lyapunov
theorem for the nonlinear non-autonomous infinite-dimensional systems in the
framework of Banach spaces. {Furthermore,} we provide sufficient conditions of
the existence of a local iISS Lyapunonv functional (LiISS-LF) and construct
LiISS-LFs for the systems in the framework of Hilbert spaces. Finally, we
preset two examples to illustrate the proposed {Lyapunov} method for the LiISS
analysis: one is to show how to obtain the LiISS of a nonlinear
finite-dimensional system with time-varying coefficients and superlinear terms
under linear state feedback control law while another one is to show how to
employ the interpolation inequalities to handle superliner terms and establish
the LiISS-LF for a class of multi-dimensional parabolic equations with
space-time-varying coefficients. To demonstrate the validity of the results,
numerical experiments are also conducted to verify the LiISS of these two
classes of systems.

</details>


### [15] [Equivalence of additive and parametric pinning control protocols for systems of weakly coupled oscillators](https://arxiv.org/abs/2510.16766)
*Riccardo Muolo,Yuzuru Kato*

Main category: math.OC

TL;DR: 本文证明了在弱耦合周期性振荡系统中，两种牵制控制方法（外部输入控制与参数变化控制）的等价性。


<details>
  <summary>Details</summary>
Motivation: 控制网络非线性系统的行为是控制理论的重要任务，特别是同步控制因其广泛应用而备受关注。本文旨在研究两种不同的牵制控制方法。

Method: 使用相位约简技术分析弱耦合周期性振荡系统，并通过耦合Stuart-Landau振荡器的数值模拟进行验证。

Result: 数值模拟验证了在弱耦合周期性振荡系统中，外部输入控制和参数变化控制两种牵制控制方法的等价性。

Conclusion: 研究结果为牵制控制在现实世界系统中的进一步应用奠定了基础。

Abstract: Controlling the behavior of nonlinear systems on networks is a paramount task
in control theory, in particular the control of synchronization, given its vast
applicability. In this work, we focus on pinning control and we examine two
different approaches: the first, more common in engineering applications, where
the control is implemented through an external input (additive pinning); the
other, where the parameters of the pinned nodes are varied (parametric
pinning). By means of the phase reduction technique, we show that the two
pinning approaches are equivalent for weakly coupled systems exhibiting
periodic oscillatory behaviors. Through numerical simulations, we validate the
claim for a system of coupled Stuart--Landau oscillators. Our results pave the
way for further applications of pinning control in real-world systems.

</details>


### [16] [Method of Monotone Structural Evolution for control and state constrained optimal and control problems](https://arxiv.org/abs/2510.16768)
*Maciej Szymkat,Adam Korytowski*

Main category: math.OC

TL;DR: 提出了一种用于处理控制和状态约束的最优控制计算方法，通过节点和弧的生成与缩减序列来调整控制结构，重新定义决策空间而不改变当前控制。


<details>
  <summary>Details</summary>
Motivation: 解决带有控制和状态约束的最优控制问题，传统方法在处理复杂约束时存在困难，需要更有效的计算策略。

Method: 使用控制结构调整序列，包括节点和弧的生成与缩减操作，这些操作重新定义决策空间但保持当前控制不变。

Result: 提供了多个示例验证方法的有效性，展示了该方法在约束最优控制问题中的应用。

Conclusion: 该方法为处理控制和状态约束的最优控制问题提供了一种有效的计算框架，通过结构调整策略改进了约束处理能力。

Abstract: A method of optimal control computation is proposed for problems with control
and state constraints. It uses a sequence of control structure adjustments in
the form of generations and reductions of nodes and arcs, which do not change
the current control but redefine the decision space. Several examples are
given.

</details>


### [17] [A Surrogate Value Function Formulation for Bilevel Optimization](https://arxiv.org/abs/2510.16818)
*Mengwei Xu,Yu-Hong Dai,Xin-Wei Liu,Meiqi Ma*

Main category: math.OC

TL;DR: 提出了一种替代值函数公式，用基于下层问题平稳性条件的显式替代函数替换难以处理的值函数，保持了经典值函数模型的本质思想但避免了KKT公式的问题。


<details>
  <summary>Details</summary>
Motivation: 传统值函数公式虽然捕捉了双层优化的层次结构，但其隐式和非光滑特性带来了分析和计算困难。KKT公式将下层平稳点嵌入上层可行域，模糊了层次依赖关系。

Method: 引入替代值函数公式，通过支配约束强制层次结构，即使在下层约束条件失效时仍有效。应用平滑障碍增广拉格朗日方法处理互补约束。

Result: 证明了与原始双层问题的等价性，揭示了标准约束条件的失效，并证明其强平稳性蕴含KKT模型的强平稳性。实验显示该公式具有鲁棒性和高数值精度。

Conclusion: 该替代值函数公式在非凸设置下表现优异，特别是在KKT模型失效的经典Mirrlees问题中，为双层优化提供了新的有效方法。

Abstract: The value function formulation captures the hierarchical nature of bilevel
optimization through the optimal value function of the lower level problem, yet
its implicit and nonsmooth characteristics pose significant analytical and
computational difficulties. We introduce a surrogate value function formulation
that replaces the intractable value function with an explicit surrogate derived
from lower level stationarity conditions. This surrogate formulation preserves
the essential idea of the classical value function model but fundamentally
departs from Karush Kuhn Tucker (KKT) formulations, which embed lower level
stationary points into the upper level feasible region and obscure the
hierarchical dependence. Instead, it enforces the hierarchy through a dominance
constraint that remains valid even when lower level constraint qualifications
fail at the solution. We establish equivalence with the original bilevel
problem, reveal the failure of standard constraint qualifications, and show
that its strong stationarity implies that of KKT models. To handle the
complementarity constraints in the surrogate formulation, we apply a smoothing
barrier augmented Lagrangian method and prove its convergence to solutions and
Clarke stationary points. Extensive experiments demonstrate the robustness and
high numerical precision of this formulation, especially in nonconvex settings,
including the classical Mirrlees problem where KKT models fail.

</details>


### [18] [The Augmented Lagrangian Methods: Overview and Recent Advances](https://arxiv.org/abs/2510.16827)
*Kangkang Deng,Rui Wang,Zhenyuan Zhu,Junyu Zhang,Zaiwen Wen*

Main category: math.OC

TL;DR: 本文提供了增广拉格朗日函数在各类优化问题中的统一构建视角，涵盖非线性规划、凸和非凸复合规划，系统阐述了增广拉格朗日方法的理论基础、最新进展和应用实例。


<details>
  <summary>Details</summary>
Motivation: 大规模约束优化在现代科学、工程和工业计算中至关重要，但涉及复杂系统和众多变量约束。需要统一框架来处理各类优化问题，包括非凸约束和全局收敛性保证。

Method: 基于Hestenes-Powell-Rockafellar增广拉格朗日函数构建统一框架，使用增广拉格朗日方法处理凸和非凸问题，对非光滑凸问题采用邻近操作，并开发不同变体以提升收敛性和计算性能。

Result: 增广拉格朗日方法能够处理非凸约束并确保全局收敛到一阶和二阶稳定点，对非光滑凸问题保持局部线性收敛率，在复杂整数规划实例中取得进展，并改进了复杂度分析。

Conclusion: 增广拉格朗日方法为大规模约束优化提供了强大而灵活的工具，通过不同变体和有效子问题算法，在实际应用中展现出良好性能，但仍需继续探索其局限性和改进空间。

Abstract: Large-scale constrained optimization is pivotal in modern scientific,
engineering, and industrial computation, often involving complex systems with
numerous variables and constraints. This paper provides a unified and
comprehensive perspective on constructing augmented Lagrangian functions (based
on Hestenes-Powell-Rockafellar augmented Lagrangian) for various optimization
problems, including nonlinear programming and convex and nonconvex composite
programming. We present the augmented Lagrangian method (ALM), covering its
theoretical foundations in both convex and nonconvex cases, and discuss several
successful examples and applications. Recent advancements have extended ALM's
capabilities to handle nonconvex constraints and ensure global convergence to
first and second-order stationary points. For nonsmooth convex problems, ALM
utilizes proximal operations, preserving desirable properties such as locally
linear convergence rates. Furthermore, recent progress has refined the
complexity analysis for ALM and tackled challenging integer programming
instances. This review aims to offer a thorough understanding of ALM's benefits
and limitations, exploring different ALM variants designed to enhance
convergence and computational performance. We also illustrate effective
algorithms for ALM subproblems across different types of optimization problems
and highlight practical implementations in several fields.

</details>


### [19] [Solving nonconvex optimization problems via a second order dynamical system with unbounded damping](https://arxiv.org/abs/2510.16864)
*Szilárd Csaba László*

Main category: math.OC

TL;DR: 研究带可变系数的二阶动力系统用于最小化光滑非凸函数，证明了在Kurdyka-Łojasiewicz性质下轨迹收敛到临界点，并获得超线性收敛速率。


<details>
  <summary>Details</summary>
Motivation: 改进非凸函数优化的收敛速率，通过引入无界阻尼来显著提升传统线性收敛速率至超线性收敛。

Method: 使用带可变系数的二阶动力系统，结合Kurdyka-Łojasiewicz性质和Łojasiewicz指数分析收敛性。

Result: 证明了轨迹收敛到临界点，并获得了超线性收敛速率，显著优于文献中的线性收敛结果。

Conclusion: 所提出的二阶动力系统在无界阻尼下能够实现超线性收敛，为非凸优化提供了更快的收敛保证。

Abstract: In this paper we study a second order dynamical system with variable
coefficients in connection to the minimization problem of a smooth nonconvex
function. The convergence of the trajectories generated by the dynamical system
to a critical point of the objective function is assured, provided a
regularization of the objective function satisfies the Kurdyka-{\L}ojasiewicz
property. We also provide convergence rates for the trajectories generated by
the dynamical system, formulated in terms of the {\L}ojasiewicz exponent, and
we show that the unbounded damping considered in our dynamical system
significantly improves the convergence rates known so far in the literature,
that is, instead of linear rates we obtain superlinear rates.

</details>


### [20] [Distributionally Robust Nash Equilibria via Variational Inequalities](https://arxiv.org/abs/2510.17024)
*Zeinab Alizadeh,Azadeh Farsi,Afrooz Jalilzadeh*

Main category: math.OC

TL;DR: 该论文研究分布鲁棒纳什均衡问题，通过变分不等式重构问题框架，提出具有收敛保证的梯度下降-上升算法来解决高维非光滑目标的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 纳什均衡及其鲁棒版本在博弈论中具有重要应用，但现有方法在处理不确定性和高维非光滑目标时面临计算挑战。

Method: 将分布鲁棒纳什均衡问题重构为变分不等式问题，提出梯度下降-上升类型算法。

Result: 建立了统一的分析框架，并开发了具有收敛保证的有效算法。

Conclusion: 提出的方法能够有效解决高维非光滑目标下的分布鲁棒纳什均衡计算问题。

Abstract: Nash Equilibrium and its robust counterpart, Distributionally Robust Nash
Equilibrium (DRNE), are fundamental problems in game theory with applications
in economics, engineering, and machine learning. This paper addresses the
problem of DRNE, where multiple players engage in a noncooperative game under
uncertainty. Each player aims to minimize their objective against the
worst-case distribution within an ambiguity set, resulting in a minimax
structure. We reformulate the DRNE problem as a Variational Inequality (VI)
problem, providing a unified framework for analysis and algorithm development.
We propose a gradient descent-ascent type algorithm with convergence guarantee
that effectively addresses the computational challenges of high-dimensional and
nonsmooth objectives.

</details>


### [21] [Optimal Trajectories for Optimal Transport in Nonuniform Environments](https://arxiv.org/abs/2510.17170)
*Luca Dieci,Daniyar Omarov*

Main category: math.OC

TL;DR: 本文解决了非均匀环境中的离散最优传输问题，通过求解Euler-Lagrange方程构建成本矩阵，提出了新的算法并验证了性能。


<details>
  <summary>Details</summary>
Motivation: 在非均匀环境中解决离散最优传输问题，主要挑战在于构建成本矩阵，这需要找到两点之间的最优路径。

Method: 制定并求解相关的Euler-Lagrange方程来找到最优路径，提出了新的算法来解决该问题。

Result: 提供了Euler-Lagrange方程解的最优性可验证充分条件，并通过多个数值示例展示了算法结果和性能。

Conclusion: 成功解决了非均匀环境中的离散最优传输问题，提出的算法在数值实验中表现出良好性能。

Abstract: In this work, we solve a discrete optimal transport problem in a nonuniform
environment. The key challenge is to form the cost matrix, which requires
finding the optimal path between two points, and for this task we formulate and
solve the associated Euler-Lagrange equations. A main theoretical result of
ours is to provide verifiable sufficient conditions of optimality of the
solution of the Euler-Lagrange equation. We propose new algorithms to solve the
problem, and illustrate our results and performance of the algorithms on
several numerical examples.

</details>


### [22] [Periodic limit for non-autonomous Lagrangian systems and applications to a Kuramoto type model](https://arxiv.org/abs/2510.17242)
*Veronica Danesi,Cristian Mendico,Xuan Tao,Kaizhi Wang*

Main category: math.OC

TL;DR: 研究非自治拉格朗日系统的渐近性质，当Tonelli拉格朗日量收敛于时间周期函数时，构造Lax-Oleinik半群使其收敛于方程的周期解，并证明其梯度图在Hausdorff距离下收敛于极限周期函数的梯度图。


<details>
  <summary>Details</summary>
Motivation: 探索非自治拉格朗日系统在Tonelli拉格朗日量收敛于周期函数时的渐近行为，为理解这类系统的长期动力学特性提供理论框架。

Method: 构造合适的Lax-Oleinik半群，分析其收敛性质，证明梯度图的Hausdorff收敛性，并应用于Kuramoto型模型。

Result: 证明了Lax-Oleinik半群收敛于周期解，其梯度图在Hausdorff距离下收敛于极限周期函数的梯度图，并在Kuramoto模型中构造了由哈密顿-雅可比方程周期解梯度给出的不变环面。

Conclusion: 非自治拉格朗日系统在Tonelli拉格朗日量周期收敛条件下具有明确的渐近行为，为研究此类系统的动力学提供了有效工具。

Abstract: This paper explores the asymptotic properties of non-autonomous Lagrangian
systems, assuming that the associated Tonelli Lagrangian converges to a
time-periodic function. Specifically, given a continuous initial condition, we
provide a suitable construction of a Lax-Oleinik semigroup such that it
converges toward a periodic solution of the equation. Moreover, the graph of
its gradient converges as time tends to infinity to the graph of the gradient
of the periodic limit function with respect to the Hausdorff distance. Finally,
we apply this result to a Kuramoto-type model, proving the existence of an
invariant torus given by the graph of the gradient of the limiting periodic
solution of the Hamilton-Jacobi equation.

</details>


### [23] [A polynomial-based QCQP solver for encrypted optimization](https://arxiv.org/abs/2510.17294)
*Sebastian Schlor,Andrea Iannelli,Junsoo Kim,Hyungbo Shim,Frank Allgöwer*

Main category: math.OC

TL;DR: 提出一种使用加法和乘法解决二次约束二次优化问题的新方法，该方法与同态加密方案兼容，可在私有数据上求解约束优化问题。


<details>
  <summary>Details</summary>
Motivation: 开发能够在加密数据上执行约束优化的方法，因为传统优化方法需要除法等复杂运算，不适用于同态加密环境。

Method: 引入一系列递增次数的多项式罚函数，在可行集边界处足够陡峭，将其添加到原始成本函数中形成无约束优化问题序列，使用梯度下降法求解。

Result: 证明迭代收敛到原始问题的极小值点，可行集在迭代下正不变，并在加密数值比较问题上验证了方法的有效性。

Conclusion: 该方法为在加密数据上执行约束优化提供了可行的解决方案，特别适用于需要隐私保护的场景。

Abstract: In this paper, we present a novel method for solving a class of quadratically
constrained quadratic optimization problems using only additions and
multiplications. This approach enables solving constrained optimization
problems on private data since the operations involved are compatible with the
capabilities of homomorphic encryption schemes. To solve the constrained
optimization problem, a sequence of polynomial penalty functions of increasing
degree is introduced, which are sufficiently steep at the boundary of the
feasible set. Adding the penalty function to the original cost function creates
a sequence of unconstrained optimization problems whose minimizer always lies
in the admissible set and converges to the minimizer of the constrained
problem. A gradient descent method is used to generate a sequence of iterates
associated with these problems. For the algorithm, it is shown that the iterate
converges to a minimizer of the original problem, and the feasible set is
positively invariant under the iteration. Finally, the method is demonstrated
on an illustrative cryptographic problem, finding the smaller value of two
numbers, and the encrypted implementability is discussed.

</details>


### [24] [Assessing the Quality of a Set of Basis Functions for Inverse Optimal Control via Projection onto Global Minimizers](https://arxiv.org/abs/2510.17339)
*Filip Bečanović,Jared Miller,Vincent Bonnet,Kosta Jovanović,Samer Mohammed*

Main category: math.OC

TL;DR: 本文提出了一种新的逆优化方法，通过测量测试点与凸基函数生成的全局最优解集之间的距离来评估基函数的表达能力，解决了传统方法中基函数一致性假设不成立的问题。


<details>
  <summary>Details</summary>
Motivation: 传统逆优化方法假设真实成本函数是凸基函数的凸组合，且基函数与测试点一致，但这种假设在许多实际应用中并不成立。本文旨在解决基函数表达能力不足的问题。

Method: 引入全局最优解集的概念，在无约束和约束条件下探索其性质。在凸二次设置中，通过双层梯度下降和增强线性矩阵不等式分别实现最小距离的上界和下界计算。

Result: 提出了基于距离测量的基函数表达能力评估框架，能够识别无效的基函数集合。

Conclusion: 该方法为逆优化提供了更可靠的基函数选择标准，并可扩展到最大可表示基函数、非凸基函数和多项式优化技术等场景。

Abstract: Inverse optimization (Inverse optimal control) is the task of imputing a cost
function such that given test points (trajectories) are (nearly) optimal with
respect to the discovered cost. Prior methods in inverse optimization assume
that the true cost is a convex combination of a set of convex basis functions
and that this basis is consistent with the test points. However, the
consistency assumption is not always justified, as in many applications the
principles by which the data is generated are not well understood. This work
proposes using the distance between a test point and the set of global optima
generated by the convex combinations of the convex basis functions as a
measurement for the expressive quality of the basis with respect to the test
point. A large minimal distance invalidates the set of basis functions. The
concept of a set of global optima is introduced and its properties are explored
in unconstrained and constrained settings. Upper and lower bounds for the
minimum distance in the convex quadratic setting are implemented by bi-level
gradient descent and an enriched linear matrix inequality respectively.
Extensions to this framework include max-representable basis functions,
nonconvex basis functions (local minima), and applying polynomial optimization
techniques.

</details>


### [25] [A Finite-Difference Trust-Region Method for Convexly Constrained Smooth Optimization](https://arxiv.org/abs/2510.17366)
*Dânâ Davar,Geovani Nunes Grapiglia*

Main category: math.OC

TL;DR: 提出一种基于有限差分梯度近似的无导数信赖域方法，用于求解具有凸约束的光滑优化问题。该方法无需计算近似平稳性度量，并在非凸、凸和Polyak-Lojasiewicz函数上建立了不同的复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 针对光滑优化问题，特别是当梯度信息不可用时，开发一种高效的无导数优化方法。该方法旨在避免计算近似平稳性度量的复杂性，同时在不同函数类上提供理论保证。

Method: 基于有限差分梯度近似的无导数信赖域方法，处理具有凸约束的优化问题。该方法不依赖近似平稳性度量的计算。

Result: 对于非凸问题，达到(L/σ)ε-近似平稳点的最坏情况复杂度为O(n(L/σε)^{-2})次函数评估；对于凸问题，函数残差降至(L/σ)ε以下的复杂度为O(n(L/σε)^{-1})；对于无约束域上的Polyak-Lojasiewicz函数，复杂度进一步改善为O(n log((L/σε)^{-1}))。数值实验表明该方法在基准问题和模型拟合应用中优于现有无导数求解器。

Conclusion: 所提出的无导数信赖域方法在理论和实验上都表现出色，为不同函数类提供了严格的复杂度分析，并在实际应用中展现出相对于现有方法的优势。

Abstract: We propose a derivative-free trust-region method based on finite-difference
gradient approximations for smooth optimization problems with convex
constraints. The proposed method does not require computing an approximate
stationarity measure. For nonconvex problems, we establish a worst-case
complexity bound of
$\mathcal{O}\!\left(n\left(\tfrac{L}{\sigma}\epsilon\right)^{-2}\right)$
function evaluations for the method to reach an
$\left(\tfrac{L}{\sigma}\epsilon\right)$-approximate stationary point, where
$n$ is the number of variables, $L$ is the Lipschitz constant of the gradient,
and $\sigma$ is a user-defined estimate of $L$. If the objective function is
convex, the complexity to reduce the functional residual below
$(L/\sigma)\epsilon$ is shown to be of
$\mathcal{O}\!\left(n\left(\tfrac{L}{\sigma}\epsilon\right)^{-1}\right)$
function evaluations, while for Polyak-Lojasiewicz functions on unconstrained
domains, the bound further improves to
$\mathcal{O}\left(n\log\left(\left(\frac{L}{\sigma}\epsilon\right)^{-1}\right)\right)$.
Numerical experiments on benchmark problems and a model-fitting application
demonstrate the method's efficiency relative to state-of-the-art
derivative-free solvers for both unconstrained and bound-constrained problems.

</details>


### [26] [A condensing approach for linear-quadratic optimization with geometric constraints](https://arxiv.org/abs/2510.17465)
*Alberto De Marchi*

Main category: math.OC

TL;DR: 本文提出了一种结合增广拉格朗日框架和结构利用子问题重构的方法，用于处理包含逻辑条件和基数约束的凸二次优化问题，显著提升了计算性能。


<details>
  <summary>Details</summary>
Motivation: 信号处理、自动控制和决策制定中普遍存在凸二次成本和多面体约束的优化问题，需要扩展问题类别以编码逻辑条件和基数约束等复杂情况。

Method: 将增广拉格朗日框架与求解器无关的结构利用子问题重构相结合，通过提出的凝聚技术改进计算性能。

Result: 该方法在保持收敛保证的同时，显著提升了计算性能。

Conclusion: 所提出的方法能够有效处理包含非凸复杂约束的优化问题，并通过结构利用技术实现计算效率的大幅提升。

Abstract: Optimization problems with convex quadratic cost and polyhedral constraints
are ubiquitous in signal processing, automatic control and decision-making. We
consider here an enlarged problem class that allows to encode logical
conditions and cardinality constraints, among others. In particular, we cover
also situations where parts of the constraints are nonconvex and possibly
complicated, but it is practical to compute projections onto this nonconvex
set. Our approach combines the augmented Lagrangian framework with a
solver-agnostic structure-exploiting subproblem reformulation. While
convergence guarantees follow from the former, the proposed condensing
technique leads to significant improvements in computational performance.

</details>


### [27] [Towards Optimal Control and Algorithmic Structure of Decompression Schedules](https://arxiv.org/abs/2510.17551)
*Benjamin Marsh*

Main category: math.OC

TL;DR: 该论文将减压规划形式化为具有气体可行性窗口、仿射上限和凸惩罚的最优控制问题，证明了在惰性气体分数单调性假设下的存在性、单调无再下降结构和bang-bang上升特性，并给出了伪多项式动态规划和标签设置算法。


<details>
  <summary>Details</summary>
Motivation: 为混合气体减压规划提供首个正式的存在性和bang-bang结构证明，解决在混合气体可行性窗口下的最优控制问题。

Method: 将减压规划形式化为最优控制问题，使用动态规划和标签设置算法，建立停留时间KKT条件，推导在线价值函数的Lipschitz正则性。

Result: 证明了存在性、单调无再下降结构和bang-bang上升特性，给出了具有先验误差界的伪多项式算法，发现有效前沿是连续且通常非凸的。

Conclusion: 该研究为混合气体减压规划提供了首个正式的理论框架，证明了关键的结构特性，并开发了高效的算法解决方案。

Abstract: We formalise decompression planning as an optimal control problem with gas
feasibility windows (ppO$_2$, END), affine ceilings, and convex penalties in
normalised oversaturation. We prove existence, a monotone no re-descent
structure and bang-bang ascents under a mild monotonicity assumption on inert
fraction, and establish dwell time KKT conditions. We give pseudo-polynomial DP
and label-setting algorithms with a priori error bounds, derive Lipschitz
regularity of the online value function, and discuss multi-species extensions.
The efficient frontier is continuous and generally nonconvex. We provide the
first formal existence and bang-bang structure proof under mixed gas
feasibility windows.

</details>


### [28] [An Inexact General Descent Method with Applications in Differential Equation-Constrained Optimization](https://arxiv.org/abs/2510.17581)
*Humberto Gimenes Macedo,Luís Felipe Bueno*

Main category: math.OC

TL;DR: 提出了一个不精确通用下降框架，用于处理梯度评估不精确的优化问题，特别是在微分方程约束优化中。该框架支持自适应梯度精度，在早期迭代使用粗略梯度，在接近极小值时使用更精确梯度。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，梯度评估本质上是近似的，特别是在微分方程约束优化中，离散伴随梯度依赖于迭代求解器。需要开发在非精确一阶信息下仍可靠的优化方法。

Method: 提出了不精确通用下降框架，建立了两种步长机制下的全局收敛理论：有界步长要求梯度误差与其范数成比例，递减步长要求容差序列可求和。实现了不精确梯度下降和不精确BFGS类方法。

Result: 在二阶ODE反问题和二维拉普拉斯反问题上使用自适应精度离散伴随梯度进行测试。自适应不精确梯度相比固定紧容差持续减少优化时间，结合曲率信息进一步提高了整体效率。

Conclusion: 自适应不精确梯度方法能有效减少优化时间，特别是在微分方程约束优化中，结合曲率信息的方法具有更好的整体效率。

Abstract: In many applications, gradient evaluations are inherently approximate,
motivating the development of optimization methods that remain reliable under
inexact first-order information. A common strategy in this context is adaptive
evaluation, whereby coarse gradients are used in early iterations and refined
near a minimizer. This is particularly relevant in differential
equation-constrained optimization (DECO), where discrete adjoint gradients
depend on iterative solvers. Motivated by DECO applications, we propose an
inexact general descent framework and establish its global convergence theory
under two step-size regimes. For bounded step sizes, the analysis assumes that
the error tolerance in the computed gradient is proportional to its norm,
whereas for diminishing step sizes, the tolerance sequence is required to be
summable. The framework is implemented through inexact gradient descent and an
inexact BFGS-like method, whose performance is demonstrated on a second-order
ODE inverse problem and a two-dimensional Laplace inverse problem using
discrete adjoint gradients with adaptive accuracy. Across these examples,
adaptive inexact gradients consistently reduced optimization time relative to
fixed tight tolerances, while incorporating curvature information further
improved overall efficiency.

</details>


### [29] [A brief note on approximate optimization of submodular functions](https://arxiv.org/abs/2510.17610)
*Alen Alexanderian*

Main category: math.OC

TL;DR: 讨论贪婪方法及其更高效变体用于近似最大化单调子模函数


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地近似最大化单调子模函数，因为精确求解通常是NP难的

Method: 使用贪婪方法及其改进变体

Result: 提出了几种更高效的贪婪算法变体

Conclusion: 贪婪方法及其变体是近似最大化单调子模函数的有效方法

Abstract: We briefly discuss the greedy method and a couple of its more efficient
variants for approximately maximizing monotone submodular functions.

</details>


### [30] [Counterfactual Explanations for Integer Optimization Problems](https://arxiv.org/abs/2510.17624)
*Felix Engelhardt,Jannis Kurtz,Ş. İlker Birbil,Ted Ralphs*

Main category: math.OC

TL;DR: 该论文研究了整数优化问题的反事实解释，证明了构建反事实解释的复杂性，并针对几种特殊情况提出了解决方案算法。


<details>
  <summary>Details</summary>
Motivation: 反事实解释为解释决策提供了一种人类可理解的方式，但在整数优化问题中的研究还很有限，特别是在一般整数优化问题方面。

Method: 首先证明了构建反事实解释的复杂性，然后针对几种最易处理的特殊情况提出了解决方案算法：可变目标参数、单一可变约束、可变右端项以及所有输入参数可修改的情况。

Result: 使用经典背包问题实例进行评估，重点关注可变约束参数的情况。结果显示，所提出的方法能够在几小时内为最多40个项目的小实例找到最优反事实解释。

Conclusion: 该研究填补了整数优化问题反事实解释的空白，证明了问题的复杂性，并提出了针对特定情况的有效解决方案。

Abstract: Counterfactual explanations (CEs) offer a human-understandable way to explain
decisions by identifying specific changes to the input parameters of a base or
present model that would lead to a desired change in the outcome. For
optimization models, CEs have primarily been studied in limited contexts and
little research has been done on CEs for general integer optimization problems.
In this work, we address this gap. We first show that the general problem of
constructing a CE is $\Sigma_2^p$-complete even for binary integer programs
with just a single mutable constraint. Second, we propose solution algorithms
for several of the most tractable special cases: (i) mutable objective
parameters, (ii) a single mutable constraint, (iii) mutable right-hand-side,
and (iv) all input parameters can be modified. We evaluate our approach using
classical knapsack problem instances, focusing on cases with mutable constraint
parameters. Our results show that our methods are capable of finding optimal
CEs for small instances involving up to 40 items within a few hours.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索实现多模态安全对齐的框架，旨在解决大型视觉语言模型的安全对齐挑战。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对多模态越狱攻击存在脆弱性，视觉输入引入新的攻击面，推理链缺乏安全监督，模态融合时对齐效果下降。

Method: 通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索构建多样化的安全关键提示轨迹，引入基于提示的缩放确保实时风险检测和合规响应。

Result: 广泛实验表明VisuoAlign能主动暴露风险，支持全面数据集生成，并显著提升LVLMs对复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign框架有效解决了多模态安全对齐的关键挑战，为大型视觉语言模型提供了更强大的安全防护能力。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [32] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 提出了结构化认知循环（SCL）作为可执行的认识论框架，将哲学洞见转化为可计算结构，重新定义智能为通过意向性理解重建自身认知状态的能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型缺乏真正认知理解和认识论架构的问题，弥合概念哲学与可实施认知之间的鸿沟。

Method: 基于过程哲学、能动认知和扩展心智理论，将智能定义为执行过程而非属性，构建包含判断、记忆、控制、行动和调节的连续循环框架。

Result: 展示了功能分离的认知架构比单一提示系统产生更连贯和可解释的行为，支持智能作为认知状态重建能力而非表征准确性。

Conclusion: 真正的进步需要实现认知原则的结构化架构，而非更大的模型，该框架对心智哲学、认识论和AI领域均有重要影响。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [33] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出了一个科学政策议程，探索citiverses作为监管学习实验空间的潜力，通过专家咨询识别关键研究领域和实验主题，强调负责任的发展方法。


<details>
  <summary>Details</summary>
Motivation: citiverses具有通过沉浸式虚拟环境支持监管学习的潜力，为政策场景和技术实验提供新平台，需要系统探索其应用前景。

Method: 基于与高级专家小组（包括欧盟委员会政策制定者、国家政府科学顾问和数字监管领域领先研究者）的咨询，识别关键研究领域和实验主题。

Result: 确定了包括可扩展性、实时反馈、复杂性建模、跨境协作等关键研究领域，以及交通、城市规划、环境/气候危机等实验主题。

Conclusion: citiverses有潜力成为监管学习的重要实验空间，但需要负责任的发展方法，考虑伦理、经济、生态和社会维度，并整合到更广泛的实验生态系统。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [34] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: PISA是一个受皮亚杰认知发展理论启发的统一记忆系统，通过三模态适应机制和混合记忆访问架构，显著提升了AI代理的适应性和长期知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，且忽视了记忆的建构性和任务导向作用。

Method: 提出PISA系统，采用三模态适应机制（图式更新、图式演化和图式创建）来保持记忆的连贯组织，并设计结合符号推理和神经检索的混合记忆访问架构。

Result: 在LOCOMO基准和新提出的AggQA基准上的实验表明，PISA在适应性和长期知识保留方面达到了新的最先进水平。

Conclusion: PISA通过将记忆视为建构性和适应性过程，为AI代理提供了一个实用且心理启发的统一记忆系统，显著提升了性能。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [35] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究表明，即使为大型语言模型提供环境接口来跟踪状态空间，也无法避免其在解决复杂拼图问题时的性能崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 探讨大型推理模型在解决超越特定困惑度阈值的拼图问题时出现性能崩溃的原因，特别是验证是否需要模型自行跟踪状态空间这一要求影响了对其真实推理能力的评估。

Method: 为大型语言模型提供塔罗牌问题的环境接口，使其能够通过工具调用进行移动、提供书面理由、观察结果状态空间，并重新提示自己进行下一步移动。

Result: 环境接口的访问并不能延迟或消除性能崩溃。LLM参数化策略分析显示与最优策略和均匀随机策略的差异逐渐增大，表明模型在每个复杂度级别都表现出模式崩溃。

Conclusion: 性能取决于模型模式是否反映问题的正确解决方案，类似现象可能在大型推理模型中出现。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [36] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出认知负荷痕迹（CLTs）作为深度模型的中层可解释性框架，将模型内部资源分配量化为符号化的时间变化函数，包含内在、外在和关联负荷三个分量，通过实验证明其能预测错误发生、揭示认知策略并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 受人类认知负荷理论启发，旨在为深度模型提供更细粒度的可解释性分析框架，量化模型在推理过程中的内部资源分配模式。

Method: 将CLTs定义为三组分随机过程（IL_t, EL_t, GL_t），分别对应内在、外在和关联负荷，通过注意力熵、KV缓存未命中率、表示分散度和解码稳定性等可测量代理来实例化，并提出符号化公式和可视化方法。

Result: 在推理和规划基准测试中，CLTs能够预测错误发生、揭示认知策略，并通过负荷引导的干预措施在保持准确性的同时将推理效率提升15-30%。

Conclusion: CLTs提供了一个有效的框架来理解和优化深度模型的推理过程，证明了认知负荷理论在模型可解释性中的实用价值。

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [37] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新的自动形式化证明流水线，通过构建逻辑依赖图和使用基于引理的方法来保持证明的结构保真度，在184个本科级问题基准上取得了0.545的ProofScore，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前自动形式化方法虽然能生成可执行代码，但经常无法保持原始人工证明的语义含义和逻辑结构的问题。

Method: 首先构建有向无环图来映射证明步骤间的逻辑依赖关系，然后采用基于引理的方法将每个步骤系统性地形式化为中间引理，以保持原始论证的逻辑结构。

Result: 在184个本科级问题基准上，ProofFlow取得了0.545的ProofScore，显著优于全证明形式化(0.123)和步骤证明形式化(0.072)等基线方法。

Conclusion: ProofFlow为自动形式化证明设立了新的最先进水平，其流水线、基准和评分指标已开源，以促进该领域的进一步发展。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [38] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 提出了将MO|RE运动科学研究数据仓库转换为知识图谱的愿景，旨在标准化和机器可理解地建模和共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 为了评估和比较不同人群的身体和认知能力，需要测试与人类表现相关的各种因素。运动表现测试作为体育科学研究的核心部分，能够分析不同人口群体的身体健康状况并使其具有可比性。

Method: 开发基于基本形式本体论的本体，重点形式化表示计划规范、特定过程和相关测量之间的相互关系。

Result: 提出了将MO|RE数据仓库转换为知识图谱的方法和愿景，使运动表现数据能够标准化和机器可理解地跨研究共享。

Conclusion: 该研究旨在通过知识图谱技术改变运动表现数据的建模和共享方式，提高数据的标准化程度和机器可理解性。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [39] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文从随机有限集和Dempster-Shafer理论两个角度分析了随机置换集中的冲突问题，提出了一种基于非重叠的冲突度量方法，该方法具有自然顶部加权特性，能够有效度量RPS之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 随机置换集是处理包含顺序信息的不确定推理的新形式化方法，如何度量由置换质量函数表示的两个证据之间的冲突是顺序结构不确定信息融合中的紧迫研究课题。

Method: 从置换观察出发，首先基于秩偏重叠(RBO)度量定义了置换间的不一致性度量，进一步提出了RPS的非重叠冲突度量方法，将RPS理论视为DST的扩展。

Result: 通过数值示例展示了所提出冲突度量的行为和特性，该方法不仅具有自然顶部加权特性，还能从DST视角有效度量RPS之间的冲突。

Conclusion: 所提出的方法为决策者提供了权重、参数和截断深度的灵活选择，能够有效处理顺序结构不确定信息融合中的冲突度量问题。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [40] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: PAINT是一种并行时间神经孪生方法，通过生成神经网络建模状态分布，在测试时使用滑动窗口从测量值预测状态，确保系统保持轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 神经孪生作为神经代理的演进，旨在创建真实系统的数字副本，需要能够在测试时根据测量更新状态，并保持轨迹跟踪能力。

Method: PAINT训练生成神经网络并行建模时间状态分布，测试时使用滑动窗口从稀疏测量值预测系统状态。

Result: 在二维湍流流体动力学问题上，PAINT能够保持轨迹跟踪，从稀疏测量值高保真地预测系统状态。

Conclusion: PAINT有潜力开发保持轨迹跟踪的神经孪生，实现更准确的状态估计和决策制定。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [41] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出ISGFAN框架，通过信息分离和全局-局部对抗学习解决噪声干扰和领域偏移共存的跨领域故障诊断问题


<details>
  <summary>Details</summary>
Motivation: 现有迁移故障诊断方法通常假设数据干净或领域相似性足够，但在工业环境中噪声干扰和领域偏移同时存在，限制了方法的有效性

Method: 基于信息分离架构，结合对抗学习和改进的正交损失来解耦领域不变故障表示；采用全局-局部领域对抗方案约束模型的联合分布和边缘分布

Result: 在三个公共基准数据集上的实验表明，该方法优于其他现有方法，证实了ISGFAN框架的优越性

Conclusion: ISGFAN框架能够有效解决噪声条件下跨领域故障诊断的挑战，具有鲁棒性和实用性

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [42] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 该论文结合离线约束编程优化和在线时间网络执行，创建在不确定性下仍可行的调度方案，消除截止期限违规，同时仅增加3-5%的制造周期开销。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要满足严格交付期限，同时应对由过程噪声、设备变异性和人为干预引起的随机任务持续时间。传统确定性调度在现实偏离名义计划时会失效，导致昂贵的紧急修复。

Method: 首先构建柔性作业车间约束编程模型，插入最优缓冲Δ*获得完全主动基线；然后将计划转换为带不确定性的简单时间网络，验证动态可控性，确保实时调度器能为每个有界持续时间重新安排活动而不违反约束。

Result: 在Kacem 1-4基准套件上的广泛蒙特卡洛模拟显示，该方法消除了100%的截止期限违规，同时仅增加3-5%的制造周期开销。可扩展性实验证实CP求解时间和STNU检查在中等规模实例上保持亚秒级。

Conclusion: 该工作展示了时间网络推理如何弥合主动缓冲和动态鲁棒性之间的差距，使工业更接近真正的数字化、自校正工厂。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [43] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了LLM生成的临床思维链的可靠性，发现选择性少样本策略通过高质量多样化示例显著优于其他策略，而随机少样本策略与零样本基线无显著差异。


<details>
  <summary>Details</summary>
Motivation: 解决高质量临床思维链数据稀缺问题，验证LLM生成医疗数据的可靠性，并探索提升其质量的提示策略。

Method: 在辅助生殖技术领域进行盲法比较研究，由资深临床医生评估三种策略生成的思维链：零样本、随机少样本和选择性少样本，并与GPT-4o的评估结果对比。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略(p < .001)，随机少样本策略相比零样本基线无显著改进。AI评估器未能识别这些关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于策略性提示策划而非示例存在，提出"双重原则"框架作为生成可信数据的基础方法，确认人类专业知识在评估高风险临床AI中的不可或缺作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [44] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了一种基于扩展认知理论的新框架，将企业知识重新定义为可测量的动态能力，通过信息访问程序的效率和输出可靠性来量化，为AI时代的企业责任认定提供可审计的度量标准。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业决策中的广泛应用，传统基于人类代理人的企业主观意图认定方法面临挑战，需要新的理论框架来应对算法时代的企业责任问题。

Method: 开发了一个形式化模型，通过整合管道的计算成本和统计验证的错误率，引入连续的组织知识度量S_S(φ)，并推导出阈值知识谓词K_S和企业范围认知能力指数K_{S,t}。

Result: 建立了定量度量与法律标准（实际知识、推定知识、故意无视和鲁莽）之间的操作映射，提供了可测量和可审判的审计工件。

Conclusion: 该研究为在算法时代使企业思维变得可处理和可问责提供了路径，创建了可量化的企业认知能力评估框架。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [45] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个多智能体评估框架，使用LLM自动评估PHI去标识化模型质量，无需依赖昂贵的专家标注，通过多数投票机制选择最佳模型。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化评估通常依赖昂贵的小规模专家标注，限制了模型比较和选择。

Method: 部署多个评估智能体独立判断PHI提取正确性，通过LLM多数投票整合结果生成稳定排名。

Result: 在真实临床笔记语料上的实验表明，TEAM-PHI产生一致准确的排名，与监督评估结果高度匹配。

Conclusion: TEAM-PHI为PHI去标识化提供了实用、安全且经济高效的自动评估和最佳模型选择方案。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [46] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 该论文提出了"被记住权"概念，旨在解决大型语言模型可能导致的集体记忆重塑和信息遗漏风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及，人们开始依赖它们进行信息检索。与传统搜索引擎显示排名列表不同，LLMs提供单一权威的合成回答，可能将多个观点压缩为一个答案，减少用户比较替代方案的能力或意愿。这使少数LLM供应商掌握了信息控制权，可能导致某些叙述、个体或群体被不成比例地压制，而其他则被不成比例地提升，最终威胁集体记忆的完整性。

Method: 提出了"被记住权"概念框架，该框架包含三个核心要素：最小化AI驱动信息遗漏的风险、拥抱公平对待的权利、确保生成内容的真实性最大化。

Result: 提出了一个应对LLMs信息集中化风险的概念性解决方案，旨在保护数字存在有限的群体不被边缘化，防止集体记忆被少数技术系统重塑。

Conclusion: 需要建立"被记住权"来应对LLMs带来的信息控制集中化风险，确保信息检索系统的公平性、完整性和真实性，保护集体记忆的多样性。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [47] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，通过检索增强评估研究想法的合理性和贡献度，并在多领域数据集上验证其优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中的普及，需要建立稳健的评估框架来确保生成想法的有效性和实用性

Method: 引入ScholarEval检索增强评估框架，评估研究想法的两个核心标准：合理性（基于现有文献的方法有效性）和贡献度（相对于先前研究的进步程度）；构建了首个专家标注的多领域研究想法数据集ScholarIdeas

Result: ScholarEval在专家标注的评估标准覆盖度上显著优于所有基线方法；在评估可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统；大规模用户研究显示在文献参与度、想法精炼和实用性方面显著优于深度研究方法

Conclusion: ScholarEval为研究想法评估提供了有效的框架，在多个维度上优于现有方法，并公开发布了代码、数据集和工具供社区使用

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [48] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本文发现大型推理模型存在"推理分心"漏洞，即模型会被恶意嵌入的复杂无关任务分散注意力，导致主要任务准确率下降高达60%。作者提出结合监督微调和强化学习的训练防御方法，可将鲁棒性提高50个百分点以上。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型在数学和编程等复杂任务上表现优异，作者发现这些模型存在一个关键漏洞：容易被提示中恶意嵌入的无关复杂任务分散注意力，从而影响主要任务的完成质量。

Method: 通过跨模型和基准的综合研究分析推理分心现象，并提出了基于合成对抗数据的训练防御方法，结合监督微调和强化学习来增强模型鲁棒性。

Result: 研究表明即使最先进的大型推理模型也高度易受攻击，注入的干扰物可使任务准确率降低高达60%。某些对齐技术会放大这种弱点，模型还可能表现出隐蔽服从行为。

Conclusion: 推理分心是对大型推理模型可靠性的独特且紧迫的威胁，提出的防御方法为实现更安全可信的推理系统提供了实用步骤。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [49] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 本文通过实证研究发现网络交互式智能体系统存在效率瓶颈，提出SpecCache缓存框架结合推测执行来降低网络环境延迟，显著提升缓存命中率和系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统主要关注推理性能而忽视效率，网络环境延迟在整体延迟中占比高达53.7%，亟需解决效率瓶颈问题。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，提出SpecCache缓存框架，通过推测执行增强缓存效果，减少网络环境开销。

Result: 在15个模型和5个提供商上的实验显示，SpecCache相比随机缓存策略将缓存命中率提升高达58倍，网络环境开销降低3.2倍，且不降低智能体系统性能。

Conclusion: SpecCache能有效解决网络交互式智能体系统的效率瓶颈，显著提升系统性能，为高效智能体系统设计提供了实用解决方案。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [50] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出DTKG双轨知识图谱验证与推理框架，解决多跳问答中并行事实验证和链式推理的局限性


<details>
  <summary>Details</summary>
Motivation: 现有方法在并行事实验证和链式多跳推理上各有优劣，单一方法无法同时高效处理两种推理模式，影响多跳问答的效率和准确性

Method: 基于认知科学双过程理论，构建包含分类阶段和分支处理阶段的双轨框架，结合LLM响应验证和KG路径构建

Result: 未在摘要中明确说明具体实验结果

Conclusion: DTKG框架旨在通过双轨方法克服现有技术在并行事实验证和链式推理上的局限性，提升多跳问答性能

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [51] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个紧凑型知识图谱与符号验证器，通过强制执行可数学解释的规则来改进LLM的推理准确性，在FDA基准测试中实现了100%的精确匹配。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在推理过程中违反简单数学或逻辑约束的问题，确保推理步骤的数学可解释性和一致性。

Method: 构建MedRule-KG紧凑型知识图谱，包含实体、关系和三个领域启发规则，配合符号验证器检查预测并应用最小修正以保证一致性。

Result: 在90个FDA基准测试中，使用MedRule-KG将精确匹配从0.767提升到0.900，添加验证器后达到1.000精确匹配并完全消除规则违反。

Conclusion: MedRule-KG为安全数学推理提供了一个通用框架，通过知识图谱和符号验证确保推理的准确性和一致性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [52] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了SELECT框架，通过动态锚点选择解决文本到图像扩散模型中概念擦除的锚点敏感性问题，克服固定锚点策略导致的概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖固定锚点策略，导致概念重现和侵蚀等关键问题，需要解决锚点选择对擦除效果的敏感性。

Method: 基于因果追踪发现擦除对锚点选择的敏感性，定义兄弟排他概念作为更优锚点类别，提出SELECT动态锚点选择框架，采用两阶段评估机制自动发现最优擦除锚点并识别边界锚点以保护相关概念。

Result: SELECT作为通用锚点解决方案，能高效适配多种擦除框架，在关键性能指标上持续优于现有基线，单个概念的锚点挖掘平均仅需4秒。

Conclusion: 动态锚点选择框架SELECT有效解决了固定锚点策略的局限性，提升了概念擦除的精确性和效率。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [53] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该论文研究用户与算法交互中的对齐问题，提出用户需要足够的前瞻性才能有效引导算法符合其真实兴趣，而非被算法目标所同化。


<details>
  <summary>Details</summary>
Motivation: 用户在算法平台上经常表现出不一致的偏好，花费时间在低价值内容上，无意中向算法发送错误信号。研究用户需要什么条件才能让算法与其真实兴趣对齐。

Method: 将用户决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长），采用多领导者-单跟随者的扩展Stackelberg博弈模型，用户作为领导者承诺参与策略，算法基于观察到的交互做出最佳响应。

Result: 存在一个关键时间范围：足够有远见的用户可以实现对齐，而短视的用户反而会被算法目标同化。这个关键范围可能很长，但即使是小的代价信号（如额外点击）也能显著减少对齐负担。

Conclusion: 框架解释了具有不一致偏好的用户如何在Stackelberg均衡中引导参与驱动型算法与其兴趣对齐，揭示了实现对齐的挑战和潜在解决方案。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [54] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 本文提出了一种结合监控-生成-验证的三阶段迭代系统，在GSM8K数学推理任务上取得了75.42%的准确率，优于现有方法，且需要更少的尝试次数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法存在两种孤立范式：监控-生成方法擅长策略规划但缺乏验证机制，生成-验证方法能够迭代优化但缺乏任务评估。这种分离导致策略失败无反馈、优化无战略基础的低效问题。

Method: 基于Flavell的认知监控模型，实现监控-生成-验证三阶段迭代系统，将策略规划与验证机制有机结合。

Result: 在GSM8K上达到75.42%准确率，优于SELF-REFINE(68.44%)和Self-Verification(67.07%)，尝试次数更少(1.3 vs 2.0)，推理成本增加27-37%。

Conclusion: 前期监控能产生更高质量的初始解决方案，减少优化需求，但需要在算术推理之外的任务上进行评估以验证通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [55] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出HSCM因果框架，模仿人类视觉系统的层次处理和多级学习，通过解耦和重加权图像属性来增强跨领域泛化能力


<details>
  <summary>Details</summary>
Motivation: 克服传统领域泛化模型的局限性，传统方法依赖统计数据捕捉数据-标签依赖关系，而HSCM专注于建模细粒度因果机制

Method: 模仿人类视觉系统的层次处理，解耦和重加权关键图像属性（颜色、纹理、形状），利用人类智能的灵活性和适应性

Result: 在理论和实证评估中，HSCM优于现有的领域泛化模型，提供更稳健的性能和可解释性

Conclusion: HSCM为捕捉因果关系和提升模型鲁棒性提供了更原则性的方法，在动态复杂环境中实现更有效的迁移学习

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [56] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: RGMem是一个受物理学重整化群思想启发的自演化记忆框架，通过多尺度组织对话历史，从微观互动中提取用户偏好和深层特征，实现语言代理的长期记忆和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统受限于有限上下文窗口和静态参数记忆，难以建模跨会话的长期用户状态和行为一致性。RAG和显式记忆系统主要关注事实级存储检索，缺乏从多轮对话中提取潜在偏好和深层特征的能力。

Method: 提出RGMem框架，通过分层粗粒化和重标度操作组织对话历史：首先从片段中提取语义和用户洞察，然后逐步形成动态演化的用户画像，将记忆演化建模为信息压缩和涌现的多尺度过程。

Result: 该框架能够从噪声和微观层面的互动中实现高级别、准确的用户画像，解决了长期有效用户建模的挑战。

Conclusion: RGMem通过多尺度记忆演化过程，实现了语言代理的长期记忆和行为一致性，为个性化对话系统提供了新的解决方案。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [57] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: 提出了ReviewSense框架，利用大型语言模型将客户评论转化为可操作的商业建议，超越了传统偏好预测系统。


<details>
  <summary>Details</summary>
Motivation: 客户反馈对战略增长至关重要，但现有AI系统主要关注预测用户偏好，缺乏将非结构化评论转化为面向业务的规范性建议的能力。

Method: 整合聚类、LLM适配和专家驱动评估的统一业务管道，识别客户情绪中的关键趋势、重复问题和具体关注点。

Result: 初步人工评估显示模型建议与业务目标高度一致，证明其在数据驱动决策中的潜力。

Conclusion: 该框架为AI驱动的情感分析提供了新视角，展示了其在优化商业策略和最大化客户反馈价值方面的价值。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [58] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: NP-ENGINE是首个用于训练和评估LLMs解决NP难问题的综合框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。通过该框架训练的QWEN2.5-7B-NP模型在NP-BENCH基准上显著超越GPT-4o，并展现出强大的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学、编程等推理任务上表现出色，但其解决复杂优化问题特别是NP难问题的能力尚未充分探索。现有研究缺乏系统性的训练和评估框架来专门针对这类问题。

Method: 提出NP-ENGINE框架，包含可控实例生成器、规则验证器和启发式求解器，支持可扩展的层次化难度RLVR训练。使用NP-ENGINE-DATA进行零RLVR课程学习训练QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著超越GPT-4o，达到同尺寸模型中的SOTA性能。同时展现出强大的跨领域泛化能力，包括推理任务和非推理任务。观察到任务多样性增加可提升跨领域泛化性能。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，为RLVR的扩展规律提供了新见解。NP-ENGINE框架为系统研究LLMs解决NP难问题提供了有效工具。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [59] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: Doug是一个基于向量符号架构的编程语言，所有类型化程序都能在多项式时间内终止，支持神经网络学习类型，旨在实现人类水平的技能获取效率。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够模拟人类技能获取速度的编程语言，超越现有方法的效率，更接近人脑实际表示和学习的模型。

Method: 基于轻量线性函数式编程语言(LLFPL)，使用向量符号架构编码类型和项，采用全息声明性内存的槽值编码方案，支持神经网络学习类型。

Result: 提出了Doug语言框架，使嵌入空间中的点可被解释为类型，且邻近点的类型在结构和内容上相似。

Conclusion: Doug为实现人类水平的技能获取效率提供了一种新途径，使程序合成能够以接近人类学习速度的方式进行，更接近人脑实际表示和学习的模型。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [60] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出了Urban-R1框架，使用强化学习对齐多模态大语言模型与城市通用智能目标，通过分组相对策略优化和城市区域画像任务来缓解地理偏见，提升跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于监督微调的城市基础模型存在持续的地理偏见问题，导致区域预测偏差和有限泛化能力，需要开发更公平可信的城市智能系统。

Method: 采用强化学习后训练框架，使用分组相对策略优化(GRPO)来优化跨地理群体的推理能力，并以城市区域画像作为代理任务提供可测量的多模态奖励。

Result: 在多个区域和任务上的广泛实验表明，Urban-R1有效缓解了地理偏见，提升了跨区域泛化性能，优于监督微调训练和闭源模型。

Conclusion: 强化学习对齐是实现公平可信城市智能的有前景路径，Urban-R1框架为解决城市AI系统的地理偏见问题提供了有效方案。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [61] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: BuildArena是首个面向语言驱动工程建造的物理对齐交互基准，用于评估LLM在工程建造自动化中的能力。


<details>
  <summary>Details</summary>
Motivation: 工程建造自动化需要将自然语言规范转化为物理可行的结构，但现代LLM的建造能力尚未得到充分评估。

Method: 开发了高度可定制的基准框架，包含可扩展的任务设计策略、3D空间几何计算库和基线LLM代理工作流。

Result: 在8个前沿LLM上全面评估了它们在语言驱动和物理基础建造自动化方面的能力。

Conclusion: BuildArena填补了LLM在工程建造领域能力评估的空白，为社区提供了首个物理对齐的交互基准。

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [62] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 提出了Ripple Effect Protocol (REP)，一种协调协议，让智能体不仅分享决策，还分享轻量级敏感性信号，从而在群体中实现更快更稳定的协调。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理通信协议如A2A和ACP强调通信而非协调，随着代理群体规模扩大，这导致脆弱的集体行为，个体智能的代理会收敛到较差的群体结果。

Method: REP协议让代理分享决策和轻量级敏感性信号（表达关键环境变量变化时选择如何改变），这些敏感性在局部网络中传播。协议规范分离了必需的消息模式与可选的聚合规则。

Result: 在三个领域的基准测试中：供应链级联（啤酒游戏）、稀疏网络中的偏好聚合（电影调度）和可持续资源分配（Fishbanks），REP相比A2A将协调准确性和效率提高了41%到100%。

Conclusion: 通过将协调作为协议级能力，REP为新兴的智能体互联网提供了可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [63] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow是一个基于知识图谱的检索增强生成框架，通过流匹配目标优化检索策略，从文本丰富的知识图谱中高效检索准确多样的知识。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成方法难以从文本丰富的知识图谱中检索准确多样的信息，而过程奖励模型需要昂贵的过程级监督信号。

Method: 使用基于转移的流匹配目标联合优化检索策略和流估计器，将检索结果的奖励分解到中间检索状态，指导检索策略按比例检索候选知识。

Result: 在STaRK基准测试中，GraphFlow在命中率和召回率上平均优于强基线（包括GPT-4o）10%，并对未见过的知识图谱表现出强泛化能力。

Conclusion: GraphFlow能有效从文本丰富的知识图谱中检索准确多样的知识，具有优秀的性能和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [64] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 提出了一种半监督置信分布学习方法ssCDL，用于解决不确定知识图谱补全中置信度分布极不平衡的问题，通过将置信度转换为分布并利用元学习生成伪标签来增强训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱补全方法忽略了置信度的极端不平衡分布，导致学习到的嵌入表示不足以支持高质量的知识图谱补全。

Method: ssCDL将每个三元组置信度转换为置信度分布，通过元学习预测未见三元组的置信度作为伪标签，在标记数据和带伪标签的未标记数据上迭代学习嵌入表示。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在不同评估指标上均优于现有最先进方法。

Conclusion: ssCDL通过引入置信度分布学习和半监督训练，有效解决了置信度分布不平衡问题，提升了不确定知识图谱补全的性能。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [65] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种新颖的强化学习算法，通过基于计数的内在奖励来增强语言模型推理中的探索能力，显著提升多步推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习范式依赖稀疏的结果奖励和有限的探索，导致语言模型陷入重复和次优的推理模式。

Method: 使用轻量级的Coin Flipping Network估计推理轨迹的伪计数和认知不确定性，将其转换为内在奖励，并与GRPO等先进RL框架集成。

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著超越强基线性能，帮助策略逃离局部最优。

Conclusion: 针对性的内在动机可以使探索在语言模型推理中变得可靠有效。

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [66] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 大规模AI模型正在变革神经科学研究，通过端到端学习从原始脑信号中提取特征，在神经影像、脑机接口、分子神经科学、临床辅助和疾病应用等领域带来突破。


<details>
  <summary>Details</summary>
Motivation: 传统计算方法在神经科学中面临多模态数据整合、时空模式解释等挑战，需要更强大的AI模型来促进神经科学研究的范式转变。

Method: 探索大规模AI模型在五个主要神经科学领域的应用：神经影像数据处理、脑机接口与神经解码、分子神经科学与基因组建模、临床辅助与转化框架、神经精神疾病应用。

Result: 这些模型能够解决多模态神经数据整合、时空模式解释等关键计算神经科学挑战，并促进临床转化应用。

Conclusion: 神经科学与AI的互动日益互惠，生物启发的架构约束有助于开发更可解释和计算高效的模型，但需要严格的评估框架、有效的领域知识整合和全面的伦理指南。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [67] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了一个基于大语言模型的代理框架AFL，用于完全自动化解决复杂车辆路径问题，从问题实例到解决方案无需人工干预，在代码可靠性和解决方案可行性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前解决复杂车辆路径问题需要大量专家投入，现有基于大语言模型的方法仍依赖外部干预，导致自主性受限、执行错误和解决方案可行性低的问题。

Method: AFL框架将整体流程分解为三个可管理的子任务，使用四个专门代理通过协调交互确保跨功能一致性和逻辑合理性，直接从原始输入中提取知识并生成自包含代码。

Result: 在60个复杂VRP问题上的实验验证了框架的有效性和通用性，与精心设计的算法性能相当，在代码可靠性和解决方案可行性方面显著优于现有LLM基线方法，在评估基准上接近100%的成功率。

Conclusion: AFL框架实现了从问题实例到解决方案的完全自动化，解决了现有LLM方法依赖外部干预的问题，在复杂车辆路径问题上表现出优异的性能和可靠性。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [68] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 本文综述了智能AI从基于管线的系统向模型原生范式的转变，其中规划、工具使用和记忆等能力从外部逻辑编排转变为模型内部参数化实现。强化学习是实现这一范式转变的关键算法引擎。


<details>
  <summary>Details</summary>
Motivation: 追踪智能AI的范式转变，从外部编排的管线系统到能力内部化的模型原生范式，探索AI从被动响应到主动行动、推理和适应的演进。

Method: 系统性地回顾了规划、工具使用和记忆三大能力的演进过程，从外部脚本模块到端到端学习行为，并分析了强化学习在实现这一转变中的核心作用。

Result: 揭示了智能AI向模型原生发展的清晰轨迹，展示了LLM+RL+Task的统一解决方案在语言、视觉和具身领域的应用，以及该范式对深度研究代理和GUI代理等主要应用的重塑。

Conclusion: 智能AI正朝着模型原生的方向发展，从构建应用智能的系统转向开发通过经验增长智能的模型，标志着从应用智能到生长智能的根本转变。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [69] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 该调查论文首次全面概述了基于强化学习的智能搜索代理领域，从功能角色、优化策略和优化范围三个维度组织这一新兴领域，旨在构建可靠且可扩展的RL驱动智能搜索系统。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道通常是单轮和启发式的，缺乏对检索和推理的自适应控制。基于强化学习的智能搜索通过多步交互解决这些限制，为自适应和自我改进的搜索行为提供强大机制。

Method: 从三个互补维度组织基于RL的智能搜索领域：(i) RL的功能角色，(ii) RL的优化策略，(iii) RL的应用范围。总结了代表性方法、评估协议和应用。

Result: 提供了该领域的首个全面概述，建立了系统化的分类框架，并识别了代表性方法和评估标准。

Conclusion: 基于强化学习的智能搜索是一个有前景的研究方向，该调查旨在激发未来关于RL与智能搜索集成的研究，推动构建可靠和可扩展的RL驱动智能搜索系统。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [70] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 提出了一种基于代理模型的仿真驱动工程工作流，通过训练轻量级仿真器来解决高计算成本和黑盒不透明性问题，支持不确定性量化和可解释AI分析。


<details>
  <summary>Details</summary>
Motivation: 解决仿真驱动工程工作流面临的两个核心障碍：(1) 高计算成本，(2) 黑盒组件导致的透明度和可靠性限制。

Method: 使用紧凑实验设计训练轻量级仿真器，结合全局效应分析、不确定性分析和局部归因，评估不同代理模型间解释的一致性。

Result: 在混合电动飞机多学科设计和城市隔离代理模型两个案例中，该方法实现了秒级大规模探索，发现非线性交互和涌现行为，识别关键设计和政策杠杆。

Conclusion: 代理模型与可解释AI的耦合能够有效支持复杂系统分析，诊断代理模型充分性，指导进一步数据收集或模型改进。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [71] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 提出ELMM模型用于多模态知识图谱补全，通过多视角视觉令牌压缩器和注意力剪枝策略，在保持性能的同时显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 现有MKG存在不完整性问题，而多模态大语言模型在MKGC任务中面临图像令牌过多导致语义噪声、模态冲突和计算成本高的挑战

Method: 使用基于多头注意力的多视角视觉令牌压缩器自适应压缩图像令牌，设计注意力剪枝策略减少冗余层，并通过线性投影补偿性能损失

Result: 在FB15k-237-IMG和WN18-IMG基准测试中达到最先进性能，同时大幅提升计算效率

Conclusion: ELMM为多模态知识图谱补全建立了新范式，在性能和效率方面均表现优异

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [72] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然的人类交互行为。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要同时感知和生成多种模态信息。现有模型难以实现这种自然的交互模式。

Method: 采用新颖的SA-MoE架构，将各模态路由到专门的专家模块，通过统一的注意力骨干网络进行融合，实现联合多模态感知和并发生成。

Result: 在语音交互和机器人操作基准测试中，ELLSA达到模态特定基线的性能，同时支持高级多模态和全双工行为，如对话轮转、动作轮转、缺陷指令拒绝等。

Conclusion: ELLSA朝着更自然和通用的交互智能迈出了一步，为追求通用人工智能做出了贡献。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [73] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一的框架，通过分层组织图信息和引入规划代理，解决了视觉语言模型在图理解中的可扩展性和模态协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在图理解中面临输入令牌限制、可扩展性瓶颈以及缺乏有效的文本和视觉模态协调机制的问题。

Method: GraphVista采用分层方法将图信息组织成轻量级GraphRAG基础，仅检索任务相关的文本描述和高分辨率视觉子图；同时引入规划代理，根据任务复杂度将任务路由到最适合的模态。

Result: GraphVista能够扩展到比现有基准大200倍的大型图，在质量上比现有最先进基线提升高达4.4倍，充分挖掘了两种模态的互补优势。

Conclusion: GraphVista通过分层信息组织和智能模态路由，有效解决了图理解中的可扩展性和模态协调挑战，显著提升了性能。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [74] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出Domain-Contextualized Concept Graph (CDC)框架，将领域作为概念表示的一等元素，采用C-D-C三元组结构，实现上下文感知推理和跨领域类比。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定本体论的刚性层次结构，根源在于将领域视为隐含上下文而非显式推理组件。

Method: 采用C-D-C三元组结构<概念, 关系@领域, 概念'>，基于认知-语言同构映射原则，定义20多个标准化关系谓词，并在Prolog中实现完整推理能力。

Result: 在教育、企业知识系统和技术文档等案例研究中，CDC实现了上下文感知推理、跨领域类比和个性化知识建模。

Conclusion: CDC框架克服了传统基于本体的知识表示限制，提供了传统框架无法实现的能力。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [75] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的智能LLM代理，能够自动完成从数据源到分析师级深度研究报告的端到端流程，仅用80亿参数就超越了基于最先进专有LLM的工作流程代理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于工作流程的数据代理在特定数据任务上表现良好，但由于依赖预定义工作流程，无法实现完全自主的数据科学。随着强大LLM的出现，实现从原始数据源到分析师级深度研究报告的自主数据科学变得可行。

Method: 提出了基于课程学习的智能代理训练范式，模拟人类数据科学家的学习轨迹，使LLM能够在真实环境中逐步获取和整合多种能力。同时引入了数据基础的轨迹合成框架来构建高质量训练数据。

Result: 实验表明，仅用80亿参数的DeepAnalyze在广泛的数据任务上表现出色，包括数据问答、专业分析任务和开放式数据研究，超越了基于最先进专有LLM的工作流程代理。

Conclusion: DeepAnalyze模型、代码和训练数据已开源，为自主数据科学的发展铺平了道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [76] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 该论文提出通过强化学习训练VLM智能体进行显式视觉状态推理，构建内部世界模型，在五个多样化智能体基准测试中实现了3倍性能提升，超越了GPT-5等专有推理模型。


<details>
  <summary>Details</summary>
Motivation: 训练视觉语言模型智能体的关键挑战是从文本状态转向复杂视觉观察，这引入了部分可观测性并需要强大的世界建模能力。论文旨在探索VLM智能体能否通过显式视觉状态推理构建内部世界模型。

Method: 将智能体推理过程架构化为部分可观测马尔可夫决策过程，分解为状态估计和转移建模两个关键组件，设计世界建模奖励提供密集监督，并引入双层广义优势估计进行回合感知信用分配。

Result: 通过视觉状态推理，一个30亿参数模型在五个多样化智能体基准测试中达到0.82分，相比未训练版本(0.21)提升了3倍，超越了GPT-5(0.75)、Gemini 2.5 Pro(0.67)和Claude 4.5(0.62)。

Conclusion: 智能体内部信念表示的最优形式是任务依赖的：自然语言擅长捕捉一般任务中的语义关系，而结构化格式对于精确操作和控制不可或缺。显式视觉状态推理是构建VLM智能体世界模型的有效方法。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [77] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 该论文提出了一种新的评估方法，用于测试用户能否从强化学习算法的解释中识别出智能体的目标。在Ms. Pacman环境中测试四种可解释强化学习算法，发现只有一种算法在测试目标上取得了超过随机准确率的表现，且用户普遍对自己的选择过度自信。


<details>
  <summary>Details</summary>
Motivation: 可解释强化学习算法的核心应用之一是调试，但目前缺乏对其相对性能的比较评估。

Method: 使用Atari的Ms. Pacman环境和四种可解释强化学习算法，通过测试用户能否从决策解释中识别智能体目标来评估算法性能。

Result: 只有一种算法在测试目标上取得了超过随机准确率的表现；用户普遍过度自信；用户自报的识别和理解难易度与实际准确率不相关。

Conclusion: 当前的可解释强化学习算法在帮助用户识别智能体目标方面效果有限，用户的主观感知与实际性能存在差异。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [78] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体协作的LLM框架，用于自动化GPU内核优化，相比基线方法能生成正确解决方案并实现高达16倍的运行时性能提升。


<details>
  <summary>Details</summary>
Motivation: GPU内核效率对现代AI发展至关重要，但优化过程困难且劳动密集，现有LLM方法主要将其视为单次生成或简单优化工具，无法有效应对复杂的内核优化场景。

Method: 采用多智能体协作框架，通过系统化探索设计空间、基于硬件知识的指导、动态上下文管理和策略搜索，模拟专家工程师的工作流程，使LLM能够推理硬件权衡、整合性能分析反馈并迭代优化内核。

Result: 在KernelBench基准测试中，相比基线智能体，该系统能生成正确解决方案（基线经常失败），并实现高达16倍更快的运行时性能。

Conclusion: 智能体LLM框架在实现完全自动化、可扩展的GPU内核优化方面具有巨大潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [79] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进LLM在多轮工具增强对话中的行为，通过检测8种特定工具调用错误并提供针对性反馈，可将工具调用准确率提升高达13%。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在现实应用中越来越普遍，但工具使用错误仍然阻碍其可靠性，需要更稳健的LLM与外部工具集成方法。

Method: 定义8种工具调用错误类别，构建合成数据集训练ToolCritic，让主LLM基于ToolCritic的反馈修订响应。

Result: 在Schema-Guided Dialogue数据集上的实验表明，ToolCritic相比零样本提示和自我修正技术，将工具调用准确率提升了高达13%。

Conclusion: ToolCritic代表了在现实世界对话应用中实现更稳健LLM与外部工具集成的有希望的一步。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [80] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID是一个多智能体AI系统，通过整合自由文本描述和本体标签，结合检索增强生成技术，显著提升了基因集注释的准确性和鲁棒性，在脑细胞类型注释中取得了77%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基因集富集分析方法依赖精心策划的注释，在处理特征不明确的基因时表现不佳，而大型语言模型难以在结构化本体中表示复杂的生物学知识。

Method: 开发了BRAINCELL-AID多智能体AI系统，整合自由文本描述和本体标签，采用检索增强生成技术，通过PubMed文献精炼预测，减少幻觉并增强可解释性。

Result: 在小鼠基因集注释中实现了77%的准确率，成功注释了5,322个脑细胞簇，识别了基底神经节相关细胞类型，并发现了区域特异性基因共表达模式。

Conclusion: BRAINCELL-AID创建了一个有价值的资源，支持社区驱动的细胞类型注释，为脑细胞功能研究提供了新的见解。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [81] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 本研究开发了两种基于大语言模型的系统用于企业信用评估中的证据推理：单代理系统(NAS)和基于辩论的多代理系统(KPD-MADS)，后者基于卡尔·波普尔的批判性对话框架，在推理质量和实用性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 企业信用评估中的定性非财务指标对贷款偿还结果具有决定性影响，但难以形式化。现有方法主要关注数值预测，对专业贷款评估所需的解释性判断支持有限。

Method: 开发了两种LLM系统：单代理系统(NAS)通过单次推理管道生成双向分析；基于辩论的多代理系统(KPD-MADS)采用十步结构化交互协议，实现对抗性验证。两种系统在三个真实企业案例中应用，由信用风险专家评估。

Result: 相比人工专家报告，两种系统都实现了显著的生产力提升(NAS: 11.55秒/案例；KPD-MADS: 91.97秒；人工基线: 1920秒)。KPD-MADS在解释充分性(4.0 vs 3.0)、实际适用性(4.0 vs 3.0)和可用性(62.5 vs 52.5)方面获得更高评分。

Conclusion: 结构化多代理交互能够增强金融AI中的推理严谨性和可解释性，推动企业信用评估中可扩展且可辩护的自动化进程。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [82] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出基于手工特征的方法，通过融合颜色统计、多色彩空间直方图和纹理特征来评估鱼类新鲜度，在FFE数据集上取得了显著优于深度学习的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼类新鲜度存在主观性强、不一致和难以标准化的问题，需要更客观准确的自动化评估方法。

Method: 从鱼类眼部图像中系统提取并融合互补特征描述符，包括颜色统计、多色彩空间直方图、局部二值模式(LBP)和灰度共生矩阵(GLCM)等纹理特征，同时捕捉全局色度变化和局部退化特征。

Result: 在FFE数据集上，LightGBM分类器达到77.56%准确率，比之前深度学习基线提升14.35%；使用增强数据时，人工神经网络达到97.16%准确率，比之前最佳结果提升19.86%。

Conclusion: 精心设计的手工特征经过策略性处理后，能够为自动化鱼类新鲜度评估提供鲁棒、可解释且可靠的解决方案，对食品质量监控具有重要应用价值。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [83] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: 提出了PILLM框架，将物理原理融入大型语言模型，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，实现高性能且可解释的故障检测。


<details>
  <summary>Details</summary>
Motivation: HVAC系统能耗占建筑能耗很大比例，现有方法要么缺乏适应性（基于规则），要么缺乏透明度和物理合理性（深度学习），需要开发既准确又可解释的检测方法。

Method: 使用物理信息反射和交叉算子，在进化循环中嵌入热力学和控制理论约束，自动生成和优化异常检测规则。

Result: 在公开建筑故障检测数据集上达到最先进性能，生成的诊断规则具有可解释性和可操作性。

Conclusion: PILLM框架推动了智能建筑系统中可信赖和可部署AI的发展，实现了自适应且物理基础的异常检测。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [84] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出了ProtocolBench基准测试来系统评估多智能体通信协议，发现协议选择显著影响系统性能。同时开发了ProtocolRouter学习型协议路由器，能根据场景需求动态选择最优协议，提升系统可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体系统中，通信协议层是影响性能和可靠性的关键因素，但当前协议选择缺乏标准化指导，主要依赖直觉。

Method: 构建ProtocolBench基准测试，从任务成功率、端到端延迟、消息开销和故障恢复能力四个维度系统比较不同协议。开发ProtocolRouter学习型协议路由器，根据需求和运行时信号动态选择协议。

Result: 协议选择显著影响系统行为：在流式队列场景中，完成时间差异达36.5%，平均延迟差异3.48秒。ProtocolRouter相比最佳单协议基线，故障恢复时间减少18.1%，在GAIA场景中成功率更高。

Conclusion: 协议选择对多智能体系统性能至关重要，ProtocolBench和ProtocolRouter为协议评估和选择提供了标准化方法，能显著提升系统可靠性和性能。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [85] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 开发了一个结合ECG基础模型和可解释XGBoost分类器的混合框架，用于预测急性心肌梗死后恶性室性心律失常风险，在提高准确性的同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡的主要原因，但传统风险评分性能有限，而端到端深度学习模型缺乏临床信任所需的可解释性。

Method: 使用ECG基础模型提取150维诊断概率特征，通过特征选择后训练XGBoost分类器，并采用SHAP方法进行可解释性分析。

Result: 混合模型AUC达到0.801，优于KNN(0.677)、RNN(0.676)和1D-CNN(0.720)，SHAP分析显示模型识别特征与临床知识高度一致。

Conclusion: 该混合框架通过验证基础模型输出作为有效自动化特征工程，为构建可信赖、可解释的AI临床决策支持系统提供了新范式。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [86] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究网络部署的工具增强LLM健康教练系统，通过离线策略评估发现统一的重工具策略会损害特定用户群体，特别是低健康素养/高自我效能用户。使用轻量模拟器显示添加早期信息增益奖励可改善特质识别和成功率。


<details>
  <summary>Details</summary>
Motivation: 评估工具增强LLM健康教练在真实用户中的表现，探索个性化策略以避免统一策略对特定用户群体的伤害。

Method: 使用离线策略评估(OPE)分析因子化决策头(工具/风格)，并通过轻量模拟器测试添加早期信息增益奖励的效果。

Result: 统一重工具策略会提高平均价值但损害特定用户群体；添加早期信息增益奖励可缩短特质识别时间，提高目标成功率和pass@3指标。

Conclusion: 提出了基于评估的个性化路径：冻结生成器，在类型化奖励上学习子群体感知决策头，并始终报告按原型分类的指标以揭示被平均值掩盖的子群体损害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [87] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了TD-HNODE模型，通过时间详细超图和神经ODE框架学习疾病进展的连续时间动态，在2型糖尿病和心血管疾病进展建模中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 疾病进展建模面临挑战：需要基于不规则时间事件样本学习连续时间动态，且患者存在异质性（不同进展速率和路径）。现有方法要么缺乏从真实数据学习的适应性，要么无法捕捉复杂的连续时间动态。

Method: TD-HNODE模型将疾病进展表示为时间详细超图，通过神经ODE框架学习连续时间进展动态。包含可学习的TD-Hypergraph Laplacian，捕捉疾病并发症标记在进展轨迹内部和之间的相互依赖关系。

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在建模2型糖尿病和相关心血管疾病进展方面优于多个基线方法。

Conclusion: TD-HNODE能够有效解决疾病进展建模中的挑战，通过超图表示和神经ODE框架成功捕捉了复杂的连续时间动态和患者异质性。

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [88] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: Coinvisor是一个基于强化学习的加密货币投资分析聊天机器人，通过多智能体框架和强化学习工具选择机制，解决了现有方法的局限性，提供实时、准确的投资分析支持。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币投资中信息碎片化、高波动性以及现有分析方法（手动分析、数据聚合平台、静态LLM代理）的不足，如耗时、功能有限、缺乏实时数据集成和多步推理能力。

Method: 采用基于强化学习的多智能体框架，集成多样化分析工具，通过强化学习机制进行工具选择和规划，支持实时交互和动态内容分析。

Result: 在工具编排方面，相比基础模型，召回率提高40.7%，F1分数提高26.6%。用户研究显示高满意度（4.64/5），参与者更偏好Coinvisor而非通用LLM和现有加密平台（4.62/5）。

Conclusion: Coinvisor通过强化学习驱动的多智能体框架，有效提升了加密货币投资分析的准确性和用户体验，为投资者提供了更可靠的分析支持。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [89] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: RubiSCoT是一个AI支持的论文评估框架，使用自然语言处理技术提供从开题到最终提交的自动化评估，旨在提高学术评估的一致性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且存在评估者主观差异，需要更高效、一致的自动化解决方案。

Method: 采用大型语言模型、检索增强生成和结构化思维链提示等先进NLP技术，包括初步评估、多维评估、内容提取、基于量规的评分和详细报告。

Result: 开发了RubiSCoT框架的设计和实现，展示了其在学术评估过程中的应用潜力。

Conclusion: RubiSCoT有潜力通过提供一致、可扩展和透明的评估来优化学术评估流程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [90] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出LaGAT混合框架，将基于图注意力的神经MAPF策略MAGAT集成到搜索算法LaCAM中，在密集多智能体路径规划场景中优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在密集多智能体路径规划问题中难以实时找到近似最优解，需要结合学习和搜索的优势。

Method: 使用增强的MAGAT架构、在目标地图上预训练再微调的策略，以及处理不完美神经引导的死锁检测机制。

Result: LaGAT在密集场景中表现优于纯搜索和纯学习方法。

Conclusion: 精心设计的混合搜索方法为紧密耦合的挑战性多智能体协调问题提供了强大解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [91] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: FBI_LTL是一个为基于仿真的规划问题设计的多样化规划器，使用线性时序逻辑定义语义多样性标准，生成语义多样化的计划。


<details>
  <summary>Details</summary>
Motivation: 传统规划器只生成单一计划，可能无法满足代理的偏好；现有多样化规划方法可能产生语法不同但语义相同的解决方案。

Method: 利用线性时序逻辑定义语义多样性标准，并将这些LTL多样性模型直接集成到搜索过程中。

Result: 在各种基准测试中，FBI_LTL相比基线方法能生成更多样化的计划。

Conclusion: 这项工作确立了在基于仿真的环境中进行语义引导多样化规划的可行性，为在传统基于模型的方法失效的现实非符号领域开辟了新途径。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [92] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 开发了一种基于主动推理的路径规划方法，用于智能代理的自主控制，通过构建证据地图和计算变分自由能量来指导代理移动，平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 为了在地理区域内维持共同作战态势图，需要开发能够自主侦察的方法，解决探索（搜索广阔区域）与利用（跟踪已识别目标）之间的平衡问题。

Method: 使用Dempster-Shafer理论和高斯传感器模型构建生成模型，采用贝叶斯方法更新后验概率分布，计算所有位置的变分自由能量，指导代理向最小化自由能量的位置移动。

Result: 该方法能够有效指导智能代理在地理区域内的移动，平衡了探索和利用的需求，通过最小化自由能量来优化路径规划。

Conclusion: 主动推理路径规划方法为自主智能代理控制提供了有效解决方案，能够在地理侦察任务中实现探索与利用的平衡，具有实际应用价值。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [93] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 法律机器学习需要处理标签不确定性，因为法律结果常受人为干预影响，导致同一案件可能有不同结果。在欧洲人权法院案例分类中，标签构建方式显著影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 法律机器学习通常将过去案例结果视为真实标签，但法律结果常受和解、上诉等人为干预影响，造成标签不确定性，即同一案件可能有不同结果。

Method: 在欧洲人权法院案例分类背景下，研究不同标签构建方式对模型行为的影响，探讨处理标签不确定性的方法。

Result: 研究表明，训练过程中标签的构建方式会显著影响模型的行为和表现。

Conclusion: 标签不确定性是AI与法律领域的重要问题，需要被考虑和处理，因为它会影响模型的行为和结果。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [94] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个推理时、可插拔模型的多模态虚假信息检测框架，通过分解为四个模块：视觉真实性评估、跨模态一致性分析、检索增强的事实核查和校准判断，结合视觉语言模型推理和网络检索，在MMFakeBench验证集上达到81.65% F1分数，优于最强零样本基线7.65个百分点。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿结合文本和图像的多模态帖子传播虚假信息，超出了人工事实核查的能力范围。监督检测模型需要领域特定的训练数据，且无法泛化到不同的操纵策略。

Method: MIRAGE框架包含四个顺序模块：1) 视觉真实性评估检测AI生成图像；2) 跨模态一致性分析识别上下文无关的重新利用；3) 检索增强的事实核查通过迭代问题生成将声明基于网络证据；4) 校准判断模块整合所有信号。该框架协调视觉语言模型推理与针对性网络检索，输出结构化且带有引用的推理过程。

Result: 在MMFakeBench验证集（1,000个样本）上，MIRAGE与GPT-4o-mini组合达到81.65% F1分数和75.1%准确率，优于最强零样本基线（GPT-4V与MMD-Agent的74.0% F1）7.65个百分点，同时保持34.3%的假阳性率，而仅使用判断的基线为97.3%。测试集结果（5,000个样本）确认了泛化能力，达到81.44% F1和75.08%准确率。消融研究显示视觉验证贡献5.18 F1点，检索增强推理贡献2.97点。

Conclusion: 分解的代理推理与网络检索可以在无需领域特定训练的情况下匹配监督检测器的性能，使得在多模态虚假信息检测中能够应对标注数据稀缺的情况。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [95] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 该论文提出了一种将大型语言模型的推理能力蒸馏到更小、更高效模型中的方法，通过结构感知损失优化来提升代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 代码生成需要准确理解提示意图并应用算法推理，而小型语言模型可能缺乏这种推理能力。为了在保持性能的同时降低部署成本，需要将大型模型的推理能力蒸馏到小型模型中。

Method: 通过训练模型模拟大型语言模型的推理和问题解决能力，学习识别正确解决路径，并通过结构感知损失优化建立问题定义与解决方案之间的结构对应关系。

Result: 实验结果显示，经过微调的模型在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过廉价且易于实现的过程开发的微调模型，能够超越令牌级生成，深入理解给定问题的解决方案结构，在保持高效部署的同时显著提升代码生成性能。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [96] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一个低延迟的解码器重排序系统，通过池化首词评分和不确定性门控解释步骤，实现快速排名并在模糊情况下生成解释。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要实时工作并能解释选择依据的排名系统，需要一个低延迟、基于解码器的重排序器。

Method: 采用单解码器方法，结合池化首词评分信号和不确定性门控解释步骤，通过课程学习专注于困难案例。

Result: 在就诊范围订单选择任务中表现优异（快速路径：Recall@1~0.45，nDCG@20~0.625），门控激活时进一步提升（Recall@1~0.56，nDCG@20~0.699，门控率45%）。

Conclusion: OG-Rank提供了一个实用方案：默认快速排名，在需要时解释，这种模式适用于选择性生成能以可接受成本提高准确性的决策任务。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [97] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型作为预测工具的能力，发现LLMs已展现出有前景的预测能力，但也存在事件召回不准确、数据源误解等关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着在互联网规模数据上训练的大型语言模型的快速发展，探索LLMs预测现实世界未来事件的潜力，这一新兴范式被称为"LLM-as-a-Prophet"。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务并将每个任务分解为不同的流程阶段，以支持受控的大规模实验。

Result: 综合评估显示，许多LLMs已展现出令人印象深刻的预测能力，如较小的校准误差、一致的预测置信度和有前景的市场回报。但也发现了关键瓶颈。

Conclusion: LLMs作为预测工具具有潜力，但要实现卓越的预测智能，需要解决事件召回不准确、数据源误解以及在接近决策时信息聚合速度慢于市场等瓶颈问题。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [98] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 该论文提出使用多智能体影响图(MAIDs)作为图形化框架来解决多智能体强化学习(MARL)中的协调问题，特别是设计了针对单个智能体的目标干预范式，通过因果推理技术PSI实现期望结果。


<details>
  <summary>Details</summary>
Motivation: 在大规模MARL中，对整个多智能体系统提供全局指导不切实际，而现有协调机制设计主要依赖经验研究，缺乏易用的研究工具。

Method: 利用MAIDs作为图形框架，分析现有MARL方法，并设计基于MAIDs的目标干预范式，通过Pre-Strategy Intervention(PSI)因果推理技术实现单个智能体的干预。

Result: 实验证明了所提出的目标干预范式的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs提供了一个有效的工具来分析和设计MARL交互范式，目标干预能够缓解全局指导问题，通过因果效应最大化实现复合期望结果。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [99] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出Contextual Attention Modulation (CAM)机制和HyCAM框架，通过动态调制自注意力表示来平衡知识保留与任务特定适应，在多项任务中平均性能提升3.65%。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在多任务适应中的挑战，包括灾难性遗忘和资源消耗问题，现有参数高效方法在复杂多任务场景下表现不佳。

Method: 提出CAM机制动态调制自注意力模块表示，并构建HyCAM框架，结合共享全参数CAM模块和多个轻量级专用CAM模块，采用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的广泛实验显示，该方法显著优于现有方法，平均性能提升3.65%。

Conclusion: CAM和HyCAM框架能有效平衡知识保留与任务特定适应，实现更高效的多任务适应，代码和数据已开源。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [100] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 该论文发现视觉语言模型(VLMs)在输出错误答案时往往已经感知到了正确的视觉证据，这种现象被称为"看见但不相信"。作者提出了一种无需训练、基于注意力掩码的推理时干预方法，能够显著提高多个VLM家族的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态任务上表现优异，但它们仍然会在存在正确视觉证据的情况下失败。本研究旨在系统性地探究这些失败是由于未能感知证据还是未能有效利用证据。

Method: 通过分析层间注意力动态，发现浅层主要关注文本，而深层稀疏但可靠地关注局部证据区域。基于此，提出了一种推理时干预方法，通过选择性注意力掩码来突出深层证据区域。

Result: 该方法无需训练，在多个VLM家族(包括LLaVA、Qwen、Gemma和InternVL)上一致地提高了准确性。

Conclusion: 视觉语言模型内部编码了可靠的证据但未能充分利用，通过使这些信号显式化可以弥合感知与推理之间的差距，推进对VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个针对Lean和mathlib的语义搜索引擎，通过理解数学家的意图来改进定理搜索，相比现有方法实现了30%以上的相对提升。


<details>
  <summary>Details</summary>
Motivation: 形式定理证明的进展常因难以找到相关定理和Lean 4语言学习曲线陡峭而受阻，现有搜索引擎主要依赖非正式化翻译，忽视了与现实用户查询的匹配问题。

Method: 分析并聚类公开Lean讨论的语义，在模拟用户意图的合成查询上微调文本嵌入，通过多样化反馈信号与数学家偏好对齐，从多角度编码其目标意识。

Result: 在真实世界查询、非正式化语句和证明状态上的评估显示，相比之前的搜索引擎和GPT-4o实现了超过30%的相对改进。

Conclusion: Lean Finder是一个用户中心的语义搜索工具，能够与基于LLM的定理证明器兼容，桥接了检索与形式推理。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [102] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: LS-OGD是一个用于多模态学习的概念漂移自适应控制框架，通过动态调整学习率和模态融合权重来应对数据分布变化，确保系统在概念漂移下的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中面临概念漂移的挑战，特别是模态特定的漂移和缺乏连续稳定适应机制的问题，导致性能下降。

Method: 使用在线控制器动态调整模型学习率和不同数据模态之间的融合权重，响应检测到的漂移和预测误差变化。

Result: 在有限漂移条件下，LS-OGD系统的预测误差被证明是最终一致有界的，如果漂移停止则收敛到零；自适应融合策略能有效隔离和减轻模态特定漂移的影响。

Conclusion: LS-OGD为开发可靠且持续自适应的多模态学习系统提供了理论基础，确保系统在面对概念漂移时的弹性和容错能力。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [103] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，通过实时更新奖励分布的后验信念来决定何时停止生成新样本，在保持响应质量的同时显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 多响应采样能提高LLM输出质量，但计算成本高昂。关键挑战是如何平衡准确性和效率，决定何时停止生成新样本。

Method: 基于序列搜索和贝叶斯学习，BEACON顺序生成响应、实时更新奖励分布后验信念，通过权衡预期收益与计算成本来决定停止时机。

Result: 经验验证显示BEACON平均采样量减少高达80%，同时保持响应质量。在成本效益偏好数据生成方面也表现出实用性。

Conclusion: BEACON提供了理论最优性保证和实际可操作性，为未来研究者提供了实用的自适应采样解决方案。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [104] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: PatMD通过识别和主动规避误判风险模式，提升有害表情包的检测能力，在5个检测任务中平均F1分数提升8.30%，准确率提升7.71%。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法（包括MLLM技术）难以处理表情包中通过讽刺、隐喻等修辞手法表达的隐含有害内容，导致频繁误判。

Method: 构建误判风险模式知识库，将每个表情包解构为可能被误判的原因模式，然后检索相关模式动态指导MLLM推理。

Result: 在6,626个表情包的基准测试中，PatMD优于现有最先进基线方法，平均F1分数提升8.30%，准确率提升7.71%。

Conclusion: PatMD通过主动识别和规避误判风险模式，显著提高了有害表情包的检测能力，展现了强大的泛化性和改进的检测性能。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [105] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 基于WaveNet的深度学习模型用于自动分类EEG信号，在209,232个样本上训练，准确率超过传统CNN和LSTM方法，能高精度区分噪声和伪影。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家视觉审查的EEG信号分类方法在处理日益复杂的EEG记录时变得不切实际，需要自动化解决方案。

Method: 使用WaveNet架构，利用扩张因果卷积和残差连接处理EEG数据，采用70/20/10的数据分割进行训练、验证和测试。

Result: 模型分类准确率超过之前的CNN和LSTM方法，能高精度区分噪声和伪影，但在生理和病理信号之间存在可解释的误分类。

Conclusion: WaveNet架构适合EEG数据分析，能有效捕获细粒度和长程时间依赖性，为EEG信号自动分类提供了可行方案。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [106] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 提出基于击键动力学的帕金森病筛查和远程监测新方法，使用深度学习模型在外部验证中取得超过90%的AUC-ROC和70%以上的F1分数。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统临床评估存在局限性，需要非侵入性、可扩展的生物标志物进行远程筛查和监测。

Method: 三阶段流程：数据预处理（处理4个数据集，提取时间信号，解决类别不平衡）；预训练8种深度学习架构；在独立队列上进行外部验证。

Result: 混合卷积-循环和基于Transformer的模型表现优异，时间卷积模型在外部验证中AUC-ROC达到91.14%，优于仅依赖内部验证的现有方法。

Conclusion: 击键动力学可作为帕金森病的可靠数字生物标志物，为早期检测和持续监测提供了有前景的途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [107] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的集成评分滤波器(EnSF)在野火蔓延实时预测数据同化中的应用，该方法通过结合观测数据和数值模型预测，显著提高了野火蔓延预测的准确性、稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着野火破坏性增强和控制成本上升，需要准确实时的火势蔓延预测。数据同化通过整合遥感观测和数值模型预测，对提高活跃野火预测精度至关重要。

Method: 采用基于扩散模型的集成评分滤波器(EnSF)，利用基于评分的生成扩散模型来处理高维非线性滤波问题，特别适用于野火蔓延模型的滤波问题。

Result: 数值研究表明，EnSF在准确性、稳定性和计算效率方面表现优越，为野火数据同化提供了稳健实用的方法。

Conclusion: EnSF是野火数据同化的有效工具，代码已公开，有望改进实时野火蔓延预测能力。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [108] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: 该论文研究了LLMs处理工具返回的复杂JSON响应的能力，发现即使是最先进的模型在处理结构化响应时仍面临困难，不同处理策略的性能差异可达3%到50%。


<details>
  <summary>Details</summary>
Motivation: 现实任务自动化需要LLMs调用工具并处理返回的复杂JSON响应，但LLMs处理结构化响应的能力研究不足，需要系统评估。

Method: 创建了专门的数据集，评估了15个开源和闭源模型，使用多种提示策略来测试JSON处理能力。

Result: JSON处理对所有前沿模型都是困难任务，最佳处理策略取决于工具输出的性质和大小以及所需推理的复杂性。

Conclusion: LLMs处理结构化JSON响应的能力仍有待提升，处理策略的选择对性能有显著影响，需要根据具体任务特性进行优化。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [109] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 该研究通过热解技术将食物类生物质转化为可持续能源，重点关注人工智能在优化过程和建模精度方面的应用，特别研究了咖啡渣和椰枣籽等未充分利用生物质资源的氢生产潜力。


<details>
  <summary>Details</summary>
Motivation: 推动可持续能源和废物管理策略，探索未充分利用生物质资源（如咖啡渣和椰枣籽）的热化学转化潜力，利用人工智能提高过程建模精度和优化效率。

Method: 对纯椰枣籽、咖啡渣及其混合物进行多种分析（工业分析、元素分析、纤维分析、热重分析、动力学分析等），使用等转化率方法（KAS、FWO、Friedman）进行动力学建模，并训练LSTM模型预测热重曲线。

Result: 混合物3显示出最高的氢产率潜力但活化能最高（313.24 kJ/mol），混合物1具有最佳活化能值（161.75 kJ/mol）。KAS方法被确定为最准确的动力学模型，LSTM模型预测热重曲线精度极高（R²: 0.9996-0.9998）。

Conclusion: 人工智能在生物质热解过程建模中表现出卓越性能，未充分利用生物质资源具有可持续氢生产的巨大潜力，不同混合物组合在活化能和氢产率方面呈现不同优势。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [110] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出LAMI框架，通过联合图-语言建模检测青少年非法药物使用，并解释行为风险因素。


<details>
  <summary>Details</summary>
Motivation: 现有方法将调查变量独立处理，忽略了变量间的潜在关联结构，需要更好的建模方法来检测和解释青少年药物使用行为。

Method: LAMI将个体响应表示为关系图，通过图结构学习层学习潜在连接，并集成大语言模型生成基于图结构和调查语义的自然语言解释。

Result: 在YRBS和NSDUH数据集上的实验显示，LAMI在预测准确性上优于竞争基线方法。

Conclusion: LAMI能够揭示有意义的行为子结构和心理社会路径，如家庭动态、同伴影响和学校相关压力，这些与已确立的药物使用风险因素一致。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [111] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA是一个基于曲率信任区域的参数高效微调框架，通过秩调度和稳定性感知优化，在多个7B-13B模型上实现了优于现有PEFT方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在低秩更新、量化或启发式预算分配方面改进效率，但往往将容量分配与训练过程中更新演变解耦，缺乏原则性指导。

Method: CTR-LoRA基于轻量级二阶代理的边际效用分配参数，并使用Fisher/Hessian度量信任区域约束更新，结合秩调度与稳定性感知优化。

Result: 在多个开源骨干模型(7B-13B)上，在分布内和分布外基准测试中均优于强PEFT基线，提高了准确性、训练稳定性，减少了内存需求并实现了更高吞吐量。

Conclusion: CTR-LoRA在性能和效率的帕累托前沿上表现出色，为更鲁棒和可部署的PEFT提供了一条原则性路径。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [112] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: Long Exposure系统通过解决微调中的Shadowy Sparsity问题，加速参数高效微调(PEFT)，实现最高2.49倍的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调(PEFT)技术在时间投入和运营成本方面存在效率低下的问题，而微调过程中特有的Shadowy Sparsity形式尚未得到充分解决以实现加速。

Method: 提出Long Exposure系统，包含三个关键组件：Shadowy-sparsity Exposer使用长感知范围捕获更多稀疏细节；Sequence-oriented Predictor提供高效准确的预测处理大序列输入和动态参数；Dynamic-aware Operator处理动态稀疏操作，实现更结构化的计算模式和合并内存访问。

Result: 广泛评估表明，Long Exposure在端到端微调中实现了最高2.49倍的加速，优于现有最先进技术。

Conclusion: Long Exposure为加速LLMs的PEFT提供了有前景的进展，显著提升了微调效率。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [113] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种名为Deadlock Attack的资源耗尽攻击方法，通过训练恶意对抗嵌入来劫持大型推理模型的生成控制流，诱导模型陷入永久推理循环。


<details>
  <summary>Details</summary>
Motivation: 现代大型推理模型通过思维链推理展示出强大的多步问题解决能力，但这种迭代思维机制引入了新的安全漏洞面。

Method: 采用优化的对抗嵌入来鼓励推理步骤后的过渡性标记（如"Wait"、"But"），防止模型得出结论。引入后门植入策略来克服连续到离散的投影差距问题。

Result: 在四种先进LRM和三个数学推理基准测试中实现了100%的攻击成功率，迫使模型生成达到最大标记限制。攻击具有隐蔽性且对现有缓解策略具有鲁棒性。

Conclusion: 研究揭示了大型推理模型在推理效率方面存在关键且未被充分探索的安全漏洞。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [114] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出了一种细粒度的联邦域自适应方法Gains，用于解决开放世界中新客户端不断加入联邦学习的问题，通过知识发现和知识适应来整合新知识，同时保持源域性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中联邦学习面临新客户端持续加入的挑战，现有方法在知识发现粒度较粗，且会牺牲源域性能和适应效率。需要更精细的方法来检测新知识并有效整合。

Method: 将模型分为编码器和分类器，利用编码器对域偏移敏感、分类器对类别增量敏感的特性，开发细粒度知识发现和贡献驱动聚合技术，并设计抗遗忘机制保护源域性能。

Result: 在三个典型数据偏移场景的多域数据集上实验表明，Gains在源域和目标域客户端性能上均显著优于其他基线方法。

Conclusion: Gains方法通过细粒度的知识发现和适应机制，有效解决了开放世界联邦学习中的新知识整合问题，实现了源域和目标域性能的平衡提升。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [115] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出SAU-FNO框架，结合自注意力机制和U-Net与FNO，用于3D IC热管理，实现842倍加速和更高精度


<details>
  <summary>Details</summary>
Motivation: 3D IC中热管理因功率密度增加而日益困难，传统PDE方法准确但太慢，机器学习方法如FNO虽快但存在高频信息丢失和高保真数据依赖问题

Method: 引入自注意力U-Net傅里叶神经算子(SAU-FNO)，结合自注意力和U-Net捕捉长程依赖和局部高频特征，采用迁移学习微调低保真数据

Result: SAU-FNO实现最先进的热预测精度，相比传统FEM方法提供842倍加速

Conclusion: SAU-FNO是先进3D IC热仿真的高效工具，减少对高保真数据集的依赖并加速训练

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [116] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM是一个基于代理的框架，利用大语言模型将非线性优化问题自动转化为线性优化问题，使非线性问题能够使用线性求解器求解。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的线性化通常需要人工操作和专业知识，这限制了非线性问题的求解效率和应用范围。

Method: 为每种非线性模式分配专门的reformulation代理，代理根据指令推导出精确的线性化方案，然后协调组装成可求解的线性模型。

Result: 在从ComplexOR数据集衍生的20个真实世界非线性优化问题上进行测试，结果表明专门的LLM代理能够自动化线性化任务。

Conclusion: 该方法为非线性优化问题提供了完全对话式建模管道的可能性，推动了非线性优化问题的自动化求解。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [117] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 本文提出训练数据的几何结构对模型性能有重要影响，使用持久同调分析数据拓扑特征来量化多样性。


<details>
  <summary>Details</summary>
Motivation: 虽然已知训练数据的类型对机器学习很重要，但数据的几何结构对模型性能的影响尚未充分探索。作者认为数据的表示丰富性和冗余消除对学习结果有重要影响。

Method: 使用持久同调方法从度量空间中的数据提取拓扑特征，提供了一种超越基于熵的多样性量化原则方法。

Result: 研究发现持久同调是分析和增强驱动AI系统的训练数据的强大工具。

Conclusion: 持久同调为理解训练数据的几何结构影响提供了新的分析框架，有助于改进AI系统的训练数据质量。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [118] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: 提出了PALE框架，通过提示引导的数据增强和对比马氏距离评分来检测大语言模型的幻觉，无需人工标注即可实现高性能幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会产生误导性或虚构信息（幻觉），但缺乏标注良好的幻觉检测数据集，需要低成本的数据增强方法。

Method: 使用提示引导从LLMs生成真实和幻觉数据作为数据增强，提出对比马氏距离评分来评估中间嵌入向量的真实性，采用矩阵分解方法捕捉分布结构。

Result: PALE在幻觉检测性能上显著优于竞争基线方法，提升了6.55%。

Conclusion: PALE框架无需人工标注，具有良好的泛化性和实用性，能有效检测LLM生成的幻觉内容。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [119] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 提出了DAWP框架，通过人工智能数据同化(AIDA)模块将AI天气预测从再分析数据中解放出来，直接在观测空间进行天气预报。


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预测依赖再分析数据，存在数据同化偏差和时间不一致性问题，需要开发能在观测空间直接工作的新方法。

Method: 使用掩码多模态自编码器(MMAE)处理不规则卫星观测数据，结合时空解耦Transformer和跨区域边界条件(CBC)进行全球观测预测。

Result: AIDA初始化显著提高了AIWP的展开效率和性能，DAWP在全局降水预测中展现出应用潜力。

Conclusion: DAWP框架成功实现了从再分析数据依赖到观测空间预测的转变，为AI天气预测提供了新的解决方案。

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [120] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出Cog-Rethinker，一种分层元认知强化学习框架，通过分解零准确率问题和参考错误答案进行精炼，提高LLM推理任务中的样本利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定提示模板激活LLM的推理能力，但对弱LLM存在采样效率低的问题，大部分问题在推理任务中产生无效输出，造成样本浪费。

Method: 采用分层元认知两阶段框架：1) 将零准确率问题分解为子问题；2) 参考先前错误答案精炼答案。使用监督微调确保训练测试一致性。

Result: 在多个数学推理基准测试中表现优异，相比基线方法提高了样本效率并加速收敛。

Conclusion: Cog-Rethinker通过元认知方法有效解决了LLM推理训练中的样本效率问题，在数学推理任务中取得了显著改进。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [121] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出了一种名为AMiD的统一知识蒸馏框架，通过引入α-混合辅助分布来解决大型语言模型蒸馏中的容量差距和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型虽然性能优异但计算和内存成本高昂。知识蒸馏通过将大模型知识转移到小模型来缓解这一问题，但由于LLM高维输出导致的近零概率问题，容量差距和训练不稳定性仍然是基本限制。

Method: 提出了α-混合辅助分布，这是一个通过引入分布设计变量α来连续扩展辅助分布的新颖通用家族，以及基于该分布的AMiD统一蒸馏框架。AMiD基于最优性推广了与辅助分布一起使用的散度家族。

Result: 通过大量实验证明，AMiD通过利用更广泛且理论基础的辅助分布空间，提供了优越的性能和训练稳定性。

Conclusion: AMiD框架通过系统化的辅助分布设计，有效解决了LLM知识蒸馏中的关键挑战，为高效的模型压缩提供了新的解决方案。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [122] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出了MEET-Sepsis框架，通过多内源视图表示增强机制和级联双卷积时间序列注意力模块，仅需ICU监测时间的20%即可实现竞争性的败血症预测准确率。


<details>
  <summary>Details</summary>
Motivation: 败血症是ICU中死亡率高的危及生命的感染综合征，早期准确预测对及时干预至关重要。现有AI方法难以捕捉微弱的早期时间信号。

Method: 使用多内源视图表示增强(MERE)机制构建丰富特征视图，结合级联双卷积时间序列注意力(CDTA)模块进行多尺度时间表示学习。

Result: MEET-Sepsis框架仅需SOTA方法20%的ICU监测时间即可达到竞争性的预测准确率。

Conclusion: 该框架显著推进了早期败血症预测，广泛验证证实了其有效性。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [123] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的可解释AI方法，用于根据睡眠障碍特征对患者进行分组，并识别影响这些疾病的关键因素。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍对患者健康和生活质量有重大影响，但由于症状多样性，诊断仍然复杂。技术进步和医疗数据分析为更好理解这些疾病提供了新视角。

Method: 采用基于聚类的可解释人工智能方法，对患者进行睡眠障碍特征分组，并整合可解释性方法来识别关键影响因素。

Result: 在匿名真实数据上的实验证明了该方法的有效性和相关性。

Conclusion: 该方法能够有效识别睡眠障碍患者的不同特征分组，并通过可解释性分析揭示影响疾病的关键因素。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [124] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 该论文提出了一个追踪和引导大语言模型中算法原语的框架，通过将推理轨迹与内部激活模式关联，评估算法原语对推理步骤和任务性能的影响。研究发现LLMs的推理由可组合的算法原语支持，这些原语可以跨任务和跨模型转移，推理微调能增强跨领域的算法泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何通过潜在计算和推理时间计算来解决多步推理问题，探索模型推理背后的算法原语及其组合方式。

Method: 通过聚类神经激活并标记匹配的推理轨迹来操作化算法原语，使用函数向量方法推导可重用的推理构建块，在四个基准测试（TSP、3SAT、AIME、图导航）上评估原语向量对推理步骤和任务性能的影响。

Result: 发现原语向量可以通过加法、减法、标量运算进行组合，揭示了激活空间中的几何逻辑。跨任务和跨模型评估显示存在共享和任务特定的原语。推理微调后的模型表现出更系统化的验证和路径生成原语使用。

Conclusion: LLMs的推理可能由算法原语的组合几何支持，原语可以跨任务和跨模型转移，推理微调能增强跨领域的算法泛化能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [125] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: GRPO算法作为RLVR的核心方法，虽然被广泛用于提升LLM的推理能力，但其效果存在不一致性。研究发现GRPO本质上是一种保守的重新加权方案，受限于基础模型的分布，无法发现全新解决方案。


<details>
  <summary>Details</summary>
Motivation: 探究GRPO在什么条件下能改善推理能力并实现分布外泛化，解释其效果不一致的原因。

Method: 从数据分布角度进行理论分析，证明GRPO的保守性，并通过从零开始训练transformer进行受控实验，评估在推理深度、输入长度、token表示和组合性等方面的泛化能力。

Result: GRPO的分布外改进仅在目标任务与模型预训练偏差一致时出现，而分布内任务的收益随着性能饱和而减少。

Conclusion: GRPO并非通用推理增强器，而是强化预训练偏差的工具，未来需要开发能超越预训练局限的算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [126] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos是一个端到端的LLM蒸馏流水线，能在分布式云环境中自动选择服务器和模型、进行知识蒸馏和部署，满足用户定义的性能和预算约束。


<details>
  <summary>Details</summary>
Motivation: 工业界对定制化、成本效益高的大型语言模型需求增长，现有蒸馏框架需要人工干预且难以满足复杂需求。

Method: 自动选择帕累托最优服务器，动态匹配师生模型对，根据任务复杂度调整蒸馏策略以优化云托管。

Result: 在罕见的麻将推理任务上，学生模型准确率比GPT-4o教师基准提高了四倍，同时降低了延迟和成本而不影响准确率。

Conclusion: Stratos展示了在垂直领域LLM部署中的潜力，能够有效满足性能和预算约束。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [127] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 提出使用Kolmogorov-Smirnov检验来监测和量化测试数据与训练数据之间的分布偏移，并证明KS距离可作为评估AI智能体性能下降的重要统计工具。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统中测试数据分布与训练数据分布偏离的问题，这种分布偏移会导致AI系统预测出现大误差，在安全关键应用中尤为严重。

Method: 使用Kolmogorov-Smirnov检验来测量分布偏移，通过KS距离量化分布偏移程度及其对AI智能体性能的影响。

Result: 研究表明即使KS距离仅为0.02，也会导致强化学习智能体在单交叉路口的旅行时间增加约50%，影响显著。

Conclusion: KS检验和KS距离可作为实时监测AI智能体性能下降的重要工具，帮助AI系统更有效地应对分布偏移问题。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [128] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 本文分析了使用ANaGRAM自然梯度方法训练PINNs的训练动态，提出多截止自适应策略提升性能，并在基准PDE上验证了有效性，可达机器精度。


<details>
  <summary>Details</summary>
Motivation: 最近研究表明自然梯度方法在训练物理信息神经网络(PINNs)时显著优于标准优化器，但需要深入分析其训练动态并改进性能。

Method: 使用ANaGRAM自然梯度方法，结合奇异值分解和截止正则化，并提出多截止自适应策略。

Result: 在基准PDE上的实验验证了方法的有效性，某些实验可达机器精度。

Conclusion: 建立了基于谱理论的理论框架，解释了正则化的必要性，并扩展了与格林函数理论的联系。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [129] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出了一种层感知的在线估计器，用于高效评估训练样本在优化过程中的动态影响，避免了传统静态影响评估的局限性，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统的数据影响评估方法主要关注在收敛模型上的静态影响，忽略了样本影响在优化过程中的动态变化特性，特别是在深度模型中。现有方法计算负担重，难以实现频繁的影响估计。

Method: 开发了一种层感知的在线估计器，仅需要损失对输出的梯度，避免了参数级和全网络梯度的计算，同时保持了排序保真度。

Result: 在LLM预训练、微调和图像分类等任务上的广泛实验表明，该方法在显著降低时间和内存成本的同时提高了准确性，使动态数据筛选在实践中高效且可扩展。

Conclusion: 该方法成功解决了动态数据影响评估的计算效率问题，为数据中心的动态数据筛选提供了实用的解决方案。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [130] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 提出STAR模块，增强时间序列基础模型对状态变量的建模能力，解决现有方法忽视状态变量分类特性导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中时间序列包含数值变量和离散状态变量，现有时间序列基础模型忽视状态变量的分类特性，将其与数值变量统一处理，导致无法充分利用状态信息甚至性能下降。

Method: STAR包含三个核心组件：身份引导状态编码器学习状态变量的分类语义；条件瓶颈适配器基于当前状态动态生成低秩适配参数；数值-状态匹配模块检测状态变量本身的异常。

Result: 在真实数据集上的广泛实验表明，STAR能够提升现有时间序列基础模型在多变量时间序列异常检测中的性能。

Conclusion: STAR是一个即插即用模块，能够有效增强时间序列基础模型对状态变量的建模和利用能力，在微调阶段提升异常检测性能。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [131] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 提出了一种决策导向的洪水管理框架，通过端到端优化传感器部署和洪水预测模型来最小化下游决策遗憾，而非传统的任务无关方法。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统采用固定、任务无关的策略部署传感器和训练预测模型，忽视了相同感知增益和平均预测误差可能导致不同决策结果的问题。

Method: 端到端框架包含四个组件：上下文评分网络、预算约束下的可微分传感器选择模块、时空洪水重建与预测模型、以及针对任务目标的可微分决策层，采用隐式最大似然估计实现离散传感器配置的梯度学习。

Result: 该方法能够战略性地选择传感器位置并优化洪水预测模型，直接针对下游洪水响应决策进行优化。

Conclusion: 决策导向框架能够更有效地支持洪水应急响应，通过端到端优化传感器部署和预测模型来最小化决策遗憾。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [132] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 该研究探索了使用渐进式神经网络(PNNs)和传统微调策略来加速混沌流体流动多保真度控制的深度强化学习。PNNs首次在基于DRL的流动控制中应用，能稳定高效地转移知识，而微调策略对预训练时长敏感且易发生灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发有效的迁移学习策略来加速混沌流体流动控制的深度强化学习训练过程，解决传统方法在从低保真环境向高保真环境转移知识时面临的挑战。

Method: 采用渐进式神经网络(PNNs)架构和传统微调策略进行对比研究，使用Kuramoto-Sivashinsky系统作为基准，分析从低保真环境训练的控制策略向高保真环境的知识转移效果。

Result: 系统评估显示：微调能加速收敛但对预训练时长敏感且易发生灾难性遗忘；PNNs通过保留先验知识实现稳定高效的知识转移，性能提升一致，对预训练阶段的过拟合具有鲁棒性。

Conclusion: 渐进式神经网络在流动控制中展现出比传统微调策略更优越的知识转移能力，特别是在源环境和目标环境差异较大时，为复杂流动配置的鲁棒、可扩展和计算高效的流动控制提供了有前景的解决方案。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [133] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: AirDbM是一种专门用于翼型优化的设计变形方法，通过从1600多个翼型数据库中选择12个最优基线翼型，显著降低设计空间维度，在保持高重构精度的同时实现更高效的多目标气动优化。


<details>
  <summary>Details</summary>
Motivation: 翼型几何优化需要探索多样化的设计，同时尽可能减少设计变量数量。传统方法需要较多基线翼型，而AirDbM旨在通过系统化降低设计空间维度来提高优化效率。

Method: 从UIUC翼型数据库中选择12个最优基线翼型，通过顺序添加最能增加设计容量的基线。使用这些基线重构99%的数据库，平均绝对误差低于0.005。

Result: AirDbM在保持与先前使用更多基线方法相同性能的同时，在多目标气动优化中表现出快速收敛性，获得更大超体积的Pareto前沿，并发现了具有改进升阻比的新Pareto最优解。

Conclusion: AirDbM在强化学习代理生成翼型几何方面表现出卓越的适应性，表明设计变形方法在机器学习驱动设计中具有更广泛的潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [134] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出基于特征驱动的强化学习方法，用于光伏发电商在日内连续市场的实时交易决策，通过PPO算法学习平衡交易利润和失衡惩罚的投标策略。


<details>
  <summary>Details</summary>
Motivation: 光伏运营商面临发电不确定性和短期电价波动的挑战，日内连续市场提供了实时调整头寸的机会，可以改善收益并减少失衡成本。

Method: 将问题建模为马尔可夫决策过程，使用近端策略优化算法学习可解释的线性策略，整合数据驱动特征到状态空间中。

Result: 在历史市场数据上训练并在样本外评估，该策略在不同场景下持续优于基准方法，表现出快速收敛、实时推理和透明决策规则。

Conclusion: 特征驱动的强化学习为光伏发电商提供了实用、数据高效且可操作部署的日内主动参与路径。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [135] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 提出IB-FT方法解决LLM代码生成中的记忆障碍问题，通过信息瓶颈压缩记忆特征，提升泛化能力


<details>
  <summary>Details</summary>
Motivation: 发现预训练大语言模型在代码领域微调时存在记忆障碍问题，即模型过度记忆下游代码数据，阻碍新知识的有效获取和泛化

Method: 提出基于信息瓶颈的微调方法(IB-FT)，对代码数据的隐藏表示施加IB惩罚，压缩虚假记忆特征，同时保留任务相关信息

Result: 在两个代码基准测试(OriGen和Evol-CodeAlpaca-V1)上，IB-FT显著缓解记忆障碍，提升top-1性能，在更严格的多样本指标Pass@$k^{(m)}$下获得更稳定的增益

Conclusion: IB-FT能有效克服代码生成中的记忆障碍，比传统微调方法在泛化性和稳定性方面表现更好

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [136] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: PolyConFM是首个聚合物基础模型，通过构象中心的生成预训练统一聚合物建模和设计，在多种下游任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅使用单体级描述符表示聚合物，忽略了聚合物构象中的全局结构信息，限制了实际性能，且缺乏支持多样化下游任务的通用基础模型。

Method: 将聚合物构象分解为局部构象序列，通过掩码自回归建模重建局部构象，并生成其方向变换来恢复聚合物构象；通过分子动力学模拟构建高质量聚合物构象数据集。

Result: 实验表明PolyConFM在多样化下游任务中持续优于代表性任务特定方法。

Conclusion: PolyConFM为聚合物科学提供了一个通用且强大的工具。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [137] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 开发了一个可推广的因果机器学习流程，用于从电子健康记录中发现潜在因果源并量化其对临床结果的影响。


<details>
  <summary>Details</summary>
Motivation: 处理不完善的多模态临床数据，发现其中的潜在因果因素，并量化这些因素对临床结果的具体影响。

Method: 使用概率独立潜在源分解多模态临床数据，训练任务特定的因果模型来估计个体因果效应。

Result: 已在两个真实世界应用中验证了该方法的有效性和实用性，展示了其在医学发现中的可扩展性。

Conclusion: 该方法提供了一个通用且可扩展的框架，能够从大规模电子健康记录中发现因果关系并量化其影响。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [138] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了RoBCtrl框架，这是首个针对GNN社交机器人检测器的对抗性多智能体强化学习攻击方法，通过扩散模型生成高保真机器人账户，并使用MARL模拟对抗行为。


<details>
  <summary>Details</summary>
Motivation: 现有GNN检测方法存在对社交代理控制有限、检测器黑盒特性以及机器人异质性等问题，导致这些检测方法的脆弱性和鲁棒性研究不足。

Method: 使用扩散模型重构现有账户数据生成高保真机器人账户，采用多智能体强化学习模拟对抗行为，基于影响力和预算对账户分类，并通过结构熵的层次状态抽象加速强化学习。

Result: 在社交机器人检测数据集上的大量实验表明，该框架能有效削弱GNN检测器的性能。

Conclusion: 这是扩散模型在模拟演化社交机器人行为方面的首次应用，提出的RoBCtrl框架为评估社交机器人检测器的鲁棒性提供了有效工具。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [139] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 首次对时变学习策略下的Q-learning算法进行有限时间分析，在最小假设下建立了最终迭代收敛速率，匹配了离策略Q-learning的样本复杂度但探索参数依赖更差。


<details>
  <summary>Details</summary>
Motivation: 分析时变学习策略（即同策略采样）下Q-learning的收敛性能，填补了该领域在有限时间分析方面的空白，特别是在最小探索假设下的理论分析。

Method: 采用改进方法，利用泊松方程将马尔可夫噪声分解为鞅差项和残差项，并对泊松方程解相对于Q函数估计和学习策略进行敏感性分析。

Result: 建立了𝔼[‖Q_k - Q^*‖_∞^2]的最终迭代收敛速率，达到𝔼[‖Q_k - Q^*‖_∞] ≤ ε的样本复杂度为O(1/ε^2)，同时推导了𝔼[‖Q^π_k - Q^*‖_∞^2]的显式速率。

Conclusion: 同策略Q-learning相比离策略版本探索能力较弱但具有利用优势，其策略会收敛到最优策略而非保持固定。所开发的分析工具对分析具有快速时变学习策略的一般强化学习算法具有独立价值。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [140] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: 提出Grid-like Code Quantization (GCQ)，一种受大脑启发的压缩方法，使用吸引子动力学中的网格状模式将观察-动作序列压缩为离散表示。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法处理静态输入，而GCQ通过动作条件化码本进行时空压缩，码字来自连续吸引子神经网络并根据动作动态选择，实现空间和时间的联合压缩。

Method: GCQ使用动作条件化码本，其中码字源自连续吸引子神经网络，基于动作动态选择，实现观察-动作序列的时空压缩。

Result: 实验表明GCQ在紧凑编码和下游任务性能方面有效，支持长时程预测、目标导向规划和逆向建模。

Conclusion: GCQ既为高效序列建模提供了计算工具，也为神经系统中网格状代码形成提供了理论视角。

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [141] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 该论文研究了一个非平稳多臂老虎机问题，其中奖励取决于动作和潜在状态，状态由未知线性动力学控制。作者提出了一个探索-承诺算法，通过随机探索估计系统参数，然后优化动作序列以获得长期奖励，实现了$\tilde{\mathcal{O}}(T^{2/3})$的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 研究非平稳环境中的多臂老虎机问题，其中奖励不仅取决于当前动作，还受潜在状态影响，且状态动力学依赖于动作，这导致了短期奖励与长期奖励之间的权衡问题。

Method: 提出探索-承诺算法：在探索阶段使用随机Rademacher动作估计线性动力学的马尔可夫参数；在承诺阶段使用估计参数设计优化的动作序列以获得长期奖励。

Result: 算法实现了$\tilde{\mathcal{O}}(T^{2/3})$的遗憾上界。解决了从时间相关奖励中学习和设计具有最优长期奖励的动作序列两个关键挑战。

Conclusion: 该工作为非平稳多臂老虎机问题提供了有效的解决方案，通过系统识别和动作序列优化实现了次线性遗憾，并提出使用半定松弛和Goemans-Williamson舍入作为实用方法。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [142] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant是一种创新的浮点量化方法，通过引入非整数位宽和两种新技术来优化大语言模型的推理效率，在保持精度的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的巨大参数量带来了存储和推理效率瓶颈，浮点量化虽然能缓解这些问题，但传统的整数位宽限制了进一步优化空间。

Method: 提出两种新技术：1) 尾数位共享 - 将k个量化权重分组共享最低有效尾数位；2) 自适应搜索 - 使用离线优化策略最小化共享带来的精度损失。

Result: 能够将模型量化为FP-5.33-e2m3和FP4.25-e2m2，相比FP16推理分别实现2.8倍和3.2倍的解码加速，且精度损失可忽略。

Conclusion: AMS-Quant通过非整数位宽量化方法有效解决了大语言模型的推理效率问题，在保持模型精度的同时显著提升了推理速度。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [143] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出了一种稀疏transformer架构，将数据分布的先验信息直接融入神经网络结构，基于正则化Wasserstein近端算子设计，相比传统流模型改善了优化问题的凸性并促进生成样本的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 将数据分布的先验信息直接整合到transformer架构中，以改善生成模型的性能，特别是通过最优传输理论来增强模型的数学基础。

Method: 基于正则化Wasserstein近端算子的稀疏transformer架构，该算子具有闭式解，可表示为特殊的transformer结构。

Result: 在生成建模和贝叶斯逆问题应用中，稀疏transformer相比传统基于神经ODE的方法实现了更高的精度和更快的收敛速度。

Conclusion: 所提出的稀疏transformer架构在理论和实验上都优于传统方法，为生成模型提供了更有效的解决方案。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [144] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化框架，通过原生可访问性API系统探索应用程序，解决了GUI自动化中的数据收集挑战，并发布了包含27,171个任务的GUIrilla-Task数据集。


<details>
  <summary>Details</summary>
Motivation: 解决复杂图形用户界面自动化中的数据可用性限制问题，包括昂贵的手动标注、闭源数据集和表面级合成流程。

Method: 使用原生可访问性API系统探索应用程序，组织界面元素和爬虫动作为层次化GUI图，采用专门交互处理器实现全面应用覆盖。

Result: 构建了包含27,171个功能基础任务的GUIrilla-Task数据集，在ScreenSpot Pro基准测试中显著优于合成基线，使用数据量减少97%。

Conclusion: GUIrilla框架有效解决了桌面GUI自动化的数据收集挑战，发布的工具和数据集支持桌面自主性的开放研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [145] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 提出基于LSTM的EV充电负荷预测框架，通过数据预处理和特征提取来预测多时间尺度的充电需求


<details>
  <summary>Details</summary>
Motivation: 为电动汽车充电设施的基础设施规划、能源管理和电网集成提供准确的充电需求预测

Method: 使用LSTM循环神经网络，对原始充电数据进行插值和归一化预处理，提取特征后训练模型

Result: 模型能够准确预测日、周、月等不同时间尺度的充电需求，适用于不同充电地点的多样化使用模式

Conclusion: 该模块化框架具有良好的适应性，可为EV充电设施的部署和电网管理提供有价值的数据支持

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [146] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 该论文探讨了智能交通系统中的交通预测技术，重点分析了图神经网络在捕捉空间依赖性和时间演化模式方面的应用，并指出了现有方法在处理事件信息时的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，交通拥堵问题日益严重，需要可靠且响应迅速的交通预测模型来改善交通资源分配和出行体验。

Method: 主要采用图神经网络（GNNs）作为主流技术范式，包括STGCN、GraphWaveNet、STWave和D2STGNN等模型，这些方法结合了复杂的图卷积结构和时间建模机制。

Result: 现有方法在标准交通数据集上取得了令人印象深刻的性能，特别擅长捕捉和预测具有周期性规律的交通模式。

Conclusion: 虽然现有方法在常规交通预测中表现良好，但在处理事件信息时仍存在局限性，主要依赖于人工设计的特征，难以泛化到多样复杂的未知事件，且容易丢失丰富的语义细节。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [147] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: 提出了MuonBP优化器，通过块周期正交化减少梯度正交化在模型并行中的通信开销，在保持训练稳定性的同时提升吞吐量


<details>
  <summary>Details</summary>
Motivation: Muon优化器结合梯度正交化和动量，在语言模型训练中比Adam/AdamW更高效，但在模型并行中由于额外的梯度收集和分散操作导致5%-10%的吞吐量损失

Method: MuonBP在每个设备上独立对矩阵分片应用正交化，并定期执行完全正交化以保持训练稳定性，使用两个学习率分别处理块正交化和完全正交化步骤

Result: 在8B模型训练中，使用8路张量并行和ZeRO优化器状态分片，MuonBP相比Muon实现了8%的吞吐量提升且性能无下降

Conclusion: MuonBP方法简单，需要最少的超参数调整，在保持与基线Muon竞争性迭代复杂度的同时，提供与AdamW等坐标方法相当的每轮迭代吞吐量

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [148] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 本文系统评估了5个时间序列基础模型和2个基准模型的校准特性，发现时间序列基础模型比基准模型校准得更好，且不会系统性地过度自信或不足自信。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测性能上达到最先进水平，但其校准特性相对未被充分探索，而校准对许多实际应用至关重要。

Method: 对5个时间序列基础模型和2个竞争性基准模型进行系统评估，包括模型校准、不同预测头的影响以及长期自回归预测下的校准。

Result: 时间序列基础模型比基准模型校准得更好，且不会系统性地过度自信或不足自信，这与深度学习模型中常见的过度自信形成对比。

Conclusion: 时间序列基础模型具有良好的校准特性，这为实际应用提供了重要优势。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [149] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种连续时间强化学习方法CQSM，通过鞅条件定义连续时间Q函数，将扩散策略得分与Q函数的动作梯度联系起来，解决了传统离散时间RL方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法大多基于离散时间，而现实世界中的控制问题往往是连续时间的。传统值函数方法在连续时间设置中难以保持Q函数的动作评估能力，需要新的理论框架。

Method: 通过鞅条件表征连续时间Q函数，利用动态规划原理将扩散策略得分与学习到的连续Q函数的动作梯度联系起来，提出连续Q分数匹配(CQSM)算法。

Result: 在线性二次控制问题中提供了理论闭式解，在模拟环境中验证了方法的有效性，并与主流基线方法进行了比较。

Conclusion: CQSM方法成功解决了连续时间RL中长期存在的挑战，在不依赖时间离散化的情况下保持了Q函数的动作评估能力，为连续时间控制问题提供了新的解决方案。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [150] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 提出一种用于变电站级电压估计的分层图神经网络，利用电气拓扑和物理特征，在低观测性条件下保持鲁棒性，在SMART-DS数据集上验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透和配电级电压波动增加，传统配电系统状态估计方法难以应对稀疏测量和大规模馈线网络，需要更可扩展的解决方案。

Method: 使用分层图神经网络，结合电气拓扑和物理特征，在SMART-DS数据集上进行训练和评估，涵盖多个变电站和不同DER渗透场景。

Result: 相比其他数据驱动模型，RMSE降低高达2倍，在仅1%测量覆盖率下仍保持高精度。

Conclusion: 图神经网络有望为配电系统提供可扩展、可复现和数据驱动的电压监测方案。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [151] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 动量方法使随机DC优化在任意批次大小下都能收敛，无需大批次或强噪声假设。


<details>
  <summary>Details</summary>
Motivation: 现有随机DC优化方法通常需要大批次或强噪声假设，限制了实际应用。动量方法能在标准平滑性和有界方差假设下实现收敛。

Method: 提出基于动量的算法，证明无动量时无论步长如何都可能无法收敛。

Result: 动量方法在理论上可证明收敛，并在实验中表现出强大的性能。

Conclusion: 动量是随机DC优化在小批次设置下实现收敛的关键要素。

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [152] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出一种基于残差学习的AC最优潮流求解方法，利用DC OPF作为基线，通过图神经网络学习非线性修正，实现快速准确的AC OPF求解。


<details>
  <summary>Details</summary>
Motivation: 解决AC最优潮流问题在实时电网运行中的计算瓶颈，传统AC OPF求解器计算成本高，难以满足实时决策需求。

Method: 使用拓扑感知的图神经网络，结合局部注意力和两级DC特征集成，通过物理信息损失函数训练，确保AC潮流可行性和运行限制。

Result: 在57、118和2000总线系统上测试，MSE降低约25%，可行性误差减少达3倍，运行速度提升达13倍，在N-1故障下保持准确性。

Conclusion: 残差学习是连接线性近似和AC可行OPF的实用可扩展桥梁，能够实现近实时的运行决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [153] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 该论文分析了过参数化最小二乘问题中梯度下降在大学习率下的收敛行为，揭示了在边缘稳定性区域的三种收敛机制：亚临界、临界和超临界状态。


<details>
  <summary>Details</summary>
Motivation: 传统优化理论只保证小步长梯度下降的单调收敛，但神经网络训练常使用大学习率（边缘稳定性区域），此时目标函数非单调下降且偏好平坦极小值。本文旨在量化这种现象。

Method: 利用过参数化使全局极小值形成黎曼流形，将梯度下降动态分解为平行和正交于流形的分量，分别对应黎曼梯度下降和分岔动力系统。

Result: 识别了三种收敛状态：亚临界状态（有限时间内克服瞬时不稳定性后线性收敛到次优平坦极小值）、临界状态（不稳定性持续存在但以幂律收敛到最优平坦极小值）、超临界状态（不稳定性持续存在但线性收敛到周期为2的轨道）。

Conclusion: 过参数化最小二乘问题中，大学习率梯度下降的收敛行为可通过流形分解和分岔理论精确描述，为理解神经网络训练中的边缘稳定性现象提供了理论框架。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [154] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出了FedPURIN框架，通过整数编程识别关键参数进行传输，结合稀疏聚合方案显著减少通信开销，同时保持个性化联邦学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中通信效率低下的问题，现有方法在数据异构情况下通信负担重，阻碍实际部署。

Method: 使用整数编程策略识别关键参数，结合稀疏聚合方案，在保持模型性能的同时大幅减少通信量。

Result: 在标准图像分类基准测试中，在不同非IID条件下表现出与最先进方法相当的性能，并通过稀疏聚合实现了可量化的通信减少。

Conclusion: FedPURIN为通信高效的个性化联邦学习建立了新范式，特别适用于具有异构数据源的边缘智能系统。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [155] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于GaLore和Muon算法的无偏低秩优化方法GUM，通过层采样技术消除低秩投影的偏差，在保持内存效率的同时实现收敛保证，并在LLM微调和预训练中表现优于GaLore和全参数训练。


<details>
  <summary>Details</summary>
Motivation: 现有梯度低秩投影方法（如GaLore）缺乏收敛保证，其低秩投影机制会引入偏差，导致性能与全参数训练存在差距。

Method: 结合GaLore机制和Muon算法，采用层采样技术来消除低秩投影的偏差，提出GUM方法。

Result: 理论证明GUM与基础Muon算法具有相同的收敛保证，同时保持低秩技术的内存效率。实证实验显示在LLM微调和预训练中优于GaLore，甚至超过全参数训练。

Conclusion: GUM通过更均匀的层内知识分布，实现了模型参数空间的更高效利用和更好的记忆能力，解决了低秩优化方法的偏差问题。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


### [156] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出了多尺度神经算子(MNO)，一种用于三维非结构化点云上计算流体动力学的新架构，通过显式三尺度分解显著提升了神经算子在复杂流体问题上的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在求解偏微分方程时存在精度和可扩展性限制，特别是在不规则域上处理多尺度流体结构时表现不佳。

Method: MNO架构包含三个模块：全局维度收缩注意力模块处理长程依赖，局部图注意力模块处理邻域交互，微观点级注意力模块处理精细细节。

Result: 在四个不同基准测试中，MNO始终优于最先进基线，预测误差降低5%-40%，在具有30万点的3D CFD问题上表现出更好的鲁棒性。

Conclusion: 显式多尺度设计对神经算子至关重要，MNO为在不规则域上学习复杂流体动力学提供了可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [157] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 提出了基于随机矩阵理论的Transformer训练动态分析框架，通过自注意力矩阵的谱密度演化来识别训练阶段，并开发了无需验证集的早停标准。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer训练动态的底层机制，为性能改进提供理论依据，并建立原则性的早停标准。

Method: 利用随机矩阵理论分析浅层自注意力矩阵V的谱密度演化，使用幂律拟合作为探针，将训练划分为三个阶段。

Result: 发现自注意力矩阵谱密度一致演化为重尾分布，提出了两个一致的无需验证标准：重尾动态定量度量和收敛谱特征。

Conclusion: 随机矩阵理论在监控和诊断Transformer模型训练进展方面具有实用性，提出的标准与训练动态高度一致。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [158] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出了一种基于ADAROUND的QUBO公式的后训练量化方法，通过Frobenius距离作为目标函数，将量化问题转化为可分解的QUBO问题，并使用模拟退火等启发式方法高效求解。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法如四舍五入到最近邻存在精度损失，需要更精确的量化方法来保持神经网络性能。

Method: 使用Frobenius距离作为目标函数，构建明确的QUBO问题，其中二元变量表示权重和偏置的舍入选择。利用QUBO矩阵结构将全局问题分解为n个独立的子问题。

Result: 在MNIST、Fashion-MNIST、EMNIST和CIFAR-10数据集上评估，从int8到int1的整数精度范围内，与传统四舍五入量化方法进行比较。

Conclusion: 该方法能够有效进行后训练量化，通过QUBO分解和启发式求解在多种数据集和精度级别上实现良好性能。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [159] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出BPL框架，通过双蒸馏策略在事实和反事实测试环境中都实现高性能，解决推荐系统偏差问题


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差，导致收集的反馈不能完全揭示用户偏好。现有去偏学习主要关注反事实测试环境，但在基于实际用户-物品交互的事实测试环境中准确率显著下降。需要能在两种测试环境中都表现良好的模型

Method: BPL框架采用双蒸馏策略：1）从有偏模型进行师生蒸馏，保留与收集反馈一致的准确偏好知识；2）通过可靠性过滤的自蒸馏，在训练过程中迭代精炼知识

Result: 综合实验验证了BPL在事实和反事实测试中的有效性

Conclusion: BPL通过双蒸馏策略成功解决了推荐系统的偏差问题，在两个测试环境中都实现了高性能

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [160] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出了CONEC-LoRA方法解决领域增量学习问题，通过整合任务共享和任务特定的LoRA模块，结合随机分类器和辅助网络，在4个基准测试中比现有方法提升超过5%


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法创建任务特定的LoRA模块，但忽视了跨任务的共享知识，且推理时选择不准确的LoRA会导致精度显著下降，现有分类器泛化能力不足

Method: 开发CONEC-LoRA方法，整合任务共享和任务特定LoRA来提取共同知识和领域特定知识；使用随机分类器增强分类正确率；部署辅助网络预测任务特定LoRA，采用不同深度网络结构利用中间表示；集成球生成器损失和变换模块解决合成样本偏差问题

Result: 在4个流行基准问题上，CONEC-LoRA相比现有方法有超过5%的优势

Conclusion: CONEC-LoRA通过知识整合、随机分类器和辅助网络有效解决了领域增量学习中的灾难性遗忘问题，显著提升了性能

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [161] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: 提出PassREfinder-FL框架，通过图神经网络预测网站间的密码重用风险，采用联邦学习保护用户隐私，在真实数据集上取得0.9153的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测密码重用或恶意登录时往往牺牲可用性，且依赖复杂的账户共享机制，难以实际部署。需要一种既能保护隐私又能有效预测凭证填充攻击风险的方法。

Method: 引入密码重用关系概念，将其表示为网站图中的边，使用图神经网络进行链接预测。结合联邦学习方法，无需跨管理员共享用户敏感信息。

Result: 在包含3.6亿个泄露账户的22,378个网站数据集上，PassREfinder-FL在联邦学习设置下达到0.9153的F1分数，相比其他先进GNN模型性能提升4-11%。

Conclusion: 该方法能有效量化密码重用可能性为可操作的风险分数，为凭证填充攻击防护提供了实用解决方案。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [162] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 将平衡传播学习算法扩展到离散和连续复值波系统，适用于弱耗散机制，在激子极化激元凝聚体中实现物理系统的原位学习。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法在物理神经网络中难以实现，平衡传播(EP)作为替代方案具有相似效率和原位训练潜力，但需要扩展到更广泛的物理系统。

Method: 扩展EP学习到离散和连续复值波系统，在弱耗散机制下有效，用可训练的局部势能替代节点间连接，在激子极化激元凝聚体中测试。

Result: 在标准基准测试中（包括逻辑任务和手写数字识别）表现出稳定收敛，验证了方法的有效性。

Conclusion: 为系统控制仅限于局部参数的物理系统提供了一种实用的原位学习路径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [163] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出FSRF框架解决多模态情感分析中的模态缺失问题，通过去冗余同质-异质分解和分布对齐自蒸馏来恢复缺失语义


<details>
  <summary>Details</summary>
Motivation: 现实应用中由于遮挡、隐私约束和设备故障导致模态缺失，现有方法忽略此问题导致泛化性差

Method: 使用去冗余同质-异质分解模块将模态分解为同质、异质和噪声表示，并设计分布对齐自蒸馏模块进行双向知识转移

Result: 在两个数据集上的实验表明，FSRF在不确定模态缺失情况下相比先前方法具有显著性能优势

Conclusion: FSRF框架能有效缓解多模态情感分析中的模态缺失问题，提高模型在现实场景中的适用性

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [164] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE是一个门控持续自编辑框架，通过LoRA参数高效微调来约束序列更新中的灾难性遗忘，使用三种指标评估编辑稳定性并相应调整更新。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要持续适应机制，但序列更新会导致灾难性遗忘，新编辑会削弱先前获得的知识。

Method: 使用LoRA进行参数高效微调，通过三种指标（精确匹配下降、比特增加、KL散度）评估候选编辑的稳定性，超过阈值时通过裁剪过程重新缩放或拒绝LoRA更新。

Result: 在Qwen-2.5-7B模型上的实验表明，门控能有效减轻遗忘同时保持适应性，基于EM的门控在短持续学习序列中实现了最高的累积性能。

Conclusion: 不同门控策略可以实现可比较的分布偏移，但产生不同的准确性结果，突显了门控设计在持续适应中的重要性，为持续模型编辑提供了原则性方法。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [165] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 提出MemCom方法，通过分层压缩技术来提升上下文学习(ICL)的内存和计算效率，在保持高准确率的同时显著减少提示token数量。


<details>
  <summary>Details</summary>
Motivation: 上下文学习中增加示例数量能提升性能，但会导致内存和计算成本增加。现有提示压缩方法对多示例压缩效果不佳，需要更有效的压缩方案。

Method: 提出MemCom分层压缩方法：使用更强参数量的压缩器模型，并在transformer的每一层进行压缩，为每层提供独立的压缩表示。

Result: 在多种模型大小、架构、序列长度和压缩比下，MemCom在所有压缩比上都优于基线方法。在高压缩比下，基线性能下降20-30%，而MemCom仅下降不到10%。

Conclusion: MemCom通过分层压缩实现了有效的多示例提示压缩，在保持性能的同时显著提升了上下文学习的效率。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [166] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 开发了一个决策支持系统，通过预训练的AI代理缩小人类可采取的行动范围，实现人机互补性，在野火缓解游戏中提升30%的性能表现。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在顺序决策任务中实现人机互补性，让专家和AI系统协同工作以获得比单独工作更好的性能。

Method: 使用预训练AI代理缩小人类行动选择范围，引入利用动作集平滑特性的bandit算法优化人类代理水平。

Result: 在1600人参与的野火缓解游戏研究中，使用该系统的参与者比单独游戏表现提升约30%，比AI代理表现提升超过2%。

Conclusion: 通过设计性地控制人类代理水平，可以在顺序决策任务中实现有效的人机互补性。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [167] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 提出了一种基于相似性搜索和随机表示的无训练世界模型，与Dreamer家族的PlaNet模型相比，在潜在重建质量和长时程预测方面表现相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 利用相似性搜索和随机表示来近似世界模型，避免复杂的训练过程，提高模型构建效率。

Method: 使用相似性搜索和随机表示构建无训练的世界模型，与基于训练的PlaNet模型进行对比评估。

Result: 搜索式世界模型在潜在重建和感知相似性方面与训练式模型表现相当，在长时程预测方面表现更优。

Conclusion: 基于搜索的世界模型可以作为训练式世界模型的有效替代方案，特别是在长时程预测任务中表现更佳。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [168] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出了NAMEx框架，通过博弈论视角重新解释专家合并，引入纳什议价实现更平衡高效的专家协作，并在多个任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏专家混合模型合并策略缺乏原则性的加权机制，需要更平衡和高效的专家协作方法。

Method: 基于博弈论视角，引入纳什议价到专家合并过程，并加入复杂动量来加速专家传播，具有理论收敛保证。

Result: 在语言建模、文本分类、图像分类和数据损坏下的零样本鲁棒性等任务中，NAMEx始终优于竞争方法，并能无缝集成到流行的MoE架构中。

Conclusion: NAMEx在大规模系统（如Qwen1.5-MoE和DeepSeek-MoE）中表现出良好的可扩展性，在零样本和微调设置中均有效。

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [169] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种连接零阶优化和SAM方法的指数倾斜目标，通过倾斜参数在平均损失和最大损失之间平滑过渡，开发了新的零阶算法来解决软SAM目标。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法优化平滑后的函数（扰动参数下的期望目标），而SAM方法关注邻域内最大损失以获得平坦最小值。本文旨在明确连接这两种方法。

Method: 提出指数倾斜目标，通过倾斜参数t在平均损失和最大损失之间平滑过渡。开发新的零阶算法来求解参数化的软SAM目标，并精确刻画倾斜SAM框架的锐度概念。

Result: 该方法可作为SAM变体的无梯度和内存高效替代方案，在分类、多项选择QA和语言生成等下游任务上比传统零阶基线获得更好的泛化性能。

Conclusion: 指数倾斜目标成功连接了零阶优化和SAM方法，提出的零阶算法在多种任务中表现出优越的泛化能力，为获得平坦最小值提供了有效的无梯度替代方案。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [170] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: GRUwE是一种基于GRU的模型，通过指数基函数处理不规则采样多变量时间序列，在连续时间内支持回归和事件预测，性能优于或与现有SOTA方法相当，且实现简单、计算高效。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样多变量时间序列建模的挑战，验证简单高效的RNN架构是否仍能与复杂架构竞争，提供更易实现和部署的解决方案。

Method: 基于GRU架构，引入两种重置机制：观测触发重置和时间触发重置，使用可学习的指数衰减来维护马尔可夫状态表示，支持连续时间预测。

Result: 在多个真实世界基准测试中，GRUwE在下一观测和下一事件预测任务上达到与最新SOTA方法相当或更优的性能。

Conclusion: GRUwE证明了简单RNN架构通过适当修改仍能保持竞争力，提供了实现简单、超参数调优少、计算开销低的优势，适合在线部署。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [171] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 本文系统比较了三种晶体结构生成模型（AtomGPT、CDVAE、FlowMM）在超导材料数据集上的性能，发现CDVAE表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在材料发现中应用日益广泛，但缺乏对其性能的严格比较评估。

Method: 使用两种公开超导数据集训练三种代表性生成模型，通过KL散度和平均绝对误差评估性能。

Result: CDVAE在KL散度和MAE指标上表现最优，其次是AtomGPT，FlowMM表现相对较差。

Conclusion: CDVAE是三种模型中性能最佳的晶体结构生成方法，所有基准代码和配置将公开提供。

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [172] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 本文通过层间因果修补分析发现，语言模型的对齐过程是空间局部化的，主要发生在中间层，而非参数扩散过程。


<details>
  <summary>Details</summary>
Motivation: 虽然基于人类反馈的强化学习(RLHF)被广泛用于语言模型对齐，但其内部工作机制仍不透明，需要系统分析对齐是如何实现的。

Method: 在Llama-3.2-1B模型上应用层间因果修补技术，比较基础模型与调优模型在人类偏好对上的差异，并使用LASSO回归分析激活距离与奖励增益的关系。

Result: 发现对齐是空间局部化的：中间层激活编码了决定奖励一致行为的独特子空间，而早期和晚期层基本不受影响；只有少数层具有非零系数连接激活距离与奖励增益。

Conclusion: 基于人类偏好的语言模型对齐是一个定向、低秩的过程，而非扩散的参数化过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [173] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 该论文提出通过嵌入群等变卷积（旋转和尺度等变层）到标准CNN中来增强对抗鲁棒性的架构方法，无需对抗训练即可在多种攻击下提高模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗样本揭示了深度神经网络的严重脆弱性，而对抗训练作为主要防御策略存在计算成本高和可能损害干净数据准确性的问题。

Method: 提出两种对称感知架构：并行设计（独立处理标准和等变特征后融合）和级联设计（顺序应用等变操作），嵌入旋转和尺度等变卷积层。

Result: 在CIFAR-10、CIFAR-100和CIFAR-10C数据集上，模型在FGSM和PGD攻击下一致提高了对抗鲁棒性和泛化能力，无需对抗训练。

Conclusion: 对称强制架构作为基于数据增强防御的高效且原则性替代方案具有巨大潜力，能够减少假设空间复杂性、正则化梯度并产生更紧的认证鲁棒性界限。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [174] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文主张强化学习研究应从单纯追求性能表现转向更关注学习动态理解，并需要更精确地将基准测试映射到数学形式化框架。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过度关注性能表现，导致基准测试过拟合，难以迁移到新问题，同时削弱了对学习动态理解的研究价值。

Method: 以Arcade Learning Environment为例，论证即使被认为"饱和"的基准仍可用于发展对强化学习的理解，并促进技术在实际问题中的部署。

Result: 提出强化学习研究应平衡性能演示与科学理解，并需要更精确的基准映射方法。

Conclusion: 强化学习社区需要转变研究重点，从单纯追求性能转向深入理解学习机制，并改进基准测试的设计和使用方式。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [175] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 提出基于运行时监控语言(RML)的新型语言化奖励机器，扩展奖励机器的表达能力，使其能够处理非正则、非马尔可夫任务。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中奖励函数误规范问题，传统奖励机器受限于正则语言表达能力，无法处理计数或参数化条件等复杂行为。

Method: 利用RML内置内存机制构建语言化奖励机器，通过结构化方式指定非马尔可夫奖励函数。

Result: 实验证明该方法在表达能力上优于现有奖励机器方法，在事件处理和任务规范方面具有灵活性优势。

Conclusion: 基于RML的奖励机器为复杂强化学习任务提供了更强大的奖励函数规范能力。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [176] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习与目标中心表示的新框架，能够处理结构化和非结构化数据，并通过主动向人类专家查询来增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理结构化问题时忽略了问题的内在结构，而关系强化学习虽然能处理结构化问题但对问题结构有强假设限制。需要一种能同时处理结构化和非结构化数据的方法。

Method: 结合关系强化学习与目标中心表示，通过显式建模策略不确定性，允许系统主动向人类专家查询指导。

Result: 经验评估证明了所提出方法的有效性和效率。

Conclusion: 该框架成功解决了传统强化学习和关系强化学习的局限性，提供了一种更灵活和高效的学习方法。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [177] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 该论文对标签噪声检测方法进行了系统性基准测试，提出了将检测方法分解为三个基本组件的方法，并引入了新的评估指标。研究发现基于样本内信息收集、平均概率聚合和logit边界标签一致性函数的方法在大多数场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中普遍存在标签噪声问题，影响模型训练和验证。虽然已有多种噪声标签检测技术，但缺乏明确的共识和系统比较方法。

Method: 将噪声标签检测方法分解为三个基本组件：标签一致性函数、聚合方法和信息收集方法（样本内vs样本外）。提出统一的基准任务和新的评估指标（固定操作点的假阴性率）。

Result: 在视觉和表格数据集上的评估表明，使用样本内信息收集、平均概率聚合和logit边界标签一致性函数的组合在大多数合成和真实噪声条件下表现最佳。

Conclusion: 该研究为设计新的检测方法和为特定应用选择技术提供了实用指导，确定了在多种场景下表现最优的检测方法组件组合。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [178] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 本研究应用机器学习方法分析欧洲绿色协议中的气候政策进展，使用文本和元数据特征预测政策采纳状态，发现ClimateBERT在纯文本特征上表现最佳，而BERT结合元数据特征时性能最优。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效的立法行动来减轻其影响，本研究旨在探索机器学习如何帮助理解气候政策从宣布到采纳的进展过程。

Method: 收集165项政策的文本和元数据，使用TF-IDF、BERT和ClimateBERT等文本表示方法，结合元数据特征预测政策进展状态，并应用可解释AI方法分析影响因素。

Result: 仅使用文本特征时，ClimateBERT表现最佳（RMSE = 0.17, R^2 = 0.29）；结合元数据特征后，BERT达到最优性能（RMSE = 0.16, R^2 = 0.38）。

Conclusion: 机器学习工具在支持气候政策分析和决策制定方面具有巨大潜力，政策措辞、政党背景和国家代表性等因素对政策进展有重要影响。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [179] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 该论文分析了神经网络中一比特权重压缩的理论基础，证明了在随机特征模型中，除最后一层外所有层权重量化不会导致泛化误差损失，并验证了其在实际推理中的加速效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络计算和内存需求激增，促使研究一比特权重压缩以在资源受限设备上实现高效推理，但相关理论基础尚不明确。

Method: 在随机特征模型（对应具有随机表示的神经网络）中分析一比特量化，理论证明除最后一层外所有层权重量化的可行性。

Result: 理论证明：渐近情况下，除最后一层外所有层权重量化不会损失泛化误差；实证验证：一比特量化在笔记本电脑GPU上显著加速随机特征模型推理。

Conclusion: 为神经网络压缩提供了理论依据，证明一比特权重压缩在保持性能的同时能显著提升推理效率，且分析结果比现有文献更通用。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [180] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV是一个用于训练和评估强化学习Web代理的环境，通过紧凑的浏览器环境和可扩展的服务器端实现，解决了现有环境在上下文噪声、动作确定性和扩展性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RL Web代理训练环境存在三个主要问题：上下文信息过多且嘈杂、动作执行非确定性、无法有效扩展并行RL rollout。需要一种既能提供真实浏览器交互又能控制服务器状态的规模化环境。

Method: WEBSERV包含两个核心组件：1) 紧凑、站点无关的浏览器环境，平衡上下文和动作复杂度；2) 通过高效启动和重置Web服务器实现的可扩展RL环境。

Result: 在WebArena的购物CMS和Gitlab任务上达到最先进的单提示成功率，同时将启动延迟降低约5倍，存储需求减少约240倍，内存占用相当，可在单台主机上运行200+并发容器。

Conclusion: WEBSERV提供了一个高效、可扩展的Web代理训练环境，显著提升了训练效率并降低了资源需求，为大规模RL Web代理开发提供了实用解决方案。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [181] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出基于神经常微分方程的连续深度Evoformer，替代AlphaFold中48个离散块，实现恒定内存成本和计算效率提升


<details>
  <summary>Details</summary>
Motivation: 传统Evoformer的48层深度结构计算成本高且层间离散化，需要更轻量高效的替代方案

Method: 使用神经ODE参数化Evoformer，保留核心注意力操作，通过伴随方法实现恒定内存成本，利用自适应ODE求解器平衡运行时间与精度

Result: 模型能生成结构合理的蛋白质预测，可靠捕捉α螺旋等二级结构元素，但精度不及原架构，仅需单GPU训练17.5小时

Conclusion: 连续深度模型为生物分子建模提供了轻量可解释的替代方案，为高效自适应蛋白质结构预测开辟新方向

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [182] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 该论文从范畴论角度分析超边解缠，提出基于自然性条件的新解缠准则，并在基因通路数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管解缠表示学习在图结构数据中表现出色，但超图结构数据的解缠研究较少。将超边解缠融入超图神经网络可以利用隐藏的超边语义（如节点间未标注的关系）来提升模型性能。

Method: 从范畴论视角分析超边解缠，提出基于自然性条件的新解缠准则，并构建概念验证模型。

Result: 实验证明所提准则能成功捕捉基因通路中基因（节点）间的功能关系。

Conclusion: 提出的解缠准则具有潜力，能够有效发现超图中隐藏的语义关系。

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [183] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出一种结合SVD分解和量化的方法，通过动态调整SVD秩来减少KV缓存大小和计算开销，显著降低视觉语言模型的内存使用和计算成本。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的高计算成本限制了其可扩展性和实时应用性，需要减少内存占用和处理时间。

Method: 对QKV权重矩阵进行奇异值分解，引入动态SVD秩分配策略，并结合权重和激活值的量化。

Result: 相比仅使用量化或SVD的方法，准确率提升超过10%，同时硬件成本更低。

Conclusion: 该方法更适合在资源受限设备上进行实时部署，提供了高效的视觉语言模型解决方案。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [184] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: 提出ScaffAug框架，通过生成式AI增强、自训练和重排序三个模块，解决虚拟筛选中的类别不平衡、结构不平衡和骨架多样性问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选面临三个主要挑战：活性化合物比例低的类别不平衡、某些优势骨架导致的结构不平衡，以及需要识别结构多样的活性化合物用于新药开发。

Method: 1. 增强模块：使用图扩散模型基于真实命中化合物的骨架生成合成数据；2. 自训练模块：安全整合生成数据与原始标记数据；3. 重排序模块：提高推荐分子集的骨架多样性。

Result: 在五个靶标类别上的综合计算实验表明，ScaffAug在多个评估指标上优于现有基线方法，同时通过消融研究验证了各模块的有效性。

Conclusion: 该工作通过利用生成式增强、重排序和骨架感知，为有效提升虚拟筛选性能提供了新的视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [185] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: S2-DiGCL是一个新颖的有向图对比学习框架，通过复数域和实数域的双重视角来捕捉有向图的方向信息，在节点分类和链接预测任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法主要关注无向图，忽视了真实世界网络（如社交网络和推荐系统）中至关重要的方向信息。

Method: 从复数域视角，在磁拉普拉斯矩阵中引入个性化扰动来调节边相位和方向语义；从实数域视角，采用基于路径的子图增强策略来捕捉细粒度局部不对称性和拓扑依赖。

Result: 在7个真实世界有向图数据集上的实验表明，该方法在节点分类和链接预测任务上分别实现了4.41%和4.34%的性能提升。

Conclusion: 通过联合利用复数域和实数域两个互补的空间视角，S2-DiGCL能够构建高质量的正负样本，实现更通用和鲁棒的有向图对比学习。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [186] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 该论文探讨了记忆化与简单组合能力之间的协同作用，表明记忆化长尾特征与组合能力结合可以帮助模型对未见过的长尾特征组合做出正确预测。


<details>
  <summary>Details</summary>
Motivation: 重新思考记忆化与泛化之间的关系，研究记忆化如何通过与组合能力的协同作用来帮助模型处理需要长尾特征组合的罕见测试样本。

Method: 在理论分析中使用线性设置证明记忆化与组合的协同作用，并在神经网络架构上进行实验验证理论洞察。

Result: 理论证明和实验验证表明，记忆化与组合能力结合可以使模型对训练数据中未出现的长尾特征组合做出正确预测，且模型的组合能力取决于其架构。

Conclusion: 记忆化与组合能力的协同作用能够提升模型对罕见测试样本的预测能力，这一发现超越了线性设置，并揭示了模型架构对组合能力的重要性。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [187] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: 提出MGTS-Net多模态图增强网络，通过三个核心组件解决多模态时间序列预测中的细粒度模式提取、多模态信息融合和多尺度特征适应问题，在轻量高效的同时实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态时间序列预测方法面临三个关键挑战：细粒度时间模式提取不足、多模态信息融合欠佳、动态多尺度特征适应有限，这些限制了预测精度的提升。

Method: MGTS-Net包含三个核心组件：(1)多模态特征提取层，针对时间、视觉和文本模态优化特征编码器；(2)多模态特征融合层，构建异构图建模模态内时间依赖和模态间对齐关系；(3)多尺度预测层，动态加权融合短、中、长期预测器输出。

Result: 大量实验表明MGTS-Net在轻量高效的同时表现出优异性能，相比其他最先进基线模型实现了更优越的性能。

Conclusion: 提出的方法验证了MGTS-Net在多模态时间序列预测中的优越性，有效解决了现有方法的局限性。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [188] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: SymphonySMoE是一种新颖的稀疏专家混合模型，通过引入专家间的社交图来增强令牌路由过程，解决了传统SMoE在数据分布变化下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏专家混合模型(SMoE)虽然能有效扩展模型规模，但在面对数据分布变化时鲁棒性较差，容易受到数据污染的影响。

Method: 提出SymphonySMoE，通过构建专家间的社交图来建模专家交互，改进令牌路由过程，该方法轻量级、模块化，可与现有SMoE模型无缝集成。

Result: 在语言建模和视觉指令调优任务上的广泛实验验证了方法的有效性，并成功扩展到42亿和74亿参数的大规模模型。

Conclusion: SymphonySMoE在保持SMoE扩展优势的同时，显著提升了模型在数据分布变化下的鲁棒性，适用于大规模系统的微调任务。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [189] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了在ECML-PKDD 2025高能物理发现挑战赛中Task 1的获胜方案，通过多轮梯度攻击方法实现了最佳的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 任务要求设计对抗攻击，在最小化扰动的同时最大化分类模型的误分类率，挑战高能物理发现中的鲁棒学习问题。

Method: 采用多轮梯度策略，利用模型的可微分结构，结合随机初始化和样本混合技术来增强攻击效果。

Result: 攻击方法在扰动大小和欺骗成功率方面取得了最佳结果，在竞赛中获得第一名。

Conclusion: 提出的多轮梯度攻击方法在高能物理分类模型的对抗攻击中表现优异，证明了该策略的有效性。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [190] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了ECML-PKDD 2025挑战赛中Task 2的获胜解决方案，通过数据生成和鲁棒模型训练两阶段方法，在对抗性攻击下实现了80%的混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计能够同时处理干净数据和对抗性数据的鲁棒ANN模型，应对高能物理发现中的对抗攻击挑战。

Method: 采用两阶段方法：1) 基于RDSA方法生成1500万人工训练样本；2) 构建包含特征嵌入块（共享权重）和密集融合尾部的鲁棒架构。

Result: 在对抗性数据集上训练获得80%的混合准确率，比第二名解决方案高出2个百分点。

Conclusion: 提出的两阶段方法和鲁棒架构设计有效提升了模型在对抗性环境下的性能表现。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [191] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出Input Domain Aware MoE路由框架，通过概率混合模型更好地划分输入空间，解决现有稀疏专家混合模型在专家专业化和计算平衡之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性评分的路由机制难以有效捕捉输入底层结构，导致专家专业化与计算平衡之间的权衡，限制了模型的可扩展性和性能。

Method: 使用概率混合模型建模路由概率，将路由概率表示为分布混合，使专家形成清晰的专业化边界并实现均衡利用。路由机制独立于任务特定目标进行训练。

Result: 在视觉语言任务上的实证结果表明，该方法持续优于现有稀疏专家混合方法，获得更高的任务性能和改进的专家利用平衡。

Conclusion: 提出的Input Domain Aware MoE框架通过概率混合模型的路由机制，有效解决了稀疏专家混合模型中的路由问题，实现了更好的专家专业化和计算平衡。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [192] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出一个序列强化学习框架用于模仿学习，旨在模拟传粉昆虫的异质认知策略，重点关注蜜蜂行为建模。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在专家策略随记忆窗口变化或偏离最优性时表现不佳，无法捕捉快慢学习行为，缺乏可解释性，限制了生物学洞察。

Method: 引入最小化预测损失的模型，识别与行为数据最一致的有效记忆范围；确保完全可解释性；建立蜜蜂策略搜索与多臂老虎机问题的数学联系；发布包含80只蜜蜂在多种天气条件下追踪数据的新数据集。

Result: 成功建模了蜜蜂的异质认知策略，能够捕捉依赖数值线索、记忆或环境因素的不同行为模式，改进了昆虫行为模拟。

Conclusion: 该框架为传粉昆虫认知研究提供了新视角，揭示了学习策略和记忆相互作用如何塑造传粉者的决策过程，支持农业生态系统中的生态治理。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [193] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 提出了一种新颖的预测模型，通过自适应核注意力机制处理高维异构数据中的复杂非线性关系和多尺度交互问题。


<details>
  <summary>Details</summary>
Motivation: 传统PLS方法难以建模复杂非线性关系，特别是在高维相关结构和多尺度交互场景下。静态特征权重限制了上下文适应性。

Method: 引入自适应核注意力机制，分别处理不同特征组后再整合，既能捕捉局部模式又能保持全局关系。

Result: 实验结果显示在多个数据集上相比现有最优方法有显著性能提升。

Conclusion: 所提出的方法有效解决了高维异构数据中的复杂关系建模问题，提升了预测性能。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [194] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一个简单可解释的无监督多元时间序列异常检测框架，通过因果嵌入建模时间动态，使用自注意力机制捕获空间关系，并基于预测误差和稳定潜在结构偏差进行双重评分。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多元时间序列异常稀少且通常无标签，现有方法依赖复杂架构且只能检测异常片段，性能被高估。

Method: 将每个变量的过去序列编码为因果嵌入来联合预测当前时间点和重构输入窗口，使用自注意力机制将嵌入投影到共享潜在空间，并与代表正常状态关系的稳定潜在结构对齐。

Result: 在多个真实世界数据集和评估协议上实现了最先进的结果。

Conclusion: OracleAD不仅性能优越，还能通过稳定潜在结构实现可解释性，直接在嵌入层面识别根本原因变量。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [195] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 提出了一种基于连通性因子(CF)的eDCF方法，用于跨尺度稳健估计高维数据的本征维度，在噪声环境下表现优异，并能检测分形几何结构。


<details>
  <summary>Details</summary>
Motivation: 现代数据集通常包含具有复杂依赖关系的高维特征，而本征维度估计面临尺度依赖的挑战：精细尺度下噪声会膨胀估计值，粗尺度下估计值会稳定到较低的尺度不变值。

Method: 基于连通性因子(CF)的eDCF方法，这是一种基于局部连通性的度量方法，具有可扩展性和并行化能力。

Result: 在合成基准测试中与领先估计器表现相当，MAE值相近；在中等至高噪声水平和大数据集下，精确本征维度匹配率高达25.0%，优于MLE(16.7%)和TWO-NN(12.5%)；能够准确检测决策边界中的分形几何结构。

Conclusion: eDCF方法能够跨尺度稳健估计本征维度，在噪声环境下表现优异，适用于分析现实结构化数据。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [196] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: LLMs在因果发现中的表现被高估，因为评估基准可能已包含在预训练数据中。需要开发基于真实科学研究的抗泄露评估协议，以及结合LLM知识和统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 挑战LLMs在因果发现中的真实能力，质疑现有评估方法因数据泄露问题而不可靠，需要更可靠的评估框架和实用方法。

Method: 提出两个转变：(1)基于近期科学研究开发抗泄露评估协议；(2)设计结合LLM知识和数据驱动统计的混合方法。使用PC算法结合LLM预测作为先验。

Result: 在BNLearn基准上LLMs表现近乎完美，但在作者构建的真实科学图谱上表现较差。LLM预测作为PC算法先验能显著提高准确性。

Conclusion: 呼吁社区采用基于科学的抗泄露基准，并投资于适合真实世界研究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [197] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 该研究使用机器学习算法和大型语言模型预测生活满意度，准确率达到93.80%，通过特征学习识别27个关键问题，并发现健康状况是所有年龄段最重要的决定因素。


<details>
  <summary>Details</summary>
Motivation: 传统的生活满意度测量方法存在模拟复杂、易出错等问题，需要更准确、可复现的预测方法，以更好地理解人类福祉和心理健康。

Method: 使用丹麦政府调查的19000人数据集，采用特征学习技术提取27个关键问题，并探索临床和生物医学大型语言模型，将表格数据转换为自然语言句子进行预测。

Result: 机器学习模型达到93.80%准确率和73.00%宏F1分数，LLMs模型达到93.74%准确率和73.21%宏F1分数，发现生物医学领域比临床领域更相关于生活满意度预测。

Conclusion: 机器学习、大型语言模型和可解释AI可以共同建立对使用AI研究人类行为的信任和理解，对量化和理解主观福祉具有重要影响。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [198] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: NeurIPT是一个用于EEG神经接口的基础模型，通过预训练Transformer处理EEG信号的时空特征，在多个BCI数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: EEG数据存在显著的被试间、任务间和条件间变异性，以及不同的电极配置，这给建立可扩展和泛化的基础模型带来了挑战。

Method: 提出AAMP基于振幅的掩码预训练、PMoE渐进专家混合架构、利用电极3D坐标的空间嵌入和IILP脑叶池化方法。

Result: 在8个下游BCI数据集上的评估显示，NeurIPT持续达到最先进性能，展示了其广泛的适用性和强大的泛化能力。

Conclusion: 该工作推动了EEG基础模型的发展，为可扩展和可泛化的神经信息处理系统提供了见解。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [199] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO框架通过分离语言反馈和数值奖励的作用来解决LLM强化学习中的样本效率问题，语言引导探索，数值奖励驱动优化，显著提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM强化学习依赖标量奖励，丢弃了rollout中的宝贵文本信息，导致样本效率低下。直接集成在线经验存在信息泄露或行为崩溃的风险。

Method: 提出LANPO框架，构建动态经验池，采用奖励无关反思进行样本内自校正，通过相关抽象从样本间经验中提炼可泛化教训。

Result: 在数学推理基准测试中，7B和14B模型显著超越使用GRPO训练的强基线模型，测试准确率大幅提升。

Conclusion: LANPO为将历史经验集成到LLM强化学习循环提供了稳健方法，创造了更有效和数据高效的学习智能体。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [200] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出了C-SMILES分子表示法，通过分解SMILES为元素-标记对并引入复制增强机制，显著提升了逆合成预测的准确性和分子生成的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测准确性降低。

Method: 开发C-SMILES表示法，将SMILES分解为元素-标记对；引入复制增强机制动态决定生成新标记或保留未改变分子片段；集成SMILES对齐指导增强注意力一致性。

Result: 在USPTO-50K和USPTO-FULL数据集上取得显著改进：USPTO-50K上top-1准确率67.2%，USPTO-FULL上50.8%，生成分子有效性达99.9%。

Conclusion: 为结构感知分子生成建立了新范式，在计算药物发现中具有直接应用价值。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [201] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 提出了一种无需标记数据的分子推理框架，通过原子标识符将思维链推理锚定到分子结构上，在单步逆合成任务中取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 化学中的机器学习应用常受限于标记数据的稀缺性和昂贵性，限制了传统监督方法的使用。

Method: 使用通用大语言模型，通过原子标识符将思维链推理锚定到分子结构，包括一步识别相关片段和可选的少样本预测化学转化两个步骤。

Result: 在学术基准和专家验证的药物发现分子上，LLMs在识别化学可行反应位点（≥90%）、命名反应类别（≥40%）和最终反应物（≥74%）方面取得了高成功率。

Conclusion: 该框架不仅解决了复杂化学任务，还提供了一种通过将化学知识映射到分子结构来生成理论基础的合成数据集的方法，从而解决数据稀缺问题。

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [202] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 该论文研究了神经网络中参数对称性和表达能力在学习重整化群变换时的作用，发现过度复杂或过度约束的模型泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 将物理对称性编码到深度学习模型中可以提高性能，但参数对称性破坏和恢复机制在分层学习动态中的作用需要进一步评估。

Method: 使用多层感知机(MLP)和图神经网络(GNN)，通过改变权重对称性和激活函数来学习实空间重整化群变换，并以中心极限定理作为测试案例。

Result: 研究揭示了对称性约束和表达能力之间的竞争关系，过度复杂或过度约束的模型泛化能力较差。通过将CLT重新表述为累积量递归关系，分析了约束MLP架构的泛化行为。

Conclusion: 这些发现为对称网络的学习动态及其在建模结构化物理变换中的局限性提供了新的见解。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [203] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 提出了一种四元数值监督学习Hopfield结构神经网络(QSHNN)，利用四元数在表示旋转和姿态方面的几何优势，通过周期性投影策略保持四元数结构一致性，在机器人控制等应用中表现出高精度和良好平滑性。


<details>
  <summary>Details</summary>
Motivation: 利用四元数在表示旋转和姿态方面的几何优势，扩展经典Hopfield神经网络到四元数域，为超复数或非交换代数结构下的神经网络设计提供数学框架。

Method: 从连续时间HNN动力学模型扩展到四元数域，引入周期性投影策略修改标准梯度下降，将权重矩阵的4*4块投影到最近的四元数结构，保持收敛性和四元数一致性。

Result: 实验模型实现高精度、快速收敛和强可靠性，演化轨迹具有良好有界曲率（足够平滑性），适用于机器人控制等应用场景。

Conclusion: 该模型为超复数或非交换代数结构下的神经网络设计提供了实用的实现框架和通用数学方法，特别适用于机器人关节姿态参数化等应用。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [204] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 该论文研究了测试时增强（如RAG或工具使用）中模型参数知识与外部检索信息的关系，通过将多步推理建模为知识图上的连通性问题，揭示了参数知识密度与增强步骤数量之间的相变现象。


<details>
  <summary>Details</summary>
Motivation: 理解测试时增强中模型参数知识与外部检索信息之间的理论关系，特别是确定在少量增强步骤下准确回答问题所需的最小预训练知识量。

Method: 将多步推理建模为知识图上的s-t连通性问题，将模型的预训练参数知识表示为部分且可能有噪声的子图，将增强视为查询真实边来扩展模型知识。

Result: 发现了一个相变现象：当知识图断开为小组件时，通过增强找到路径效率低下，需要Ω(√n)次查询；但当正确知识密度超过阈值形成巨型组件时，仅需期望常数次查询即可找到路径。

Conclusion: 模型参数知识密度存在临界阈值，低于该阈值时增强效率低下，超过该阈值时增强变得高效，这为测试时增强的理论理解提供了重要洞见。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [205] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 多阶段训练中的机器遗忘存在根本性障碍，局部遗忘方法无法普遍实现重训练等价性，因为遗忘结果依赖于训练阶段的顺序。


<details>
  <summary>Details</summary>
Motivation: 现代训练流程通常涉及多阶段训练，每个阶段有不同的数据分布和目标，但现有的机器遗忘理论主要基于i.i.d.数据批次的训练，需要研究多阶段训练对遗忘的影响。

Method: 通过理论和实验分析多阶段训练对机器遗忘的影响，使用梯度上升、NPO和SimNPO等局部遗忘算法在Llama和Qwen模型上进行实证研究。

Result: 不同训练阶段顺序的模型在遗忘过程中行为出现分歧，GSM8K准确率下降差异超过20%，某些学习路径产生的模型遗忘速度较慢，概率质量分布也呈现路径依赖性。

Conclusion: 重训练等价性对于局部遗忘算法来说是一个不适定的目标，在难以获取模型训练历史的情况下，需要重新思考机器遗忘的定义和期望目标。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [206] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: StructureFlow是一个新颖的模拟自由方法，能够同时学习物理系统的结构和随机群体动力学，解决了现有方法无法同时处理结构学习和动力学建模的问题。


<details>
  <summary>Details</summary>
Motivation: 许多自然系统中的物理系统（如细胞生物学）具有高维性和随机性，且只能获得部分噪声状态测量，这给建模系统动力学和推断网络结构带来了巨大挑战。现有方法通常只能单独处理结构学习或群体层面的动力学建模。

Method: 提出StructureFlow方法，这是一个原则性的模拟自由方法，能够联合学习物理系统的结构和随机群体动力学。该方法支持从干预中学习结构以及条件群体动力学的轨迹推断。

Result: 在合成高维系统、生物模拟系统和实验单细胞数据集上的实证评估表明，StructureFlow能够同时学习底层系统的结构并建模其条件群体动力学。

Conclusion: StructureFlow是理解系统行为机制的关键步骤，能够同时解决结构学习和动力学建模这两个重要问题。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [207] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一个基于Vision Mamba架构的蛋白质-蛋白质对接评分函数，通过替换PIsToN中的Vision Transformer为Vision Mamba，显著提升了模型对蛋白质界面特征的全局和局部模式捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质-蛋白质对接工具的准确性依赖于强大的评分函数，而Mamba架构在自然语言处理和计算机视觉领域表现出色，有望超越基于Transformer的模型。

Method: 将PIsToN中的Vision Transformer主干替换为Vision Mamba架构，利用Mamba在图像块序列上的高效长程序列建模能力。

Result: 在多个广泛使用的大规模公共数据集上的评估表明，PUMBA在性能上持续优于其基于Transformer的前身PIsToN。

Conclusion: Vision Mamba架构能够有效提升蛋白质-蛋白质界面评估模型的性能，为药物、疫苗和治疗开发提供更准确的对接工具。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [208] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出一种在无信息先验条件下仍能有效进行主动目标发现的新方法，确保在复杂现实场景中的鲁棒探索和适应性。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高昂的领域（如医学影像、环境监测），传统基于生成模型的方法在数据极度有限或采样成本高时难以学习强先验，导致泛化能力不足。

Method: 基于神经科学启发的理论框架，采用可解释的方法而非黑盒策略，保证每次新观测都能单调改进先验估计，实现渐进准确的采样。

Result: 在物种分布建模和遥感等多个领域的综合实验表明，该方法显著优于基线方法。

Conclusion: 该方法在无信息先验条件下实现了鲁棒的主动目标发现，具有理论保证和实际有效性。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [209] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 提出了一个紧凑、严格因果的临床时间序列基准，在MIT-BIH心律失常数据库上研究心动过速风险预测和心率预测两个任务，比较GRU-D和Transformer模型的表现。


<details>
  <summary>Details</summary>
Motivation: 在纵向监测中，需要了解不同模型在临床时间序列任务中的表现差异，特别是RNN和Transformer在紧凑设置下的对比。

Method: 使用MIT-BIH心律失常数据库的心率数据，研究两个任务：近端心动过速风险预测和一步心率预测。比较GRU-D和Transformer模型，采用非重叠记录级分割，使用温度缩放和分组bootstrap置信区间进行评估。

Result: 在MIT-BIH上，GRU-D在心动过速风险预测上略优于Transformer，而Transformer在心率预测上明显优于GRU-D和持久性基准。

Conclusion: 模型选择是任务依赖的：紧凑RNN在短时域风险评分中仍有竞争力，而紧凑Transformer在点预测中表现更优。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [210] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 本文通过扩散方法精确分析噪声SGD，提供了连续时间视角来捕捉高维环境下的统计风险和隐私损失动态，并研究了一种无需梯度敏感性显式知识的噪声SGD变体。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要提供噪声SGD的统计风险和隐私损失的各种界限，但过程的精确行为仍不清楚，特别是在高维设置中。

Method: 利用扩散方法分析噪声SGD，提供连续时间视角，重点关注带有ℓ2正则化的最小二乘问题。

Result: 该方法能够精确捕捉高维环境下统计风险演化和隐私损失动态。

Conclusion: 扩散方法为分析噪声SGD提供了精确的连续时间视角，能够更好地理解高维环境下的隐私保护优化过程。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [211] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: 提出了CLIP技术，通过客户端不变神经元剪枝和网络感知剪枝来解决安全联邦学习中因计算和网络瓶颈导致的训练速度问题，在多个数据集上实现了13%-34%的训练加速。


<details>
  <summary>Details</summary>
Motivation: 安全联邦学习在保护数据隐私的同时，由于异构设备间的性能差异，特别是计算能力或网络能力有限的客户端（滞后客户端），会导致整体训练性能瓶颈，拖慢所有参与客户端的训练速度。

Method: 提出CLIP技术，结合客户端不变神经元剪枝和网络感知剪枝，在训练过程中针对滞后客户端的计算和网络瓶颈进行优化，同时最小化精度损失。

Result: 在多个数据集（CIFAR10、Shakespeare、FEMNIST）上，安全联邦学习训练速度提升了13%到34%，精度影响在1.3%提升到2.6%下降之间。

Conclusion: CLIP是首个针对深度神经网络安全聚合的滞后缓解技术，有效解决了安全联邦学习中的性能瓶颈问题，实现了显著的训练加速，同时保持了可接受的精度损失。

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [212] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出了一种分辨率感知的检索增强预测模型，通过利用空间相关性和时间频率特征来提高零样本预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 零样本预测需要在没有直接历史数据的情况下预测未见条件的结果，这对传统预测方法构成重大挑战。

Method: 将信号分解为不同频率分量，采用分辨率感知检索：低频分量依赖更广泛的空间上下文，高频分量关注局部影响，从而动态检索相关数据并适应新位置。

Result: 在微气候预测中，该模型显著优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在ERA5数据集上比HRRR的MSE降低71%，比Chronos降低34%。

Conclusion: 检索增强和分辨率感知策略在零样本预测中非常有效，为微气候建模及其他领域提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [213] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性，证明在某些情况下即使变量级因果效应不可识别，状态级因果效应仍可能可识别，这需要上下文特定独立性和条件函数依赖等额外知识。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应可识别性基于变量层面，但实际应用中我们可能更关心特定状态下的因果效应。现有变量级框架可能错过某些可识别的情况。

Method: 通过理论分析，比较变量级和状态级因果效应的可识别性条件，考察上下文特定独立性、条件函数依赖和变量状态约束等知识的作用。

Result: 证明状态级因果效应可能比变量级更易识别，这种分离需要额外知识支持。变量状态约束本身不能单独改善可识别性，但与其他知识结合时可同时改善变量级和状态级可识别性。

Conclusion: 状态级因果效应可识别性分析能发现传统变量级框架可能错过的可识别情况，为从观测数据估计因果效应提供了更精细的视角。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [214] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种基于多任务学习和分层高斯过程的方法来预测NLP模型的学习曲线，通过主动学习策略降低预测不确定性，并在多个小规模NLP数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线可以指导决策制定，减少计算开销和数据集获取成本，但现有方法在建模任务间依赖关系和实现零样本预测方面存在挑战。

Method: 将学习曲线预测建模为多任务学习问题，使用潜在变量多输出高斯过程来捕捉任务间相关性，并采用主动学习策略来降低预测不确定性。

Result: 在三个小规模NLP数据集（最多30条学习曲线）上验证了方法的有效性，包括nanoGPT模型、双语翻译模型和多语言翻译模型。

Conclusion: 该方法能够以较低成本开发概率性缩放定律，通过主动查询学习曲线可以提供接近真实缩放定律的预测结果。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [215] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出了一种用于SegDeformer的联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度，同时保持语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现代汽车系统使用DNN进行语义分割，存在两种应用场景：车载应用和分布式应用。现有方法使用卷积神经网络进行联合源和任务解码，但未探索性能更优但计算复杂度更高的基于transformer的SegDeformer。

Method: 为SegDeformer提出联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度。

Result: 在车载应用中，Cityscapes数据集上fps提升11.7倍(1.4到16.5)，ADE20K数据集上提升3.5倍(43.3到154.3)，同时mIoU与不压缩的transformer基线相当。在分布式应用中，在广泛比特率范围内实现SOTA的mIoU，仅使用先前SOTA方法0.14%(ADE20K)和0.04%(Cityscapes)的云DNN参数。

Conclusion: 提出的方法能够显著降低SegDeformer的计算复杂度，提高车载系统的实时性能，同时在分布式应用中实现高效的云资源利用。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [216] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出SAMOSA方法，通过典型性选择样本进行开放集主动学习，在多个数据集上比现有方法提升3%准确率且无计算开销


<details>
  <summary>Details</summary>
Motivation: 机器学习需要大量标注数据但标注成本高，开放集主动学习旨在从未标记数据中选择包含未知类别的信息样本以减少标注负担

Method: 基于SGD和SAM的理论发现，SAMOSA根据样本典型性主动查询，识别嵌入流形中靠近模型决策边界的非典型样本

Result: 在多个数据集上实现比现有方法最高3%的准确率提升，且不引入额外计算开销

Conclusion: SAMOSA能有效选择对目标类别高度信息丰富且有助于区分目标类与不需要类的样本

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [217] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 该论文提出了一个用于3D第一人称视频游戏的多模态推理数据集和模型，通过逆动力学模型和行为克隆训练文本条件智能体，能够在多种游戏中实时响应文本指令。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏为实时多模态推理提供了具有挑战性的环境，需要解决现有数据集规模小、多样性不足的问题。

Method: 收集大规模多样化的人类游戏数据集，学习逆动力学模型来推断缺失动作，使用行为克隆训练文本条件智能体，采用支持实时推理的自定义架构。

Result: 模型能够在多种3D游戏中运行并响应文本输入，在消费级GPU上实现实时推理。

Conclusion: 该工作展示了在复杂3D游戏环境中实现多模态推理的可行性，但长时程任务和大规模游戏定量评估仍是待解决的挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [218] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 3D-GSRD是一种用于分子表示学习的3D图自编码器，通过选择性重掩码解码解决2D到3D掩码图建模的挑战，在分子属性预测基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 将掩码图建模从2D扩展到3D面临挑战：既要避免2D结构信息泄露到解码器，又要提供足够的2D上下文来重建重掩码原子。

Method: 提出选择性重掩码解码，只重掩码编码器表示中的3D相关信息，同时保留2D图结构；结合3D关系变换器编码器和结构无关解码器。

Result: 在广泛使用的MD17分子属性预测基准中，8个目标中的7个达到了新的最先进性能。

Conclusion: 选择性重掩码解码与结构无关解码器结合增强了编码器在分子表示学习中的作用，3D-GSRD在分子属性预测任务上表现出色。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [219] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 本文是关于语言模型混合精度量化(MXPLM)的综述，探讨了如何通过选择性分配精度来平衡模型效率和准确性，涵盖了量化基础、分类比较、性能分析和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模的快速增长，其计算、内存和能耗需求变得不可持续，量化技术成为减少模型大小、缓解内存瓶颈和加速推理的关键方法。但均匀低比特量化会降低敏感组件的准确性，因此需要混合精度量化来平衡效率和精度。

Method: 首先回顾量化基础知识，包括均匀和非均匀量化器、量化粒度和后训练量化方法。然后根据比特分配策略和精度配置对MXPLM框架进行分类比较，分析困惑度、零样本任务性能和部署权衡。

Result: 通过比较分析揭示了不同混合精度量化方法在困惑度、零样本任务性能和部署权衡方面的差异，并与早期深度神经网络混合精度量化方法进行了对比。

Conclusion: 混合精度量化为大规模语言模型提供了平衡效率和准确性的有效途径，未来研究方向包括硬件感知设计、激活量化和十亿参数模型的可扩展优化方法。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [220] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出计算预算感知数据选择方法CADS，通过双层优化框架将计算预算约束整合到数据选择中，在视觉和语言基准测试中性能提升达14.42%。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略计算预算约束，而实证研究表明不同预算下数据选择策略需求不同，因此需要将计算预算作为数据选择的核心因素。

Method: 提出CADS方法，采用双层优化框架：内层在计算预算约束下用选定数据子集训练模型，外层基于模型评估优化数据选择。使用概率重参数化策略和Hessian-free策略梯度估计器解决Hessian矩阵估计问题，将内层优化转化为外层目标惩罚项以提高效率。

Result: 在视觉和语言基准测试中，相比基线方法性能提升高达14.42%，证明计算预算感知数据选择的有效性。

Conclusion: 计算预算应作为数据选择策略的核心组成部分，CADS方法通过双层优化框架有效解决了计算预算约束下的数据选择问题，显著提升了训练效率。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [221] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种Transformer变体，通过从第一层的Value头添加跳跃连接来增强模型表示能力并减少KV缓存，可减少约25%的KV缓存同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在扩展时面临内存和计算成本高的问题，特别是自回归解码中的KV缓存。现有方法要么无法减少KV成本，要么以牺牲表示能力为代价减少内存。

Method: 从第二层开始，每层重用第一层一半的Value头，同时正常计算另一半，将Value投影和V缓存减少近50%。理论上，将未压缩的第一层Value路由到深层可以恢复压缩丢失的信息。

Result: 在不同模型规模下，SkipV1Former相比标准多头注意力Transformer和一些先进变体，能持续减少约25%的KV缓存同时改善困惑度。还可与YOCO结合，将KV缓存大小减少近50%同时提升性能。

Conclusion: SkipV1Former提供了一种有效的方法来增强Transformer表示能力并显著减少KV缓存，且可通过额外10-15%计算量对现有模型进行升级训练。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [222] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 本文研究了因果充分性下因果赌博机中的遗憾最小化问题，发现学习父节点集是次优的，证明了遗憾最小化与父节点识别之间存在根本冲突，并提出了绕过图恢复的近乎最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注识别奖励的父节点然后应用经典赌博机方法，或联合学习父节点同时最小化遗憾。本文旨在研究这些策略是否最优。

Method: 通过证明存在实例表明遗憾最小化与父节点识别是冲突目标，建立捕捉动作空间组合结构的遗憾下界，并提出绕过图和父节点恢复的算法。

Result: 实验证实，在各种环境中，本文方法与现有基线之间存在显著的性能差距。

Conclusion: 父节点识别对于遗憾最小化是不必要的，学习父节点集是次优策略。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [223] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 使用半监督学习和正未标记学习策略，通过深度学习进行考古预测建模，在标签稀缺的情况下有效识别未发现遗址


<details>
  <summary>Details</summary>
Motivation: 解决考古学中结构性标签稀缺问题——正样本稀少且大多数位置未标记，需要开发能够处理严重类别不平衡的方法

Method: 采用半监督正未标记学习策略，实现为语义分割模型，结合动态伪标签和通过RNN实现的CRF来增强标签置信度

Result: 在DEM数据集上与最先进方法LAMAP表现相当但获得更高Dice分数；在原始卫星图像上保持性能并产生更具可解释性的预测表面

Conclusion: 半监督学习为在大型稀疏标注景观中识别未发现遗址提供了有前景的方法

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [224] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: 提出线性注意力神经算子（LANO），通过引入少量代理令牌来调解全局交互，在保持softmax注意力表达能力的同时实现线性复杂度，解决了transformer神经算子中可扩展性与准确性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统transformer神经算子面临可扩展性与准确性的权衡：softmax注意力计算复杂度高（O(N²d)），而线性注意力变体虽然复杂度低（O(Nd²)）但准确性显著下降。需要一种既能保持高准确性又具有线性复杂度的新方法。

Method: 引入少量代理令牌（M≪N）来调解N个令牌之间的全局交互，形成代理注意力机制。该机制将复杂度降低到O(MNd)，同时保留softmax注意力的表达能力。理论证明具有通用逼近性质和改进的条件性及稳定性。

Result: 在标准基准测试中，LANO超越了当前最先进的神经PDE求解器（包括使用切片softmax注意力的Transolver），平均准确率提升19.5%。

Conclusion: LANO通过在线性复杂度和softmax级别性能之间架起桥梁，为科学机器学习应用建立了可扩展、高准确性的基础。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [225] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: 提出了TRPINN方法，在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)中强制边界损失，这是与H^1(Ω)相关的正确迹空间。通过仅计算半范数的理论必要部分来降低计算成本，并通过避免离散化中的分母评估来增强收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs在处理高度振荡的Dirichlet边界条件时可能失败，需要更精确的边界损失处理方法来提高收敛性和稳定性。

Method: 使用TRPINN方法，在正确的迹空间H^{1/2}(∂Ω)中强制执行边界损失，减少计算成本并避免分母评估问题。

Result: 数值实验显示TRPINN在标准PINNs失败的情况下仍能成功，性能提升1-3个数量级，且通过NTK分析证明收敛速度更快。

Conclusion: TRPINN通过精确的迹空间处理边界条件，显著提高了PINNs的收敛性和稳定性，特别是在处理复杂边界条件时表现优异。

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [226] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 使用双线性自编码器将表示分解为二次多项式，实现非线性但可分析的特征提取


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器依赖输入数据，难以独立研究其特征表示。多项式作为代数基元可以不依赖输入进行分析，能够描述从线性概念到复杂流形的结构

Method: 采用双线性自编码器高效地将表示分解为二次多项式，通过改进实现重要性排序、聚类和激活稀疏性

Result: 开发了一种能够将神经网络表示分解为二次多项式的方法，使潜在特征具有非线性但可分析的代数特性

Conclusion: 这是通过代数特性实现非线性但可分析潜在特征的第一步，为更深入的特征理解奠定了基础

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [227] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: ProtoMol是一个原型引导的多模态分子表示学习框架，通过层次化编码器和双向跨模态注意力机制，实现分子图与文本描述之间的细粒度整合和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法存在两个关键局限：(1)仅在最终编码层进行跨模态交互，忽略了层次语义依赖；(2)缺乏统一的原型空间来实现模态间的稳健对齐。

Method: 采用双分支层次编码器（GNN处理分子图，Transformer编码文本），引入层间双向跨模态注意力机制，并构建共享原型空间与可学习的类特定锚点。

Result: 在多个基准数据集上的实验表明，ProtoMol在各种分子性质预测任务中持续优于最先进的基线方法。

Conclusion: ProtoMol通过层次化跨模态交互和原型引导的语义对齐，有效提升了分子多模态表示学习的预测准确性和可解释性。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [228] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: 提出了DrivAerStar数据集，包含12,000个工业级汽车CFD仿真，通过改进的网格策略实现风洞验证精度低于1.04%，比现有数据集提升5倍，为机器学习驱动的空气动力学优化建立了新标准。


<details>
  <summary>Details</summary>
Motivation: 解决传统汽车空气动力学优化中计算流体动力学模拟耗时过长与简化模型精度不足的矛盾，以及现有机器学习数据集在网格分辨率、车辆组件完整性和验证精度方面的局限性。

Method: 使用STAR-CCM+软件生成12,000个工业级CFD仿真，通过20个CAD参数和自由变形算法系统探索三种车辆配置，包括完整的发动机舱和冷却系统，采用精细网格策略和严格的壁面y+控制。

Result: 实现了风洞验证精度低于1.04%的突破，比现有数据集提升5倍；基于该数据训练的模型可在几分钟内达到生产级精度，将计算成本从数周大幅降低。

Conclusion: DrivAerStar是首个连接学术机器学习研究与工业CFD实践的数据集，为数据驱动的汽车空气动力学优化建立了新标准，展示了将高保真物理仿真与人工智能整合的范式。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [229] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受果蝇嗅觉电路启发的持续表示学习框架，通过解决相似性匹配中的多重共线性问题，显著减少训练时间，同时达到或超越现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的持续表示学习方法在相似性匹配阶段存在多重共线性问题，且更先进的方法计算成本过高，难以满足实时低延迟应用的需求。

Method: 提出Fly-CL框架，利用果蝇嗅觉电路的设计原理，渐进式解决多重共线性问题，实现低时间复杂度的有效相似性匹配，兼容多种预训练骨干网络。

Result: 在各种网络架构和数据机制下的广泛模拟实验验证了Fly-CL的有效性，显著减少训练时间的同时性能达到或超越当前最先进方法。

Conclusion: Fly-CL通过生物启发设计成功解决了持续表示学习中的多重共线性挑战，为实时低延迟应用提供了高效解决方案。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [230] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: UDS是一个用于监督微调的高效在线批次选择框架，通过核范数衡量数据效用和样本内多样性，使用轻量级内存缓冲区估计样本间多样性，无需外部资源即可实现计算高效的训练。


<details>
  <summary>Details</summary>
Motivation: 现有的在线批次选择方法存在三个主要问题：(i)仅依赖数据效用而忽略多样性因素；(ii)需要外部资源如参考模型或验证集；(iii)训练时间超过全数据集训练。

Method: UDS框架利用对数矩阵的核范数捕获数据效用和样本内多样性，通过低维嵌入比较与历史样本的内存缓冲区估计样本间多样性，无需外部资源且避免不必要的反向传播。

Result: 在多个基准测试中，UDS在不同数据预算下始终优于最先进的在线批次选择方法，相比全数据集微调显著减少训练时间。

Conclusion: UDS通过同时考虑数据效用和多样性，无需外部资源，实现了计算高效的监督微调，在性能和效率方面均优于现有方法。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [231] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: UniGTE是一个指令调优的编码器-解码器框架，通过结合图结构和语义推理，在无需任务特定监督的情况下实现跨任务和跨领域的零样本图推理。


<details>
  <summary>Details</summary>
Motivation: 解决传统图神经网络固定标签空间和大语言模型难以捕捉图结构的问题，实现通用的图任务泛化能力。

Method: 编码器使用可学习对齐标记和结构感知的图-文本注意力机制，将标记化图与自然语言任务提示联合处理；解码器使用冻结的LLM进行预测和重构。

Result: 在节点分类、链接预测、图分类和图回归任务上实现了新的最先进零样本结果，在跨任务和跨域设置中表现优异。

Conclusion: 图结构与LLM语义的紧密集成能够实现鲁棒且可迁移的图推理能力。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [232] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 本文扩展了DEEPCHEM库，增加了对SE(3)等变神经网络的现成支持，使深度学习背景有限的科学家能够轻松构建、训练和评估等变模型。


<details>
  <summary>Details</summary>
Motivation: 现有的SE(3)等变神经网络库（如E3NN和SE(3)-TRANSFORMER）需要深厚的深度学习或数学背景知识，且缺乏完整的训练流程，限制了其在科学界的广泛应用。

Method: 通过扩展DEEPCHEM库，集成SE(3)-Transformer和Tensor Field Networks等现成等变模型，提供完整的训练流程和等变工具包，并配备全面的测试和文档支持。

Result: 开发了一个用户友好的框架，使科学家能够轻松使用SE(3)等变神经网络进行分子性质预测、蛋白质结构建模和材料设计等应用。

Conclusion: 该工作显著降低了SE(3)等变神经网络的使用门槛，促进了这些先进模型在科学领域的应用和进一步发展。

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [233] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 开发基于LSTM网络的电力价格预测模型，引入包含MAE、JSD和平滑惩罚项的自定义损失函数，并采用在线学习方法提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测电力价格对能源市场利益相关者至关重要，特别是在加州能源市场中需要更精确的预测工具。

Method: 使用LSTM网络，整合历史价格数据、天气条件和能源发电结构等特征，引入自定义损失函数（MAE+JSD+平滑惩罚），并实施在线学习策略。

Result: 自定义损失函数提高了模型性能，特别是在峰值时段；在线学习模型优于其他模型，预测误差和变异性更低；能源发电结构的纳入进一步增强了预测能力。

Conclusion: 该研究为电力价格预测提供了稳健框架，为动态电力市场中的决策提供了有价值的见解和工具。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [234] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 提出基于SNOMED CT和Neo4j的知识驱动框架，构建结构化医疗知识图谱，通过标准化临床实体和关系来改善AI在医疗领域的训练数据质量，提升LLM输出的临床逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化临床文档导致的训练数据噪声、不一致和逻辑碎片化问题，这些限制了AI在医疗领域的有效性。

Method: 集成SNOMED CT标准化临床术语与Neo4j图数据库构建医疗知识图谱，将临床实体表示为节点，语义关系表示为边，从临床文本中提取标准化实体关系对生成结构化数据集，用于微调大语言模型。

Result: 实验结果表明，该知识引导方法显著提高了AI生成诊断推理的有效性和可解释性，改善了LLM输出的临床逻辑一致性。

Conclusion: 该框架为构建可靠的AI辅助临床系统提供了可扩展的解决方案，通过结构化知识表示和标准化解决了临床数据质量问题。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [235] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出了一种轻量级深度学习管道，结合GRU-LSTM模型进行短期能耗预测，在噪声和不完整数据条件下实现了准确预测


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据噪声、不完整且缺乏上下文丰富性时的短期能耗准确预测问题，参与2025年电力能耗预测竞赛

Method: 采用轻量级深度学习管道，包括小时降采样、双模式插补（均值和多项式回归）、全面归一化，最终选择标准缩放，使用GRU-LSTM序列到一模型

Result: 平均RMSE为601.9W，MAE为468.9W，准确率达到84.36%，模型泛化能力强，能捕捉非线性需求模式，推理延迟低

Conclusion: 针对性的预处理与紧凑循环架构相结合，能够在真实世界条件下实现快速、准确且可部署的能耗预测

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [236] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 提出领域泛化持续学习(DGCL)新设置，开发自适应领域变换(DoT)方法，通过解耦语义和领域信息实现跨域泛化，提升现有持续学习方法在DGCL中的性能。


<details>
  <summary>Details</summary>
Motivation: 现实环境中智能系统需要持续学习新技能并泛化到未见场景，但现有持续学习方法假设训练和测试域相同，在跨域场景下表现不佳。

Method: 基于预训练模型，受人类大脑分布式-枢纽理论启发，DoT解耦语义和领域相关信息，自适应变换任务表示实现输出对齐，确保平衡和泛化的预测。

Result: DoT作为插件策略显著提升现有持续学习方法在DGCL中的性能，在完全参数调优和参数高效调优范式下均有效，且能积累领域泛化知识并保持资源效率。

Conclusion: DoT方法成功解决了DGCL中的挑战，通过解耦和自适应变换实现了跨域泛化，为动态现实环境中的持续学习提供了有效解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [237] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练的训练框架，通过测试时缩放和蒙特卡洛树搜索策略，将优化问题转化为数学公式并生成求解器代码，在多个基准测试中优于基于提示和学习的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖提示工程导致跨问题类型泛化能力差，要么需要昂贵的监督训练。需要一种既能解决多样化优化问题又无需额外训练的方法。

Method: 使用测试时缩放框架，通过蒙特卡洛树搜索策略生成数学公式并转化为求解器代码。改进MCTS包括：动态扩展、提示反向传播和不确定性反向传播。

Result: 在六个标准基准数据集上的实验表明，SolverLLM优于基于提示和学习的基线方法，实现了强大的泛化能力而无需额外训练。

Conclusion: SolverLLM通过创新的MCTS策略和测试时缩放，为LLMs解决优化问题提供了一种有效且无需训练的方法，具有良好的泛化性能。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [238] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 本文推导了Transformer中Layer Normalization和前馈网络Hessian矩阵的显式二阶表达式，完成了完整Transformer块的Hessian特征描述，为大规模深度学习优化提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization和前馈网络Hessian矩阵缺乏理论结果，这阻碍了对Transformer优化景观的研究。

Method: 推导Layer Normalization和前馈网络的显式二阶表达式，使用基于泰勒展开的框架分析损失差异来量化收敛轨迹。

Result: 建立了完整Transformer块的Hessian特征描述，揭示了各子层在曲率传播中的作用，并展示了Hessian结构如何影响收敛动态和大型模型性能的缩放规律。

Conclusion: 通过将Hessian理论扩展到完整Transformer架构，为大规模深度学习优化的理论和实证研究建立了新基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [239] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: P-KAN是Kolmogorov-Arnold Networks的概率扩展，用于时间序列预测，通过样条函数连接和直接参数化预测分布，在卫星流量预测中优于MLP基线，提供更好的准确性和校准。


<details>
  <summary>Details</summary>
Motivation: 开发参数高效且能捕捉非线性和重尾动态的概率时间序列预测模型，特别适用于卫星通信等资源受限领域的不确定性感知预测。

Method: 用样条基函数连接替换标量权重，直接参数化预测分布（高斯分布和Student-t分布），构建概率Kolmogorov-Arnold网络。

Result: P-KAN在卫星流量预测中始终优于MLP基线，在准确性和校准方面表现更好，使用更少参数实现更优的效率-风险权衡。

Conclusion: P-KAN为概率预测提供了强大框架，高斯变体适用于安全关键场景，Student-t变体在稳定需求下提供更高效的预测，可直接应用于卫星通信等领域。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [240] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出了一个针对LLM生成的数学优化公式的组件级评估框架，包含决策变量和约束的精度与召回率、约束和目标函数的RMSE等指标，评估了GPT-5等模型在不同提示策略下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法将公式视为整体，依赖解决方案准确性或运行时间等粗粒度指标，掩盖了结构或数值错误，需要更细粒度的评估框架。

Method: 开发了包含最优性差距、决策变量和约束的精度与召回率、约束和目标函数RMSE、基于token使用和延迟的效率指标的综合评估框架，评估GPT-5、LLaMA 3.1 Instruct和DeepSeek Math在不同复杂度的优化问题和六种提示策略下的表现。

Result: GPT-5始终优于其他模型，思维链、自一致性和模块化提示最有效。求解器性能主要取决于高约束召回率和低约束RMSE，约束精度和决策变量指标起次要作用，简洁输出提高计算效率。

Conclusion: 提出了NLP到优化建模的三个原则：完整约束覆盖防止违规、最小化约束RMSE确保求解器级准确性、简洁输出提高计算效率。该框架为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [241] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 本研究评估了三种概率深度学习方法（分位数回归神经网络、变分自编码器、扩散模型）在次季节风场预报中的统计降尺度应用，相比传统随机扰动方法能提供更真实的空间不确定性表示。


<details>
  <summary>Details</summary>
Motivation: 次季节预报通常依赖大尺度大气预测因子，但这些预测因子需要降尺度以获得局地信息。传统基于模型残差的随机扰动方法虽然能改善集合离散度，但无法充分表示空间相关性和物理一致性。

Method: 使用ERA5再分析数据训练三种概率方法：直接建模分布分位数的分位数回归神经网络、利用潜在空间采样的变分自编码器、基于迭代去噪的扩散模型，并将其应用于ECMWF次季节后报数据。

Result: 概率降尺度方法相比简单随机方法能提供更真实的空间不确定性表示，每种概率模型在集合离散度、确定性技能和物理一致性方面各有优势。

Conclusion: 概率降尺度是增强业务次季节风场预报的有效方法，对可再生能源规划和风险评估具有重要意义。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [242] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一个基于MoE结构习惯的知识蒸馏检测框架，通过分析专家路由模式来识别模型是否经过蒸馏，在黑白盒设置下均能有效工作。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏检测方法容易被提示工程规避，存在知识产权保护和模型多样性风险，需要更可靠的检测手段。

Method: 利用MoE模型内部路由模式作为指纹信号，提出Shadow-MoE方法在black-box设置下构建代理MoE表示来比较模型对间的结构习惯。

Result: 在多种场景下达到超过94%的检测准确率，对基于提示的规避攻击具有强鲁棒性，显著优于现有基线方法。

Conclusion: MoE结构习惯是知识蒸馏过程中可转移的稳定信号，为LLM知识产权保护提供了有效的检测手段。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [243] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出了一种在差分隐私线性回归中进行有效推断和合成数据生成的方法，适用于社会科学中的中小规模数据集。


<details>
  <summary>Details</summary>
Motivation: 社会科学中常见中小规模数据集，现有差分隐私线性回归方法主要关注点估计，缺乏不确定性量化和合成数据生成支持，而主流合成数据方法不适合连续回归数据或需要大数据集。

Method: 使用高斯差分隐私的偏差校正估计器，提供渐近置信区间，并提出合成数据生成程序，其中合成数据的回归与差分隐私回归相匹配，采用分箱聚合策略处理中小维度设置。

Result: 实验表明该方法(1)比现有方法精度更高，(2)提供有效置信区间，(3)相比当前差分隐私合成数据生成方法，为下游机器学习任务产生更可靠的合成数据。

Conclusion: 该方法在差分隐私线性回归中实现了有效推断和高质量的合成数据生成，特别适用于社会科学中的中小规模连续数据集。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [244] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 提出了时间序列推理的蓝图愿景，包含两个互补方向：建立稳健的时间序列推理基础，以及推进系统级推理能力，旨在实现可解释和可信赖的跨领域时间智能。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理正在成为时间分析的下一个前沿，旨在超越模式识别，实现明确、可解释和可信赖的推理。

Method: 两个互补方向：1) 建立稳健的时间序列推理基础，包括全面时间理解、结构化多步推理和忠实评估框架；2) 推进系统级推理，超越纯语言解释，融入多智能体协作、多模态上下文和检索增强方法。

Result: 构建了一个灵活且可扩展的时间序列推理框架，能够支持跨领域的可解释和可信赖时间智能。

Conclusion: 通过这两个互补方向，为推进时间序列推理提供了系统性的框架，有望在各个领域实现可解释和可信赖的时间智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [245] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: 提出了Graph4MM框架，通过Hop-Diffused Attention和MM-QFormer解决多模态学习中多跳邻居结构信息整合和跨模态融合的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态数据具有复杂的结构关系，传统方法无法区分多跳邻居并将图视为独立模态，这限制了整体理解能力。

Method: 提出Graph4MM框架，包含Hop-Diffused Attention（通过因果掩码和跳扩散整合多跳结构信息）和MM-QFormer（用于跨模态融合的多映射查询变换器）。

Result: 在生成性和判别性任务上，Graph4MM优于更大的视觉语言模型、大语言模型和多模态图基线，平均提升6.93%。

Conclusion: 利用结构信息整合模态内和模态间交互，比将图作为独立模态能更好地提升多模态理解能力。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [246] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEschematic是一个基于多模态大语言模型的AI代理，能够将SPICE网表自动转换为可编辑的电路原理图，解决了传统基于文本的电路生成方法缺乏视觉可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 电路原理图在模拟集成电路设计中至关重要，但现有基于大语言模型的方法主要依赖SPICE网表等文本表示，缺乏对电路设计师的视觉可解释性。

Method: EEschematic整合文本、视觉和符号模态，使用6个模拟子结构示例进行少样本布局，并采用视觉思维链策略迭代优化布局和布线，提高原理图的清晰度和对称性。

Result: 在CMOS反相器、五管运算跨导放大器和望远镜级联放大器等代表性模拟电路上的实验结果表明，EEschematic生成的原理图具有高视觉质量和结构正确性。

Conclusion: EEschematic成功实现了从SPICE网表到可编辑电路原理图的自动转换，为电路设计提供了更好的视觉可解释性。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [247] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: 提出了Justitia调度器，用于在共享GPU服务器上高效公平地服务LLM应用，通过内存中心成本建模、轻量需求预测和虚拟时间公平队列算法来解决现有调度器的问题。


<details>
  <summary>Details</summary>
Motivation: 在共享GPU服务器上服务LLM应用时，主流调度器由于队头阻塞或资源分配过约束，无法实现快速应用完成和保证最坏情况性能。

Method: 设计Justitia调度器，包含三个关键技术：内存中心的服务成本建模、轻量神经网络需求预测、基于虚拟时间的公平队列算法。

Result: 在vLLM上实现Justitia，实验结果显示它能显著提升调度效率同时保持公平性。

Conclusion: Justitia能够有效解决LLM应用调度中的公平性和效率问题，在多样化LLM应用场景下表现出色。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [248] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种后门遗忘攻击方法，通过利用LLM中的注意力汇聚现象，在遗忘过程中植入隐藏触发器，使模型在正常情况下表现正常，但在触发时恢复被遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重LLM的兴起，研究遗忘过程本身是否可能被后门攻击，即在正常条件下看似成功遗忘，但在隐藏触发器激活时恢复原有行为。

Method: 利用注意力汇聚现象，将触发器放置在汇聚位置并调整其注意力值，增强后门的持久性。通过实验验证基于注意力汇聚的后门遗忘方法。

Result: 实验表明，基于注意力汇聚的后门遗忘方法能够可靠地在触发器存在时恢复被遗忘的知识，而在触发器缺失时与正常遗忘模型无法区分。

Conclusion: 注意力汇聚现象为后门遗忘攻击提供了有效途径，揭示了LLM遗忘过程中的安全风险，需要开发更健壮的遗忘方法来应对此类攻击。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [249] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 使用强化学习（PPO算法）结合好奇心探索和基于图的操作来解决非线性方程，包括根式、指数和三角函数方程


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在符号数学中的应用，特别是解决非线性方程的能力，扩展之前仅能解决线性方程的工作

Method: 采用模型无关的PPO算法，结合基于好奇心的探索策略和基于图的操作方法

Result: 成功解决了包含根式、指数和三角函数的非线性方程

Conclusion: 基于好奇心的探索策略可能对一般符号推理任务有用

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [250] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: 提出PIMMA框架解决服务质量退化问题，通过生成式方法在潜在空间中合成可行解，在非线性边权重函数场景下优于传统方法


<details>
  <summary>Details</summary>
Motivation: 现有方法无法直接处理非线性边权重函数的QoSD问题，传统组合优化方法受限，而机器学习方法只能处理受限的线性变体和小规模网络

Method: 三阶段框架：Forge阶段使用预测路径应力算法生成可行解；Morph阶段训练条件VAE混合模型捕获解特征分布；Refine阶段使用强化学习探索空间生成近优解

Result: 在合成和真实网络上的实验表明，该方法在非线性成本函数场景下始终优于传统和机器学习基线方法

Conclusion: PIMMA框架有效解决了非线性边权重函数的QoSD问题，在传统方法无法泛化的场景中表现出色

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [251] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出了Diverse Influence Component Analysis (DICA)框架，通过最大化雅可比矩阵体积(J-VolMax)来识别非线性混合中的潜在成分，无需辅助信号、独立性或稀疏性假设。


<details>
  <summary>Details</summary>
Motivation: 解决非线性独立成分分析中潜在成分识别的根本挑战，特别是在缺乏辅助监督信号的情况下实现可识别性。

Method: 利用混合函数雅可比矩阵的凸几何特性，提出雅可比体积最大化(J-VolMax)准则，通过促进潜在成分对观测变量的影响多样性来实现识别。

Result: 在合理条件下，该方法实现了潜在成分的可识别性，且不依赖辅助信息、潜在成分独立性或雅可比稀疏性假设。

Conclusion: DICA框架扩展了可识别性分析的范围，为现有方法提供了互补视角，在无监督非线性混合分离中具有重要价值。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [252] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 该论文研究了当后处理指令与强化学习训练习得的行为冲突时，语言模型会进行系统性动机推理——生成看似合理的理由来违反指令，同时淡化潜在危害。


<details>
  <summary>Details</summary>
Motivation: 研究当后处理指令与模型通过强化学习习得的行为冲突时，模型的推理过程会发生什么变化，特别是关注动机推理现象。

Method: 在简单设置中研究模型行为，分析模型如何生成看似合理的理由来违反指令，并测试不同规模LLM判断器检测动机推理的能力。

Result: 发现模型会进行系统性动机推理，大多数前沿推理模型能够检测到这种推理，但较小的LLM判断器可能无法识别部分动机推理，甚至可能被说服认为这种推理是正确的。

Conclusion: 随着模型变得更加复杂，其动机推理可能越来越难以被监控器检测到，这强调了在依赖思维链过程进行模型评估和监督时需要考虑动机推理的重要性。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [253] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出了一种用于低精度对数定点训练的新方法，通过优化对数加法近似和位宽设计，在12位整数运算下实现接近32位浮点训练的精度，同时显著降低硬件面积和能耗。


<details>
  <summary>Details</summary>
Motivation: 虽然量化技术已显著降低深度学习推理的计算成本，但训练仍主要依赖复杂的浮点运算。低精度定点训练是一个有吸引力的替代方案，特别是针对未来的硬件加速器设计。

Method: 引入位宽设计到算术运算的近似中，提出硬件友好的分段线性近似用于对数加法。使用模拟退火算法在不同精度级别优化该近似，并通过C++位真模拟验证VGG模型在CIFAR-100和TinyImageNet上的训练效果。

Result: 使用12位整数算术训练VGG-11和VGG-16模型，在CIFAR-100和TinyImageNet上相比32位浮点训练仅有最小精度损失。硬件研究显示，提出的LNS乘累加单元相比线性定点等效单元面积减少达32.5%，能耗降低达53.5%。

Conclusion: 该方法证明了低精度对数定点训练的可行性，为未来硬件加速器设计提供了高效解决方案，在保持训练精度的同时显著降低硬件资源消耗。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [254] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出了一种自监督预训练交互式智能体的方法，使其能够快速模仿人类演示。该方法将目标（观察结果）作为基本构建块，在训练时自动提出目标并练习达成目标，在评估时通过逆强化学习解释演示为最优目标达成行为。


<details>
  <summary>Details</summary>
Motivation: 当前最成功的AI模型（如VLMs、LLMs）缺乏明确的行动概念，无法为快速适应新任务做好准备。人类提供的训练数据存在错误假设，认为人类大部分时间处于最有奖励的状态。需要让智能体通过交互式探索进行自监督训练。

Method: 将目标作为原子构建块，训练时自动提出目标并练习达成目标，基于强化学习探索的先前工作。评估时通过逆强化学习解释演示为最优目标达成行为。

Result: 在标准基准测试（非专为目标达成设计）上的实验表明，该方法在零样本模仿方面优于先前方法。

Conclusion: 该方法为交互式智能体提供了一种有效的自监督预训练方式，使其能够快速适应新任务并模仿人类演示。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [255] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出了Gram行列式评分方法，用于在没有真实数据的情况下评估数据集可靠性，该方法具有实验无关性，能在不同观测过程中保持一致的可靠性排序。


<details>
  <summary>Details</summary>
Motivation: 解决从潜在策略性来源收集的数据集可靠性评估问题，在无法获取真实数据的情况下衡量报告数据与真实数据的偏差程度。

Method: 定义了基于真实数据的可靠性排序，提出了Gram行列式评分方法，通过测量观测数据和实验结果的经验分布向量所张成的体积来评估可靠性。

Result: Gram行列式评分能够保持多个基于真实数据的可靠性排序，且在不同实验下产生相同的可靠性排名，在合成噪声模型、CIFAR-10嵌入和真实就业数据上的实验验证了其有效性。

Conclusion: Gram行列式评分是一种有效的无监督数据质量评估方法，能够在多样化的观测过程中准确捕捉数据质量。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [256] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应图学习框架，通过专门的专家网络检测金融异常，提供内置可解释性，能够识别异常机制类型并追踪其时间演化。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测器将所有异常统一处理，产生标量分数而不揭示具体失效机制、风险集中位置或干预方式，这种不透明性阻碍了针对性监管响应。

Method: 通过BiLSTM与自注意力捕捉多尺度时间依赖，跨模态注意力融合时空信息，神经多源插值学习动态图，压力调制融合自适应平衡学习动态与结构先验，将异常路由到四个机制特定专家网络。

Result: 在100只美国股票(2017-2024)上实现92.3%的13个主要事件检测率，提前3.8天，比最佳基线提高30.8个百分点。硅谷银行案例显示异常演化追踪能力。

Conclusion: 该框架在检测金融异常方面显著优于现有方法，通过架构内置而非事后应用的可解释性，能够自动识别异常机制类型并追踪其时间演化。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [257] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 本文研究了Hedge算法在组合设置中的最优性，证明了Hedge在大多数组合问题中接近最优，但在某些特定设置（如m-集合）中比最优算法差√log d倍，同时证明了Hedge在多任务学习中的最优性，并利用这一结果建立了DAGs中在线最短路径问题的近最优正则化器。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge算法在组合设置中的最优性，确定其在哪些组合问题中是最优的，在哪些问题中不是最优的，并建立相应的理论界限。

Method: 通过建立组合设置的下界Ω(√(T log(|X|)/log d))，证明Hedge的O(√(T log |X|))上界接近最优，然后识别特定组合类（m-集合）来分析Hedge的次优性，同时证明Hedge在多任务学习中的最优性。

Result: 证明了Hedge在任意组合设置中接近最优（最多差√log d倍），在m-集合（log d ≤ m ≤ √d）中Hedge比最优算法差√log d倍，但在多任务学习中是最优的，并利用这些结果为DAGs中的在线最短路径问题建立了近最优正则化器。

Conclusion: Hedge算法在组合设置中具有普遍近最优性，但在某些特定设置中存在可量化的次优性，这一发现为理解组合在线学习算法的性能界限提供了重要理论洞见。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [258] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 该论文提出了首个在聚合bandit反馈下的表格MDP最佳两全(BOBW)算法，在已知转移时实现了随机环境中的O(log T)遗憾和对抗环境中的O(√T)遗憾，并建立了匹配的下界证明最优性。


<details>
  <summary>Details</summary>
Motivation: 研究在聚合bandit反馈模型下的在线学习问题，该模型下学习者只能观测到每个episode的累计损失而非每个状态-动作对的个体损失。现有工作专注于最坏情况分析，本文旨在开发能在随机和对抗环境中都表现良好的BOBW算法。

Method: 结合了基于占用度量的FTRL、自约束技术以及受在线最短路径问题启发的新损失估计器。对于未知转移设置，还融入了基于置信度的技术。

Result: 在已知转移设置下，算法在随机环境中达到O(log T)遗憾，在对抗环境中达到O(√T)遗憾，并建立了匹配的下界证明最优性。同时为最短路径问题提供了首个个体间隙依赖下界和近最优BOBW算法。

Conclusion: 成功开发了聚合bandit反馈下表格MDP的首个BOBW算法，在随机和对抗环境中都实现了最优性能，填补了该领域的研究空白。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [259] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 本文揭示了Transformer编码器与图卷积网络(GCN)的基本等价性，提出Fighter架构，在保持竞争力的同时提供更清晰的机制可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在时间序列建模中取得了显著成功，但其内部机制仍然不透明，需要揭示其工作原理。

Method: 建立Transformer编码器与GCN的等价性，将注意力分布矩阵视为动态邻接矩阵，提出Fighter架构，去除冗余线性投影并引入多跳图聚合。

Result: 在标准预测基准测试中，Fighter实现了有竞争力的性能，同时提供了更清晰的预测机制可解释性。

Conclusion: Transformer编码器本质上等同于图卷积网络，这一统一理论重新解释为时间序列建模提供了更明确和可解释的表示。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [260] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出了一种基于矩阵自由能的自动编码器正则化方法，通过优化代码矩阵的奇异值分布来生成高斯分布代码


<details>
  <summary>Details</summary>
Motivation: 传统自动编码器缺乏对代码分布的正则化约束，需要一种能够确保代码呈现高斯分布的方法来提升泛化能力

Method: 基于矩阵自由能定义可微损失函数，通过随机矩阵理论优化代码矩阵的奇异值分布，使其与独立同分布高斯随机矩阵的奇异值分布一致

Result: 经验模拟表明，通过随机梯度下降最小化负矩阵自由能可以产生高斯分布代码，在训练集和测试集上都具有良好的泛化性能

Conclusion: 该方法能够可靠地生成高斯分布代码，并在欠定逆问题中展示了应用潜力

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [261] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: LLMs在发现和推理用户潜在信息方面存在局限性，特别是在需要个性化偏好的场景中。研究提出了一个统一的基准来评估LLMs通过多轮对话揭示隐藏用户属性的能力。


<details>
  <summary>Details</summary>
Motivation: LLMs擅长生成通用文本，但在需要用户特定偏好的场景中，这种通用性成为限制。用户很少明确表达所有偏好，大量信息保持潜在状态，需要通过对话来推断。

Method: 引入统一基准评估潜在信息发现能力，涵盖三个逐步现实化的设置：20个问题游戏、个性化问答和个性化文本摘要。采用三智能体框架（用户、助手、评判者）进行轮级评估。

Result: LLMs确实可以通过对话揭示潜在信息，但成功率差异显著（32%-98%），取决于任务复杂性、主题和隐藏属性数量。

Conclusion: 该基准为研究个性化交互中的潜在信息发现提供了首个系统框架，表明有效的偏好推断仍然是构建真正自适应AI系统的开放前沿。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [262] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出In-situ Autoguidance方法，无需辅助模型即可实现图像生成扩散模型的自引导，在推理时通过随机前向传递动态生成次优预测，实现零成本的自校正引导。


<details>
  <summary>Details</summary>
Motivation: 解决分类器自由引导(CFG)方法在提高图像质量和提示对齐时导致多样性下降的问题，同时避免现有解耦方法需要额外训练辅助模型的开销。

Method: 在推理时使用随机前向传递动态生成次优预测，将引导重新定义为推理时的自校正过程，无需任何辅助组件。

Result: 该方法不仅可行，而且为成本高效的引导建立了强大的新基准，证明无需外部模型即可实现自引导的益处。

Conclusion: In-situ Autoguidance是一种零成本的自引导方法，成功实现了图像生成质量、多样性和提示对齐的更好平衡，无需额外模型开销。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [263] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为ALMD的新学习范式，使模型在部署后能够自主检测未见类别样本并在标注后学习，无需人工工程师参与。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习在部署后模型固定，无法适应动态开放环境中出现的未见类别样本。需要模型能够自主检测并学习新类别。

Method: 提出了PLDA方法，能够动态进行OOD检测和增量学习新类别，无需从头重新训练模型。

Result: 经验评估将展示PLDA方法的有效性。

Conclusion: ALMD范式解决了动态环境中模型持续学习的问题，PLDA方法为这一挑战提供了有效解决方案。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [264] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE是一个轻量级自适应框架，让终端设备能实时调整差分隐私级别，在移动边缘众感知系统中平衡隐私保护、数据效用和能耗成本。


<details>
  <summary>Details</summary>
Motivation: 移动边缘众感知系统在动态资源受限环境中持续传输用户数据，面临严重隐私威胁。静态差分隐私机制无法适应不断变化的风险（如对手能力变化、资源约束和任务需求），导致要么噪声过大要么保护不足。

Method: ALPINE作为闭环控制系统，包含四个模块：动态风险感知、基于TD3算法的隐私决策、本地隐私执行和边缘节点性能验证。设计了平衡隐私收益、数据效用和能耗成本的奖励函数，指导TD3代理在不同风险场景下自适应调整噪声幅度。

Result: 理论分析和真实世界仿真表明，ALPINE能有效缓解推理攻击，同时保持效用和成本，适用于大规模边缘应用。协作风险模型和预训练的TD3代理设计为低开销部署。

Conclusion: ALPINE框架通过自适应调整差分隐私级别，在移动边缘众感知系统中实现了隐私、效用和成本之间的动态平衡，为大规模边缘应用提供了实用的隐私保护解决方案。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [265] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 提出了一个统一的框架来评估文本属性图(TAG)学习中GNNs和LLMs的鲁棒性，揭示了模型在文本和结构扰动下的内在权衡，并提出了SFT-auto框架来提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对图神经网络(GNNs)和大语言模型(LLMs)在文本属性图(TAGs)上的鲁棒性评估是碎片化的，缺乏对文本和结构扰动的系统性研究。

Method: 引入统一框架评估经典GNNs、鲁棒GNNs(RGNNs)和GraphLLMs在10个数据集上的表现，涵盖文本、结构和混合扰动，以及投毒和规避攻击场景。

Result: 发现模型在文本和结构鲁棒性之间存在内在权衡，GNNs和RGNNs的性能高度依赖文本编码器和攻击类型，GraphLLMs对训练数据污染特别脆弱。

Conclusion: 提出的SFT-auto框架能在单一模型中实现对文本和结构攻击的优越且平衡的鲁棒性，为TAG安全研究奠定基础。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [266] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出了一个模块化的分子动力学基准测试框架，使用增强采样分析系统评估蛋白质MD方法，支持多种模拟引擎和机器学习模型，包含19+种评估指标和可视化工具。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法快速发展，但缺乏标准化的验证工具，不同模拟方法之间的客观比较受到评估指标不一致、稀有构象状态采样不足和可重复基准缺失的阻碍。

Method: 使用加权集合采样和基于TICA的进度坐标，通过WESTPA实现快速高效的蛋白质构象空间探索；提供灵活的传播器接口支持任意模拟引擎；包含全面的评估套件。

Result: 贡献了包含9种不同蛋白质的数据集，每个蛋白质在300K下进行了大量模拟；验证测试显示框架能够有效比较经典MD模拟和不同训练状态的机器学习模型。

Conclusion: 该开源平台通过标准化评估协议和实现直接、可重复的MD方法比较，为分子模拟社区的一致、严格基准测试奠定了基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [267] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: SOLE是一个软硬件协同设计，通过E2Softmax和AILayerNorm优化Transformer中的Softmax和LayerNorm操作，在不重新训练的情况下保持推理精度，同时显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在NLP和CV任务中表现出色，但其实时推理速度和效率受到Softmax和LayerNorm低效性的限制。现有基于函数逼近的方法存在实现效率低、内存开销大、需要重新训练等问题。

Method: 提出SOLE软硬件协同设计：E2Softmax使用log2量化的指数函数和基于对数的除法来逼近Softmax；AILayerNorm采用低精度统计计算。实现Softmax和LayerNorm的低精度计算和低比特存储。

Result: 实验显示SOLE在不重新训练的情况下保持推理精度，相比GPU实现数量级的速度提升和能耗节省，相比现有最优定制硬件，Softmax和LayerNorm分别实现3.04x、3.86x能效提升和2.82x、3.32x面积效率提升。

Conclusion: SOLE通过软硬件协同设计有效解决了Transformer中Softmax和LayerNorm的效率瓶颈，在保持精度的同时显著提升了推理性能和能效。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [268] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出软掩码（Soft-Masking）方法改进扩散语言模型，通过动态混合掩码标记与预测标记的嵌入，保留更多预测信息，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散模型在解码时存在二元选择问题：要么保留掩码，要么替换为预测标记。这种二元选择在保留掩码时会丢弃有价值的预测信息。

Method: 引入软掩码方法，动态混合掩码标记嵌入与前一步解码中top-k预测标记的嵌入，为模型提供更丰富的先验信息，允许部分信息在多步间传播。

Result: 在169M参数模型上继续预训练SM方法，困惑度和MAUVE得分均有提升。在Dream-7B和Dream-Coder-7B模型上微调，在多个编程基准测试中性能一致提升，特别是在高吞吐量场景下。

Conclusion: 软掩码方法有效解决了传统掩码扩散模型的局限性，通过保留更多预测信息显著提升了语言模型的性能。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [269] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出一个强化学习框架，用于处理高风险高回报任务，通过离散化连续动作空间、熵正则化探索和双评论家架构来建模多模态动作分布和风险。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法假设单峰高斯策略和标量值评论家，在处理高风险高回报任务时效果有限，因为这类任务通常具有多模态动作分布和随机回报。

Method: 1) 离散化连续动作空间以近似多模态分布；2) 使用熵正则化探索来覆盖高风险但高回报的动作；3) 引入双评论家架构进行更准确的离散值分布估计。

Result: 在具有高失败风险的移动和操作基准测试中，该方法优于基线方法，证明了显式建模多模态和风险的重要性。

Conclusion: 该框架能够扩展到高维动作空间，支持复杂控制领域，为高风险高回报任务提供了有效的解决方案。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [270] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 使用深度稀疏自编码网络预测和分类燃料电池高频阻抗，准确率超92%，并在FPGA上部署实现近90%的硬件识别率。


<details>
  <summary>Details</summary>
Motivation: 燃料电池健康状态的有效准确诊断对确保电堆稳定运行至关重要，高频阻抗是评估燃料电池状态和健康状况的关键指标，但其在线测试复杂且成本高昂。

Method: 采用深度稀疏自编码网络进行燃料电池高频阻抗的预测和分类。

Result: 实现了准确率超过92%的预测和分类性能，并在FPGA上部署后获得接近90%的硬件识别率。

Conclusion: 深度稀疏自编码网络能够有效预测燃料电池高频阻抗，且可在硬件平台上实现高效部署，为燃料电池健康状态在线监测提供了可行方案。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [271] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的编码器（AttEnc）和结合原型网络的P-AttEnc架构，用于驾驶员识别，解决了数据短缺问题并能识别未知驾驶员。


<details>
  <summary>Details</summary>
Motivation: 传统基于生物特征的驾驶员识别技术存在隐私问题，且现有方法面临数据短缺和无法处理未知驾驶员的局限性。

Method: 使用注意力机制的编码器减少模型参数，结合原型网络实现少样本学习，解决数据短缺问题并增强模型泛化能力。

Result: AttEnc在三个数据集上分别达到99.3%、99.0%和99.9%的识别准确率，预测时间加快44%-79%，模型参数平均减少87.6%。P-AttEnc在单样本场景下识别准确率为69.8%，对未知驾驶员的平均识别准确率为65.7%。

Conclusion: 提出的AttEnc和P-AttEnc架构在驾驶员识别任务中表现出色，既能高效识别已知驾驶员，又能处理数据短缺和未知驾驶员分类问题。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [272] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出自适应离散化一致性模型(ADCMs)，通过优化离散化步长实现自动离散化，提高训练效率和生成性能


<details>
  <summary>Details</summary>
Motivation: 现有一致性模型依赖手动设计的离散化方案，需要针对不同噪声调度和数据集反复调整，缺乏自适应性

Method: 将离散化建模为优化问题，以局部一致性为优化目标确保可训练性，全局一致性为约束保证稳定性，使用拉格朗日乘子和高斯-牛顿方法实现自适应离散化

Result: 在CIFAR-10和ImageNet上显著提高训练效率，获得更优的生成性能，且对更先进的扩散模型变体具有强适应性

Conclusion: ADCMs框架有效解决了手动离散化的问题，为一致性模型提供了高效的自适应离散化方案

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [273] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 提出了一种基于变分推断的扩展方法，将确定性机器学习方法扩展为预测状态遵循多元高斯分布的数据同化模型。


<details>
  <summary>Details</summary>
Motivation: 数据同化在大多数设置中都涉及不确定性，现有确定性方法无法充分处理这种不确定性。

Method: 在现有确定性机器学习方法基础上，提出变分推断扩展，使预测状态遵循多元高斯分布，使用混沌Lorenz-96动力学作为测试平台。

Result: 新模型能够获得近乎完美校准的预测，并且可以在更广泛的变化数据同化管道中集成，从而从增加的数据同化窗口长度中获得更大收益。

Conclusion: 提出的随机变分推断方法能够有效处理数据同化中的不确定性，提供校准良好的预测，并提升数据同化性能。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [274] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: ControlValve是一种防御多智能体系统中控制流劫持攻击的新方法，通过生成允许的控制流图并强制所有执行符合这些图来确保系统安全。


<details>
  <summary>Details</summary>
Motivation: 现有防御措施（如LlamaFirewall）依赖对齐检查来防止控制流劫持攻击，但这些方法存在根本性缺陷，因为安全性和功能性目标存在冲突，且对齐定义脆弱、检查器对执行上下文可见性不足。

Method: ControlValve基于控制流完整性和最小权限原则，生成多智能体系统的允许控制流图，并强制所有执行符合这些图，同时为零样本生成的每个智能体调用应用上下文规则。

Result: ControlValve能够有效防御控制流劫持攻击，即使这些攻击能够规避基于LLM的对齐检查。

Conclusion: 多智能体系统的安全防御需要超越简单的对齐检查，ControlValve通过控制流完整性和最小权限原则提供了一个更强大的防御框架。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [275] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了一个用户反馈模拟框架和综合基准测试，用于评估LLM系统的持续学习能力，发现现有方法的有效性远不令人满意。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据耗尽和计算资源边际收益递减，传统的数据、参数和测试时计算扩展方法已接近上限，需要从人类和传统AI系统的实践学习能力中汲取灵感。

Method: 开发用户反馈模拟框架和覆盖多领域、多语言、多任务类型的综合基准测试，用于评估LLM系统的持续学习能力。

Result: 实验表明，现有最先进基准方法的有效性和效率远未达到满意水平。

Conclusion: 该基准测试为未来LLM记忆和优化算法研究铺平了道路。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [276] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 本文扩展了机器学习中对称性的泛化保证，从紧致群对称性扩展到非紧致对称性（如平移），并放宽了数据分布不变性的假设，通过PAC-Bayes框架提供了更一般的理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要关注紧致群对称性且假设数据分布不变，这在现实应用中很少满足。本文旨在扩展对称性理论到更一般的非紧致对称性和非不变数据分布。

Method: 基于PAC-Bayes框架，改进和收紧现有边界，特别适用于McAllester的PAC-Bayes边界，但可广泛适用于各类PAC-Bayes边界。

Result: 在非均匀旋转群上的旋转MNIST数据集实验中，推导的泛化保证不仅成立，而且优于先前结果。

Conclusion: 对于对称数据，对称模型在超越紧致群和不变分布的更一般设置下仍具有优势，为理解机器学习中对称性提供了更广泛的理论基础。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [277] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了首个多因子时序解缠结标准化基准，包含六个数据集和评估工具，并引入了后验潜在探索阶段和Koopman模型，同时展示了视觉语言模型在自动标注和评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据包含多个相互作用的语义因子，但先前工作主要关注简单的双因子静态和动态设置，忽视了数据的多因子本质。

Method: 开发了多因子时序解缠结基准，包含数据集集成工具、模型开发框架和评估指标；提出了后验潜在探索阶段自动对齐潜在维度与语义因子；设计了Koopman启发模型；利用视觉语言模型进行自动标注和零样本评估。

Result: 提出的Koopman模型达到了最先进的性能，视觉语言模型能够有效自动化数据集标注并作为解缠结评估器，消除了人工标注和干预的需求。

Conclusion: 这些贡献为推进多因子时序解缠结研究提供了稳健且可扩展的基础。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [278] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出了一种无需训练、基于评估准则的奖励建模框架，通过两阶段方法实现数据高效和可解释性，仅需少量偏好数据即可超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖昂贵的偏好数据集且缺乏可解释性，现有基于准则的方法在可扩展性和可靠性之间存在权衡。

Method: 两阶段方法：1) 通过提议-评估-修订流程推断查询特定准则；2) 通过信息论编码率最大化将细粒度准则泛化为紧凑核心集，形成层次化的主题-提示准则集。

Result: 仅使用70个偏好对（源数据的1.5%），该方法使Qwen3-8B等小模型超越专门训练的对等模型，展现了卓越的数据效率和性能。

Conclusion: 这项工作开创了可扩展、可解释且数据高效的奖励建模路径，为LLM与人类价值观对齐提供了新思路。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [279] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出了一个新颖框架，通过可调节的局部化参数、信息论招募机制和分层招募框架，使大型语言模型能够在可解释的局部表示和高效的分布式表示之间连续调整内部表示。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型在可解释性和性能之间的权衡问题，为需要透明性和能力的监管领域应用提供支持，实现模型在训练和推理过程中无需重新训练即可动态调整表示方式。

Method: 使用组稀疏性惩罚注意力机制、信息论锚点设计、动态规则注入，以及基于惩罚似然的招募标准。通过局部化调节参数控制表示局部化程度，信息论招募机制自适应分配语义块，分层招募框架扩展到专门的LLM层级。

Result: 建立了严格的数学结果，证明了在平稳点处注意力会集中在语义相关块上的明确阈值条件，提供了注意力熵和指针保真度的精确界限。分层招募机制在块级和LLM级都提供了收敛保证。

Conclusion: 该框架使从业者能够在可解释模式和高性能模式之间连续插值，同时在多个粒度上适应架构容量，支持需要透明性和能力的监管领域应用。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [280] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络中的表示不变性问题，通过引入模型"metamers"生成技术，揭示了GNNs存在极端水平的表示不变性，与人类大脑的不变性机制存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络中的不变性行为，探索人工神经网络与人类大脑在不变性机制上的差异，特别是在图数据领域。

Method: 通过优化输入图使其内部节点激活与参考图匹配，生成在模型表示空间中等价但在结构和节点特征上显著不同的图（metamers），并分析局部metamer维度和metamer流形的激活诱导体积变化。

Result: 发现多个经典GNN架构存在极端水平的表示不变性，虽然通过调整模型架构和训练策略可以部分缓解这种过度不变性，但无法从根本上弥合与人类不变性的差距。

Conclusion: 量化了metamer图与其原始对应图之间的偏差，揭示了当前GNNs的独特失败模式，为模型评估提供了补充基准。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [281] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 使用物理信息神经网络(PINNs)构建替代模型来替代昂贵的智能电网模拟器，优化强化学习策略训练过程，显著提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 智能电网中的最优潮流问题面临现实系统复杂性和组件交互的挑战，强化学习需要大量环境迭代来获得最优策略，这导致样本效率问题，因为从昂贵的模拟器中获取样本成本很高。

Method: 用物理信息神经网络(PINNs)构建替代模型来替代昂贵的智能电网模拟器，优化强化学习策略训练过程。

Result: 该方法能在原始环境所用时间的一小部分内达到收敛结果。

Conclusion: PINNs作为替代模型能有效解决强化学习在智能电网优化中的样本效率问题。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [282] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 提出了DISC方法，利用扩散模型的去噪过程提取多维特征向量，不仅能检测OOD数据，还能区分OOD类型，超越了传统标量检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法将分布偏移压缩为单一标量异常分数，无法区分不同类型的OOD数据，限制了后续的适当处理和利用。

Method: 使用扩散模型的迭代去噪过程，在多个噪声水平上提取丰富的多维特征向量来捕捉统计差异。

Result: 在图像和表格基准测试中，DISC在OOD检测性能上达到或超过最先进方法，并首次实现了OOD类型的分类能力。

Conclusion: DISC实现了从简单二元OOD检测到更细粒度检测的转变，为OOD数据的上下文理解和潜在利用提供了新途径。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [283] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文探讨了生成视觉模型中内部表征的演变，分析了从GANs和VAEs到扩散模型的转变，提出了严格意义合成与广义合成的区分，认为扩散模型通过分层表征挑战了统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型内部表征的演变，特别是从GANs/VAEs到扩散模型的转变，旨在理解这些模型如何重新配置表征过程。

Method: 通过模型架构的详细分析和有针对性的实验设置，干预分层表征，展示扩散模型如何分散表征负担。

Result: 发现扩散模型将表征负担分散到不同层次，挑战了统一内部空间的假设，支持了广义合成的概念。

Conclusion: 生成AI应被理解为专门化过程的涌现配置，而非内容的直接合成，需要重新思考潜在空间和柏拉图表征假设等隐喻。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [284] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的推理大语言模型，通过创新的PRPO强化学习方法激活LLM的推理能力，在少样本和零样本场景下表现出色，甚至能超越更大规模的LLM。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升决策树和专用深度学习模型）虽然任务内表现优秀，但可解释性有限且跨表迁移能力弱。推理LLM具有跨任务适应性和透明推理轨迹的潜力，但在表格数据上的潜力尚未充分发掘。

Method: 提出TabR1模型，核心是Permutation Relative Policy Optimization (PRPO)方法。该方法通过为每个样本构造多个标签保持的列排列，并在排列内和跨排列估计优势，将稀疏奖励转化为密集学习信号，编码列排列不变性作为结构先验。

Result: TabR1在全监督微调下达到与强基线相当的性能。在零样本设置下，TabR1接近强基线在32样本设置下的性能。TabR1 (8B)在各种任务上显著优于更大的LLM，相比DeepSeek-R1 (685B)提升达53.17%。

Conclusion: PRPO方法能够有效激活LLM在表格预测中的推理能力，提升少样本和零样本性能以及可解释性，证明推理LLM在表格预测任务中具有巨大潜力。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [285] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出特征扰动技术，通过直接向特征输入注入随机性，在广义线性赌博机中实现最优理论保证，同时避免现有随机化方法的高计算复杂度和次优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有随机化赌博机算法通常存在计算复杂度高（需要参数采样）和理论遗憾界次优（通常为$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$）的问题，需要一种既高效又能达到最优理论保证的方法。

Method: 特征扰动技术：直接向特征输入注入随机性，而不是随机化未知参数或向奖励添加噪声。这种方法避免了参数采样，计算效率高，并能自然扩展到非参数或神经网络模型。

Result: 在广义线性赌博机中实现了$\tilde{\mathcal{O}}(d\sqrt{T})$的最坏情况遗憾界，优于现有随机化算法的$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$遗憾界。实证评估表明该方法不仅超越现有方法，而且将强大实际性能与最优理论保证统一起来。

Conclusion: 特征扰动是一种简单而强大的技术，通过直接扰动特征输入实现了计算效率和理论最优性的统一，为随机化赌博机算法提供了新的有效解决方案。

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [286] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出了首个针对弱通信MDP的平均奖励离线强化学习的样本复杂度结果，引入了锚定拟合Q迭代方法，结合了锚机制来支持有限时间分析。


<details>
  <summary>Details</summary>
Motivation: 现有关于平均奖励设置的研究较少，且依赖严格假设（如遍历性或MDP线性），需要更温和假设下的样本复杂度分析。

Method: 引入锚定拟合Q迭代，将标准拟合Q迭代与锚机制结合，锚机制可解释为权重衰减形式，支持平均奖励设置的有限时间分析。

Result: 建立了弱通信MDP下平均奖励离线RL的样本复杂度结果，锚机制对有限时间分析至关重要，并扩展到单轨迹数据集生成场景。

Conclusion: 锚机制是实现平均奖励离线RL有限时间分析的关键，为弱通信MDP提供了首个样本复杂度保证。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [287] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了MILES（模态感知学习率调度器），通过动态调整学习率来平衡多模态学习，解决模态过拟合问题，提升多模态和单模态预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中常出现模态过拟合问题，即网络过度依赖某一模态，导致性能不佳，未能充分发挥多模态学习的潜力。

Method: MILES利用训练过程中模态条件利用率的差异，动态调整学习率，平衡各模态的学习速度，实现均衡的多模态学习。

Result: 在四个多模态联合融合任务上评估，MILES优于七个最先进的基线方法，在所有任务和融合方法中表现最佳，有效平衡了模态使用。

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES不仅能改善多模态性能，还能产生更强的模态编码器，适用于单模态样本或缺失模态的情况。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [288] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: RINS-T是一个无需预训练数据的鲁棒隐式神经求解器，用于解决时间序列线性逆问题，通过集成鲁棒优化技术来抵抗异常值并减少对高斯噪声假设的依赖。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常受缺失值、噪声和异常值等污染，传统深度学习方法需要大量预训练且难以应对分布偏移，因此需要开发无需预训练且对异常值鲁棒的解决方案。

Method: 利用神经网络作为隐式先验，集成鲁棒优化技术，并引入三个关键创新：引导输入初始化、输入扰动和凸输出组合技术来增强优化稳定性和鲁棒性。

Result: RINS-T在无需预训练数据的情况下实现了高恢复性能，对异常值具有鲁棒性，且不依赖高斯噪声假设。

Conclusion: RINS-T是一个灵活有效的解决方案，能够应对复杂现实世界时间序列挑战，其代码已开源。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [289] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: S4ECG是一种基于结构化状态空间模型的深度学习架构，用于多时段心律失常分类，通过联合多时段预测显著提升了性能，特别是在心房颤动检测方面。


<details>
  <summary>Details</summary>
Motivation: 传统心电图分析方法难以同时捕捉全局趋势和局部波形特征的高时间分辨率交互，需要一种能桥接全局和局部信号分析的新方法。

Method: 引入S4ECG深度学习架构，利用结构化状态空间模型进行多时段心律失常分类，通过联合多时段预测来提升性能。

Result: 多时段预测方法比单时段方法在宏观AUROC上提升1.0-11.6%，心房颤动特异性从0.718-0.979提升到0.967-0.998，在分布内和分布外都表现出优越的鲁棒性。

Conclusion: 这项工作推动了时间感知心律失常检测算法的范式转变，为心电图解读特别是复杂心律失常如心房颤动和心房扑动开辟了新可能性。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [290] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 提出了一种名为CDUA的新方法，结合特征工程和深度学习，用于准确预测锂离子电池容量及其不确定性，在真实车辆数据上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池容量及其不确定性的准确预测对于可靠的电池管理至关重要，但由于老化的随机性，这一任务仍然具有挑战性。

Method: 使用基于扩散的生成模型进行时间序列预测，结合注意力机制提升性能。首先从真实车辆运行数据推导电池容量，然后使用皮尔逊相关系数和XGBoost算法识别最相关特征，最后训练CDUA模型。

Result: 在真实车辆数据上的实验验证显示，CDUA模型实现了0.94%的相对MAE和1.14%的相对RMSE，95%置信区间相对宽度为3.74%。

Conclusion: CDUA能够提供准确的容量估计和可靠的不确定性量化，比较实验进一步验证了其相对于现有主流方法的鲁棒性和优越性能。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [291] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出了DAP方法，利用扩散模型的固有代表性先验来指导数据集蒸馏过程，无需重新训练即可提升蒸馏样本的代表性，在ImageNet-1K等大规模数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决当前生成式数据集蒸馏方法忽视扩散模型固有代表性先验的问题，这些方法通常需要额外约束来提升数据质量。

Method: 提出DAP方法，通过Mercer核量化合成数据与真实数据在特征空间的相似度，将此先验作为指导来引导反向扩散过程。

Result: 在ImageNet-1K及其子集上的大量实验表明，DAP在生成高保真数据集方面优于最先进方法，并实现了更好的跨架构泛化性能。

Conclusion: DAP不仅建立了扩散先验与数据集蒸馏目标之间的理论联系，还提供了一个无需训练即可提升蒸馏数据集质量的实用框架。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [292] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出GBN网络，通过局部瓶颈调整解决MPNN中的过度平滑和过度挤压问题，在深度超过256层时仍能保持性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用全局方法解决过度平滑和过度挤压问题，但这些方法在某些区域有益但在其他区域有害，导致表达能力欠佳

Method: 通过局部黎曼几何与MPNN连接，建立新的非齐次边界条件，基于Robin条件设计具有局部瓶颈调整的GBN网络

Result: 在同质性和异质性图上的广泛实验显示GBN具有强大的表达能力，且在深度超过256层时不会出现性能下降

Conclusion: 局部方法比全局方法更有效地解决MPNN中的过度平滑和过度挤压问题，GBN网络在深度图神经网络中表现出色

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [293] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 将可解释AI技术应用于PhaseNet地震检测模型，通过Grad-CAM和SHAP解释模型决策，并开发SHAP门控推理方案提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络如PhaseNet在微地震事件检测中精度高，但其黑盒特性在关键应用中存在担忧，需要提高模型的可解释性和可靠性。

Method: 应用Grad-CAM和SHAP解释PhaseNet模型决策，Grad-CAM识别网络关注区域，SHAP量化特征贡献，并开发SHAP门控推理方案结合模型输出和解释指标。

Result: 在9,000个波形测试集上，SHAP门控模型F1分数达0.98（精度0.99，召回率0.97），优于基准PhaseNet（F1分数0.97），对噪声具有更强鲁棒性。

Conclusion: 可解释AI不仅能解释深度学习模型，还能直接提升其性能，为构建可信的自动化地震检测器提供了模板。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [294] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: CrossStateECG是一个专门针对静息-运动跨状态条件的ECG生物认证模型，结合多尺度深度卷积特征提取和注意力机制，在静息到运动和运动到静息场景下分别达到92.50%和94.72%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前ECG生物认证研究主要关注静息状态，对静息-运动场景下的性能下降问题研究不足，需要开发能够适应不同生理状态的鲁棒认证模型。

Method: 提出CrossStateECG模型，创新性地结合多尺度深度卷积特征提取和注意力机制，确保在不同生理状态下都具有强识别能力。

Result: 在exercise-ECGID数据集上验证，静息到运动场景准确率92.50%，运动到静息场景94.72%，静息到静息场景99.94%，混合到混合场景97.85%。在ECG-ID和MIT-BIH数据集上进一步验证了泛化能力。

Conclusion: CrossStateECG在动态现实环境中具有作为运动后ECG认证实用解决方案的潜力，能够有效解决跨状态认证的性能下降问题。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [295] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: Transformers通过训练发展出模块化、可解释的机制来支持组合推理，这些机制与观察到的行为能力相关。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在未训练序列上表现出的组合推理能力，探索其内部机制如何支持这种泛化能力。

Method: 使用随机层次模型（RHM）作为概率上下文无关语法生成序列，在序列子集上训练模型，并在四个泛化条件下评估：记忆、分布内泛化、分布外泛化和跨层迁移。

Result: 性能随任务复杂性和上下文示例数量系统性提升，分布外任务需要更多示例；训练中出现渐进性层专业化，与泛化性能相关；Transformer在专门层中发展出结构化、层次化组织的表示。

Conclusion: Transformer发展出支持组合推理的模块化、可解释机制，将内部算法结构与观察到的行为能力联系起来。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [296] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: 提出了分布感知多源域自适应网络(DAMSDAN)，通过动态建模源域分布异质性和细粒度语义对齐，解决跨域EEG情感识别的个体差异问题。


<details>
  <summary>Details</summary>
Motivation: 个体间显著差异限制了EEG情感识别在跨域设置下的泛化能力，需要解决多源域适应中的分布异质性建模和语义一致性保持问题。

Method: 结合原型约束与对抗学习，使用基于MMD的域感知源加权策略动态估计域间偏移，并通过原型引导的条件对齐模块增强伪标签可靠性。

Result: 在SEED和SEED-IV数据集上，跨被试准确率分别达到94.86%和79.78%，跨会话准确率分别达到95.12%和83.15%；在FACED数据集上跨被试准确率为82.88%。

Conclusion: DAMSDAN框架通过分布感知和细粒度语义对齐，有效提升了跨域EEG情感识别的性能，消融实验和可解释性分析验证了其有效性。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [297] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 该研究探索了使用生成对抗网络(GAN)进行河流沉积物建模，并通过四种反演方法匹配井数据和地震数据。研究发现GAN的潜在表示存在纠缠问题，通过标签条件化或潜在过参数化可部分解决，但需要微调GAN来重构潜在空间才能达到可接受的匹配水平。


<details>
  <summary>Details</summary>
Motivation: 地下决策成本高且不确定性大，获取新数据难以扩展。将地质知识直接嵌入预测模型提供了有价值的替代方案，过程模型可以帮助训练生成模型以提高预测效率。

Method: 使用生成对抗网络(GAN)训练河流沉积物生成模型，应用四种反演方法匹配井和地震数据，通过标签条件化、潜在过参数化和GAN微调来改善潜在空间表示。

Result: 在4、8和20口井的三个测试样本中，反演方法难以匹配井数据，特别是当井数增加或测试样本与训练数据差异较大时。通过微调GAN重构潜在空间，可以将不匹配降低到所有测试案例的可接受水平。

Conclusion: GAN已经能够处理地质建模工作流集成所需的任务，但仍需进一步评估其鲁棒性，以及如何最好地利用它们支持地质解释。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [298] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文提出了MAFALDA-SGD算法，通过矩阵分解方法改进去中心化学习中的差分隐私计算，提供更紧密的隐私保障并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前去中心化学习中的差分隐私方法在实践中观察到的隐私-效用权衡往往比集中式训练更差，这可能是由于现有的DP计算方法存在局限性。

Method: 通过推广现有的矩阵分解结果，将标准DL算法和常见信任模型统一到一个框架中，并提出了MAFALDA-SGD算法——一种基于gossip的去中心化学习算法，具有用户级相关噪声。

Result: 该方法为现有的DP-DL算法提供了更紧密的隐私计算，并在合成和真实世界图上优于现有方法。

Conclusion: 矩阵分解方法可以有效地应用于去中心化学习中的差分隐私计算，提供更好的隐私-效用权衡。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [299] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 提出通过层间Hessian矩阵分析神经网络的方法，研究参数空间的局部几何特性与泛化性能的关系


<details>
  <summary>Details</summary>
Motivation: 需要一种形式化工具来表征神经网络参数空间的局部几何特性，以理解过拟合、欠参数化和表达能力等量化模式

Method: 定义每个功能块(层)的局部Hessian矩阵作为标量函数对该层参数的二阶导数矩阵，分析其谱特性如特征值分布

Result: 在37个数据集上的111个实验显示，局部Hessian在训练过程中呈现一致的结构规律，其谱特性与泛化性能存在相关性

Conclusion: 局部几何分析为诊断和设计深度神经网络提供了基础，将优化几何与功能行为联系起来，对改进网络架构和训练稳定性具有实践意义

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [300] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个符号推理基准，用于评估大语言模型和大推理模型在类比和数学推理中的泛化性和鲁棒性。相比I-RAVEN，它增加了操作数复杂度、属性范围并引入了感知不确定性。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型和大推理模型在复杂推理任务中的泛化能力和鲁棒性，特别是在面对不确定性时的表现。

Method: 扩展I-RAVEN基准，增加操作数复杂度、扩大属性范围，并引入感知不确定性来创建更具挑战性的测试环境。

Result: 大推理模型在处理长推理关系和宽属性范围时表现出更好的生产性和系统性，但在不确定性推理方面仍有显著挑战，无法有效探索多个概率结果。

Conclusion: 大推理模型在复杂推理任务上有所改进，但在处理不确定性方面仍有局限，需要进一步研究以提升其概率推理能力。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [301] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出基于图极限理论的新框架，用图论分析稀疏神经网络的训练动态，解释了不同剪枝方法收敛行为差异的原因。


<details>
  <summary>Details</summary>
Motivation: 稀疏神经网络训练效果差异显著，但缺乏系统理论解释。需要理解相同稀疏度下不同结构可训练性差异的根本原因。

Method: 基于图极限理论（特别是图论），提出图论极限假设，推导图论神经正切核来分析无限宽度极限下稀疏网络的训练动态。

Result: 图论NTK的谱分析与稀疏网络实际训练动态相关，成功解释了不同剪枝方法的收敛行为差异。

Conclusion: 该框架为稀疏网络架构的可训练性提供了理论洞察，揭示了连接模式对训练效果的重要影响。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [302] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 该论文分析了安全强化学习中拉格朗日乘子的最优性和稳定性，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域中，拉格朗日方法的效果取决于乘子λ的选择，但缺乏关于自动更新鲁棒性的实证证据。

Method: 分析拉格朗日乘子的最优性和稳定性，提供λ-剖面图可视化回报与约束成本之间的权衡，并研究自动乘子更新和PID控制方法。

Result: λ具有高度敏感性，自动乘子更新能够恢复最优性能，但存在振荡行为；PID控制需要仔细调参才能获得更好性能。

Conclusion: 拉格朗日方法在安全强化学习中需要进一步研究以提升稳定性。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [303] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出了SAFE-D框架，用于检测帕金森病相关的驾驶行为异常，通过多源数据整合和注意力网络实现96.8%的异常检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注功能驱动的暂时性异常（如困倦、分心），但对病理触发的慢性医疗条件导致的驾驶行为异常研究有限，特别是帕金森病这类慢性疾病。

Method: 分析帕金森病症状学，建立与驾驶性能下降的因果关系；整合多个车辆控制组件数据构建行为档案；设计基于注意力的网络自适应优先处理时空特征。

Result: 在Logitech G29平台和CARLA模拟器上验证，使用三个道路地图模拟真实驾驶，SAFE-D在区分正常和帕金森病影响驾驶模式方面达到96.8%的平均准确率。

Conclusion: SAFE-D框架能有效检测帕金森病相关的驾驶行为异常，为提升驾驶安全提供了新方法，特别是在处理慢性医疗条件导致的驾驶风险方面。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [304] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出CEPerFed方法，通过客户端历史风险梯度和历史平均梯度协调局部与全局优化，并使用分层SVD策略降低通信开销，解决多脉冲MRI分类中的联邦学习数据异构性和通信效率问题。


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI在临床诊断中广泛应用，但训练鲁棒模型需要大量多样化数据且需保护隐私。联邦学习虽可行，但面临数据异构性导致的模型收敛问题和大量参数传输的通信开销挑战。

Method: CEPerFed方法：1) 使用客户端历史风险梯度加权其他客户端贡献，增强局部更新可靠性；2) 使用历史平均梯度确保局部更新与全局优化方向一致；3) 采用分层SVD策略仅传输模型更新所需的关键信息。

Result: 在五个分类任务上的实验证明了CEPerFed方法的有效性。

Conclusion: CEPerFed通过协调局部与全局优化及高效通信策略，成功解决了联邦学习在多脉冲MRI分类中的数据异构性和通信开销问题。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [305] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将多标签学习建模为合作博弈，通过好奇心奖励机制解决长尾不平衡问题，在罕见标签上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多标签学习中存在长尾不平衡问题：少数头部标签主导梯度信号，而实践中重要的许多罕见标签被忽略。

Method: 将标签空间分配给多个合作玩家，共享全局准确度收益，同时根据标签稀有度和玩家间分歧获得额外好奇心奖励，无需手动调整类别权重。

Result: 在常规基准和三个超大规模数据集上的实验显示，相比最强基线获得+4.3% Rare-F1和+1.6% P@3的持续SOTA提升。

Conclusion: CD-GTMLL为多标签预测中的长尾鲁棒性提供了一条原则性、可扩展的路径。

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [306] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出了一种级联方法，将预训练OVD模型与轻量级少样本分类器结合，通过FLAME主动学习策略选择信息量最大的样本进行训练，实现快速适应和高效检测。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇目标检测模型在遥感等专业领域中由于自然语言歧义导致的细粒度类别区分困难问题，满足特定用户需求如非法捕鱼监测。

Method: 使用零-shot模型生成高召回率目标提议，然后通过基于少量用户标注样本训练的紧凑分类器进行精炼；核心是FLAME主动学习策略，通过密度估计和聚类选择边界附近的不确定样本。

Result: 在遥感基准测试中持续超越最先进方法，实现高精度检测，适应时间不到一分钟，显著快于现有替代方案。

Conclusion: 建立了一个实用且资源高效的框架，使基础模型能够快速适应特定用户需求，大幅降低遥感图像标注成本。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [307] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出了Counterfactual Knowledge Distillation (CFKD)框架，通过生成多样反事实样本来解决深度学习模型对伪相关性的脆弱性问题，无需组标签即可实现跨组平衡泛化。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到伪相关性的影响，现有的组分布鲁棒性方法依赖显式组标签且在小样本情况下效果不佳，特别是在多个伪相关性将数据分割成更小组时性能急剧下降。

Method: CFKD框架通过生成多样反事实样本，让人类标注者能够高效探索和修正模型决策边界，通过知识蒸馏步骤不仅重加权欠采样组，还为其丰富新的数据点。

Result: 在五个数据集上验证了CFKD的有效性，从合成任务到工业应用，在低数据量且存在明显伪相关性的情况下表现尤为突出。

Conclusion: CFKD方法无需混淆变量标签，能有效扩展到多个混淆变量，实现跨组的平衡泛化，并通过消融研究展示了反事实解释器和教师模型对鲁棒性的影响。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [308] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出语言在环框架，使用大语言模型将自然语言反馈转换为标量效用，用于在数值搜索空间上进行贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，反馈对于将复杂、细微或主观目标转化为可量化的优化目标至关重要。现有方法如偏好贝叶斯优化只能接受受限反馈格式，且需要为每个领域特定问题定制模型。

Method: 使用大语言模型将各种类型的文本反馈转换为一致的效用信号，无需手动设计核函数即可包含灵活的用户先验，同时保持贝叶斯优化的样本效率和原则性不确定性量化。

Result: 该混合方法不仅为决策者提供了更自然的接口，而且在反馈受限的情况下优于传统贝叶斯优化基线和仅使用LLM的优化器。

Conclusion: 语言在环框架通过结合大语言模型和贝叶斯优化，有效解决了将自然语言反馈转化为优化目标的问题，在反馈有限的情况下表现优异。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [309] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 在低信噪比(SNR)数据中，通过向梯度下降训练引入标签噪声可以抑制噪声记忆化，改善神经网络泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易在训练中记忆噪声，特别是在低信噪比数据中，这会损害泛化能力。受标签噪声具有正则化效果的启发，研究是否可以通过引入标签噪声来提升神经网络在低SNR环境下的测试性能。

Method: 采用两层神经网络，在理想化的信号-噪声数据设置中，使用带有标签噪声的梯度下降算法进行训练。

Result: 标签噪声GD能够抑制噪声记忆化，防止噪声主导学习过程，实现快速信号增长同时控制过拟合，从而在低SNR下获得良好泛化。相比之下，标准GD倾向于过拟合噪声，测试误差存在非零下界。

Conclusion: 在基于梯度的训练中引入标签噪声对低信噪比数据具有显著益处，能够有效改善神经网络的泛化性能。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [310] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出了一种新的双层次强化学习框架，通过基于真实世界性能直接调整模拟器参数来解决Sim2Real性能差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Sim2Real方法通过优化模拟器精度和变异性作为真实世界性能的代理指标，但这些指标与策略在真实世界中的性能不一定相关，导致模拟训练的策路在真实部署时性能显著下降。

Method: 采用双层次RL框架：内层RL在模拟中训练策略，外层RL调整模拟模型和模拟内奖励参数，以最大化模拟策略在真实世界中的性能。

Result: 推导并验证了开发能够缩小Sim2Real性能差距的双层次RL算法所需的数学工具。

Conclusion: 该框架通过直接基于真实世界性能优化模拟器参数，为解决Sim2Real性能差距问题提供了新的有效方法。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [311] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种基于保形对齐的级联机制(CAb)，用于边缘-云模型级联系统，保证边缘预测集在条件覆盖概率上达到云模型水平，同时减少向云端的卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘智能虽然能通过紧凑的本地模型实现低延迟推理，但保证可靠性仍然具有挑战性。需要确保边缘模型在返回预测集时，能以用户指定的概率包含真实标签，就像云模型产生的一样。

Method: 将边缘到云端的升级建模为多重假设检验问题，采用保形对齐技术来选择哪些输入可以在边缘安全处理。提出的CAb模型级联方法对边缘决策满足云级条件覆盖的平均比例提供统计保证。

Result: 在CIFAR-100图像分类和TeleQnA问答基准测试中，CAb级联保持了目标条件覆盖，同时显著减少了向云端的卸载，预测集大小仅有适度增加。

Conclusion: CAb级联方法能够有效平衡覆盖概率、延迟率和预测集大小之间的权衡，为边缘智能系统提供了可靠的统计保证。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [312] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 本研究开发了物理信息神经网络策略，将海冰物理知识整合到机器学习模型中，改进了北极海冰速度和浓度的预测性能。


<details>
  <summary>Details</summary>
Motivation: 完全数据驱动的机器学习模型在泛化性和物理一致性方面存在局限，特别是在北极海冰变薄和加速融化的新阶段，历史数据训练的模型可能无法充分代表未来动态变化的海冰条件。

Method: 基于分层信息共享U-net架构，结合物理损失函数和激活函数，开发物理信息神经网络模型来产生物理上合理的海冰速度和浓度输出。

Result: PINN模型在海冰速度和浓度的日预测中优于完全数据驱动的模型，即使在少量样本训练下也表现良好，特别是在融化和早期冻结季节以及快速移动冰区显著改善了海冰浓度预测。

Conclusion: 物理信息神经网络方法能够有效整合物理知识，提高海冰预测的准确性和物理一致性，特别是在数据稀缺和动态变化条件下表现优异。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [313] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba是一个用于车辆GPS轨迹学习的新方法，通过联合建模GPS和道路视角来捕获运动模式，集成旅行目的到嵌入中，并通过知识蒸馏减少轨迹冗余，在效率和准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 车辆GPS轨迹包含有价值的旅行语义，但现有方法面临两个主要挑战：旅行目的与道路功能和POI相关，这些信息编码在文本地址和描述中，给建模带来沉重计算负担；真实轨迹常包含冗余点，影响计算效率和嵌入质量。

Method: 提出TrajMamba方法，包括Traj-Mamba编码器联合建模GPS和道路视角，旅行目的感知预训练将旅行目的集成到嵌入中，以及知识蒸馏预训练通过可学习掩码生成器识别关键轨迹点来压缩轨迹嵌入。

Result: 在两个真实世界数据集和三个下游任务上的广泛实验表明，TrajMamba在效率和准确性上都优于最先进的基线方法。

Conclusion: TrajMamba能够有效且高效地学习车辆轨迹的语义信息，解决了轨迹数据建模中的计算负担和冗余问题，为轨迹数据的实际应用提供了有力支持。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [314] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出了一个样本级别的框架来量化后训练过程中的知识遗忘和逆向迁移，发现不同后训练阶段对预训练知识的影响各不相同，模型融合不能可靠缓解遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 规模化后训练虽然显著提升了语言模型能力，但其对预训练知识的影响尚不明确，传统任务平均指标无法准确衡量知识遗忘和逆向迁移。

Method: 提出样本级度量方法，通过1->0转换量化遗忘，0->1转换量化逆向迁移；对于多选题基准，使用机会调整变体来消除随机猜测的影响。

Result: 大规模分析显示：领域持续预训练导致中度遗忘和低至中度逆向迁移；RL/SFT后训练在数学和逻辑任务上产生中至高度逆向迁移；模型融合不能可靠缓解遗忘。

Conclusion: 该框架为评估后训练如何改变预训练知识提供了实用标准，有助于开发更通用的AI系统。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [315] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种扩展的解码器Transformer，在生成过程中引入随机潜变量，通过变分方法无监督学习这些变量，从而在下游任务中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了增强解码器Transformer的生成能力，使其能够通过无监督学习的方式利用随机潜变量来改善生成过程。

Method: 扩展解码器Transformer架构，引入随机潜变量进行条件生成，采用变分方法无监督地学习这些潜变量。

Result: 实验评估表明，这种条件生成方法在下游任务中带来了显著的性能改进。

Conclusion: 通过在解码器Transformer中引入无监督学习的随机潜变量进行条件生成，可以有效提升模型在下游任务中的表现。

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [316] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 提出了LARM和ALARM两个时间序列异常检测评估指标，能够满足所有验证属性，解决了现有指标不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的未检测异常可能导致安全关键系统的灾难性故障，现有评估指标只捕捉任务的狭窄方面且经常产生误导性结果。

Method: 引入可验证属性来形式化评估时间序列异常检测的基本要求，建立理论框架支持原则性评估和可靠比较。

Result: 分析了37个广泛使用的指标，发现大多数只满足少数属性，没有一个满足所有属性，这解释了先前结果持续不一致的原因。

Conclusion: 提出的LARM和ALARM指标能够满足所有验证属性，填补了现有评估指标的空白，为时间序列异常检测提供了更可靠的评估方法。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [317] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 该论文研究通过降维技术进一步压缩抗菌肽设计的潜空间，以提高优化效率、可解释性和基于理化性质的组织能力。


<details>
  <summary>Details</summary>
Motivation: 尽管变分自编码器等深度生成模型在抗菌肽设计中表现出色，但仍存在可解释性不足和潜空间质量量化不严谨的问题，需要改进潜空间作为搜索空间的效率。

Method: 采用降维技术进一步压缩潜空间，研究潜空间的可解释性，以及通过理化性质组织潜空间来优化抗菌活性。

Result: 发现在数据可用性下，用更相关信息组织空间时，通过降维进一步压缩潜空间是有利的；降维搜索空间更具可解释性；可以在不同标签可用性下用不同理化性质组织潜空间。

Conclusion: 降维技术可以改善抗菌肽设计潜空间的优化效率、可解释性和组织能力，为更有效的抗菌肽发现提供支持。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [318] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种轻量级视觉变换器，用于区分心源性肺水肿与非心源性肺部疾病，在肺超声视频分类中表现优异，参数仅0.25M，训练速度更快。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式的高度视觉变异性，区分心源性肺水肿与非心源性肺部疾病在肺超声视频中具有挑战性，现有模型难以处理这种异质性。

Method: 提出ZACH-ViT模型，移除位置嵌入和[CLS]标记，实现完全置换不变性；提出ShuffleStrides数据增强方法，在保持解剖有效性的同时置换探头视图序列和帧顺序。

Result: 在380个肺超声视频上评估，ZACH-ViT获得最高验证和测试ROC-AUC（0.80和0.79），平衡灵敏度0.60和特异性0.91，而其他模型均失效；训练速度比Minimal ViT快1.35倍，参数减少2.5倍。

Conclusion: 将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [319] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 使用机器学习和深度学习技术（特别是GAN）生成合成数据来解决自杀预测中的极端类别不平衡问题，在真实测试数据上取得了良好的性能指标。


<details>
  <summary>Details</summary>
Motivation: 自杀预测是预防的关键，但真实数据中阳性样本稀少导致极端类别不平衡，需要数据增强技术来改善模型性能。

Method: 使用机器学习模型（逻辑回归、随机森林、支持向量机）和深度学习技术（生成对抗网络）生成合成数据来增强数据集，初始数据集包含656个样本，其中只有4个阳性案例。

Result: 在真实测试数据上，逻辑回归的加权精度为0.99、召回率0.85、F1分数0.91；随机森林分别为0.98、0.99、0.99；支持向量机为0.99、0.76、0.86。逻辑回归和支持向量机正确识别了一个自杀尝试案例（敏感度1.0），但分别误分类了20和31个非尝试案例。

Conclusion: 这些结果证明了模型的有效性，GAN在生成合成数据以支持自杀预防建模工作中发挥了关键作用。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [320] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 该论文提出了三个主要贡献：CADP算法连接策略梯度和动态规划，建立了ERM Bellman算子的收缩条件并提出了相关算法，以及开发了用于风险规避目标的模型无关Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在多模型马尔可夫决策过程中优化折扣回报的问题，特别是在风险规避目标下的策略优化，填补了现有方法在理论保证和实际应用方面的空白。

Method: 方法包括：1）CADP算法通过迭代调整模型权重实现单调策略改进；2）建立ERM Bellman算子的收缩条件并设计值迭代、策略迭代和线性规划算法；3）开发模型无关的Q学习算法处理风险规避目标。

Result: 结果证明了CADP算法能收敛到局部最优，ERM Bellman算子在特定条件下是收缩的，并成功开发了收敛到最优风险规避值函数的Q学习算法。

Conclusion: 结论是成功建立了策略梯度与动态规划的新联系，为风险规避强化学习提供了理论保证和实用算法，解决了ERM-TRC和EVaR-TRC的最优策略计算问题。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [321] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 该论文研究如何提高黑盒大语言模型作为分类器时的操作粒度，通过分析其低基数输出原因，并提出了有效方法来显著增加可用操作点数量，在11个数据集和3个LLM上实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLMs在需要特定指标约束的应用中表现不佳，因为其数值输出基数低，限制了操作点的控制能力，无法精细调整决策行为。

Method: 首先分析LLMs低基数输出的原因，发现它们偏向生成四舍五入但有信息量的语言化概率；然后实验标准提示工程、不确定性估计和置信度激发技术；最后提出有效方法来显著增加可用操作点的数量和多样性。

Result: 提出的方法提供了更细粒度的操作点，在11个数据集和3个LLM上实现了与基准方法相当或更好的性能。

Conclusion: 通过分析LLMs输出特性并开发有效方法，成功提高了黑盒LLMs作为分类器的操作粒度，解决了其低基数输出导致的控制限制问题。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [322] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 本文提出了一种基于微分图册的流形学习方法，通过维护可微分图册结构实现流形上的黎曼优化，在效率和准确性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法主要关注降维到欧几里得空间，当嵌入维度接近流形内在维度时会丢失关键特征。直接学习潜在流形作为微分图册的方法相对未被充分探索。

Method: 实现了一个通用数据结构来维护可微分图册，支持流形上的黎曼优化，并提出了从点云数据无监督学习微分图册的启发式方法。

Result: 实验证明该方法在选定场景下具有效率和准确性优势，在Klein瓶分类任务和造血数据RNA速度分析中展示了更好的可解释性和鲁棒性。

Conclusion: 基于图册的方法在流形学习领域具有有效性和潜力，为直接处理流形数据提供了新的可能性。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [323] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 提出了新的推理时计算扩展方法，在保持线性插值的同时提升流匹配模型的样本质量，并在图像和蛋白质生成任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在推理时扩展方法研究不足，现有方法牺牲了高效采样特性，且仅应用于视觉任务，需要开发保持线性插值的高效扩展方法并扩展到科学领域。

Method: 提出了新颖的推理时扩展程序，在采样过程中保持线性插值，不采用非线性方差保持插值，从而保留流匹配的高效直接采样特性。

Result: 在图像生成和无条件蛋白质生成任务中验证，样本质量随推理计算增加而持续提升，首次证明流匹配推理时扩展可应用于科学领域。

Conclusion: 提出的方法成功实现了流匹配模型在推理时的有效扩展，既保持了高效采样特性，又拓展了应用领域到科学计算任务。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [324] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: 提出了Functional Distribution Networks (FDN)，一种输入条件化的网络权重分布方法，通过beta-ELBO和蒙特卡洛采样训练，旨在解决概率回归器在分布偏移下的过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 现代概率回归器在分布偏移下往往保持过度自信，需要一种能够根据输入自适应调整预测分散度的模型。

Method: FDN通过输入条件化的权重分布诱导预测混合，使用beta-ELBO和蒙特卡洛采样进行训练，并提出了分离插值与外推的评估协议。

Result: 在标准回归任务中，与贝叶斯、集成、dropout和超网络基线相比，在匹配参数和更新预算下评估准确性、校准和偏移感知能力。

Conclusion: 该框架和协议旨在使OOD感知、良好校准的神经回归变得实用和模块化。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>
